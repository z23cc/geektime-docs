你好，我是李锟。

我们即将踏上一段困难但充满惊喜的旅程。这段旅程意义重大，所以在出发之前，我们需要做好准备。以免因为仓促上路，没有携带必要的工具，导致半途而废。

首先我们需要掌握一些 LLM 应用开发的基础知识。对于部署在云端的商业 LLM，我们只需要学会其 API 和客户端库的使用方法即可。对于部署在本地机器上的开源 LLM，我们需要亲自来部署、调优。因此需要掌握的知识会多很多，但是这些努力的回报也会更大。

## LLM 应用开发必备基础知识

首先我们需要理解与基础 LLM 相关的几个术语。

- **训练 training：**在训练数据集上构建复杂的模型，以捕捉更多的数据特征和模式，从而提高预测准确性。
- **评估 evaluation：**对模型的性能进行定量和定性的评估，以确定模型的准确性和效率。
- **微调 fine-tuning：**在预训练模型的基础上，使用少量数据进行精细调整，以适应特定任务的需求。
- **部署 deployment：**将训练好的模型部署到生产环境中，以便实时处理用户请求或进行实时分析。
- **推理 inference：**使用已训练好的模型进行预测或分类，以及解释模型输出的过程。

大致理解了上述 5 个术语，我们可以把 LLM 开发划分成两个层次。

1. 基础 LLM 的研发

<!--THE END-->

- 包括了训练、评估、微调、部署、推理所有 5 类工作。
- 成本高昂，需要万块高端显卡，耗时漫长。其中训练的成本最为高昂，只有投资上亿元的大公司才能负担得起。

<!--THE END-->

2. 基于基础 LLM 的应用开发

<!--THE END-->

- 不包括训练、评估，仅包括微调（可选）、部署（可选）、推理。
- 因为不做训练，成本比训练小得多。仅做微调，成本只有训练的约千分之一。
- 如果微调也不做，成本比微调还要小得多，只有微调的约十分之一。
  
  - 使用普通消费级显卡即可，例如 Nvidia GeForce 系列的 RTX 3070、RTX 4090 等。
  - 将训练好的模型进一步优化瘦身之后，甚至可以部署在嵌入设备内，例如树莓派、智能手机。

LLM 的诞生，特别是 **OpenAI API**（RESTful API + 客户端库）所确立的一套 LLM 应用开发的行业事实标准，彻底改变了传统 NLP（Natural Language Processing）应用开发的经济学。

开发传统的 NLP 应用时，开发者需要从底层基础语言模型开始开发，必须非常熟悉 CNN、RNN、Transformer 等各种 AI 算法和架构。同时还需要非常熟悉特定领域的业务需求，然后为该领域量身定制相应的语言模型。这样对开发者的技能要求非常之高，学习曲线非常陡峭。因此传统 NLP 应用的开发周期很长、成本很高。即使 NLP 应用开发完成，在上线之后的很长时间内，仍然需要投入很多人力来做手工标注，以改善 NLP 应用的准确性和性能。

所以在传统 NLP 行业有一句流行的话：“有多少人工，就有多少智能”。这句话确实很搞笑，也很无奈。我把传统 NLP 应用的开发模式称作“**气宗模式**”（可参阅金庸小说《笑傲江湖》），因为按照华山派气宗的观点，在你没有多年练气获得深厚的内力之前，完全不应该练剑。

与之相对应，今天我们做 LLM 应用的开发，学习掌握大量基础 LLM 研发的知识并非是必要的。OpenAI API 实现了清晰的关注点分离，大幅降低 LLM 应用的开发成本，引爆了 LLM 应用开发的庞大市场。我把 LLM 应用的开发模式称作“**剑宗模式**”，即使你只专心练剑，同样也可以成为武林高手，而且成效更快。以后如果你能同时练气，当然也会很有帮助。专业的事交给专业的人去做，随着各种基础 LLM 的迅速进步，更适于**大规模工业化**的剑宗开发模式未来必然会大行其道。

在做 LLM 应用开发时，我们用到的语言模型可以划分为两大类：

1. **Chat 模型**

<!--THE END-->

- 包括 LLM 和一些优化后的小模型，自带聊天（Chat）能力。
- 开源 Chat 模型举例（Hugging Face Hub 上的名称）
  
  - meta-llama/Meta-Llama-3.2-8B
  - Qwen/Qwen2.5-7B-Instruct
  - THUDM/glm-4-9b-chat

<!--THE END-->

2. **Embedding 模型**

<!--THE END-->

- 用来实现文本的向量化，即所谓的嵌入，用来实现知识库类应用。
- 开源 Embedding 模型举例（Hugging Face Hub 上的名称）
  
  - BAAI/bge-large-zh-v1.5
  - nomic-ai/nomic-embed-text-v1.5

除了这两大类语言模型，还有一类不支持聊天，仅能做**文本补全**的模型，例如 OpenAI 早期发布的 text-davinci-003。不过单纯的文本补全模型现在用的已经很少了，因为 Chat 模型具有文本补全模型的所有功能。

接下来我们还需要理解一个非常重要的概念——模型的量化。

什么是量化（Quantization）？

量化在 AI 模型中，特别是在深度学习模型中，通常指的是将模型中的参数（例如权重和偏置）从浮点数转换为低位宽度的整数，例如从 32 位的浮点数转换为 8 位整数。

量化的优缺点是什么？

- 优点：减少存储需求、加速计算、减少能耗。
- 缺点：会导致模型的精度下降。因为你实际上是在用较低的精度来表示原始的浮点数，可能会损失一些信息，这意味着模型的能力会变差。

量化与语言模型文件的格式：

- 不同的语言模型文件格式，对应着不同的量化方法。
- 模型文件名中的 fp32、fp16、int8、int4 字样表示模型的量化精度。fp32为全精度，量化精度从高到低排列顺序是：fp32&gt;fp16&gt;int8&gt;int4，量化的精度越低，模型的大小和推理所需的显存就越小，但模型的能力也会越差。

## 开源语言模型的发布平台和文件格式

目前开源语言模型主要有以下几个发布平台：

- Hugging Face Hub：[https://huggingface.co](https://huggingface.co/)
- Ollama网站：[https://ollama.com](https://ollama.com/)
- ModelScope（国内）：[https://www.modelscope.cn/home](https://www.modelscope.cn/home)

**先介绍一下 Hugging Face Hub，这是世界最大的开源 AI 模型发布平台。**

Hugging Face Hub 是一个协作平台，其中托管了大量的用于机器学习的开源模型和数据集，可以将其视为 ML 的 GitHub。该平台让你可以轻松地找到、学习开源社区中有用的 ML 资产并与之交互，从而促进共享和协作。该平台已经与 Transformers 库（Hugging Face Transformers）深度集成，使用 Transformers 库部署的模型都是从该平台下载的。

**接下来我们了解一下 Ollama。**

Ollama 是一个开源的 LLM 服务，提供了类似 OpenAI 的 API 和聊天界面，可以非常方便地部署最新版本的 GPT 模型并通过 API 使用。支持热加载模型文件，无需重新启动即可切换不同的模型。Ollama 由 Meta 推出，系出名门，但不限于支持自家的 Llama 系列开源 LLM。底层代码基于著名的开源项目 llama.cpp，因此可完全跨平台（支持 Linux、macOS、Windows）。

Ollama 包括两个部分：

- Ollama 网站：类似 Hugging Face Hub，也是一个开源 LLM 的发布平台，规模远小于 Hugging Face Hub，且仅包括语言模型。
- Ollama 工具：ollama 命令行工具；ollama daemon 服务，提供了与 OpenAI API 兼容的 Ollama API。

**ModelScope 比较简单，可以简单理解为中国版的 Hugging Face Hub。**

从上述发布平台下载的开源语言模型文件有以下 5 种常见格式：

1. Safetensors 格式

<!--THE END-->

- Hugging Face Hub 上最常见的模型格式。
- Hugging Face 推出的一种可靠、易移植的机器学习模型存储格式，用于安全地存储 Tensor，而且速度很快（零拷贝）。

<!--THE END-->

2. GPTQ 格式（Generalized Post-Training Quantization）

<!--THE END-->

- Hugging Face Hub 上常见的模型格式
- GPTQ 是一种针对 4 位量化的训练后量化（PTQ）方法，主要关注GPU推理和性能。
- 该方法的思想是通过将所有权重压缩到4位量化中，通过最小化与该权重的均方误差来实现。在推理过程中，它将动态地将权重解量化为 float16，以提高性能，同时保持内存较低。

<!--THE END-->

3. GGUF 格式（GPT-Generated Unified Format，前身叫 GGML）

<!--THE END-->

- Ollama 网站上大多数模型的格式。
- 由开发者Georgi Gerganov提出，是**专为大型语言模型设计**的二进制文件格式，旨在解决当前 LLM 在实际应用中遇到的存储效率、加载速度、兼容性和扩展性等问题。

<!--THE END-->

4. AWQ 格式（Activation-aware Weight Quantization）

<!--THE END-->

- 一种类似于GPTQ的量化方法。AWQ和GPTQ作为方法有几个不同之处，但最重要的是AWQ假设并非所有权重对LLM的性能都同等重要。也就是说在量化过程中会跳过一小部分权重，这有助于减轻量化损失。所以他们的论文提到了与GPTQ相比，AWQ可以有显著加速，同时保持了相似的、有时甚至更好的性能。

<!--THE END-->

5. AQLM 格式（Additive Quantization of Language Models）

<!--THE END-->

- 于2024年2月发布，已经集成到了 Hugging Face 中。
- 现有的仅权重量化算法在技术上可以将模型权重量化到2位范围。然而，它们未能有效地保持模型的准确性。AQLM是一种新的仅权重后训练量化（PTQ）算法，为2比特/每参数范围设定了新的技术水平。与现有方法相比，它还提供了更小的基准改进，适用于3位和4位范围。具体来说，AQLM 优于流行的算法如 GPTQ，以及更近期但较不知名的方法如 SpQR 和 QuIP。

上述这 5 种文件格式不是固定的，随着开源语言模型的迅速发展，未来还会诞生新的文件格式。

了解了必备基础知识，那么在基于开源 LLM 做 LLM 应用开发之前，我们还需要准备一台适合的机器。

## 准备开发机器和系统

你需要准备一台配置较高的 Linux 主机或虚拟机，可参考以下配置：

- 内存：32G
- CPU：酷睿 8 核 i7 以上
- 显卡：Nvidia GeForce RTX 3070 以上，至少 8G显存
- 操作系统：Ubuntu Linux 22.04 或 24.04，Server 版或 Desktop 版都可以

如果不额外说明，本课程以后的所有操作均在 Linux 主机上执行。在 Windows 10 以上版本中，可以通过安装 WSL2 + Ubuntu 来使用 Linux，具体安装方法请查阅微软的官方文档。

在满足上述要求的 Linux 主机上，还需要安装好 Nvidia 的 CUDA 开发工具集。

CUDA（Compute Unified Device Architecture）是 Nvidia 推出的运算平台。是一种通用的并行计算架构，该架构使得 GPU 能够解决复杂的计算问题。

对于独立的 Linux 环境：

- 安装 Nvidia 驱动程序
  
  - 参考文档：[https://ubuntu.com/server/docs/nvidia-drivers-installation](https://ubuntu.com/server/docs/nvidia-drivers-installation)
- 安装与显卡型号和 Nvidia 驱动程序版本兼容的 CUDA 开发工具集
  
  - 使用 nvidia-smi 命令获取 Nvidia 驱动程序的版本号
  - [https://docs.nvidia.com/deploy/cuda-compatibility](https://docs.nvidia.com/deploy/cuda-compatibility) 查看 CUDA 版本与 Nvidia 驱动版本的兼容表。
  - [https://developer.nvidia.com/cuda-toolkit-archive](https://developer.nvidia.com/cuda-toolkit-archive) 下载安装兼容版本的 CUDA 开发工具集，参考官方安装文档。

对于通过 Windows 的 WSL2 使用的 Linux 环境：

- 在 WSL2 中启用 Nvidia CUDA
  
  - 参考文档：[https://learn.microsoft.com/zh-cn/windows/ai/directml/gpu-cuda-in-wsl](https://learn.microsoft.com/zh-cn/windows/ai/directml/gpu-cuda-in-wsl)

接下来我们在 Linux 主机上部署一些开源 LLM。

## 开源 LLM 的部署工具——Ollama

开源语言模型的部署工具有很多，目前主流的有三个：

- Hugging Face Hub 命令行工具 huggingface-cli
- Ollama
- vLLM

Hugging Face Hub 和 Ollama 前面已经做过介绍了，这里介绍一下 vLLM。

vLLM 是加州大学伯克利分校 LMSYS 组织开源的大语言模型高速推理框架。它利用了全新的注意力算法 “PagedAttention”，提供了易用、快速、便宜的 LLM 服务。项目地址：[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

vLLM 并没有自己的开源语言模型发布平台，它可以直接使用 Hugging Face Hub 或 Ollama 网站下载的开源语言模型，并且提供量化和加速支持。参考文档：

- [https://docs.vllm.ai/en/latest/getting\_started/quickstart.html](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html)

Hugging Face Hub 上的开源语言模型文件大多数都是 Safetensors 格式，基于 Hugging Face Transformers 库开发，依赖 PyTorch，因此需要配备很好的显卡才能流畅运行。上节课已经提到，我们希望开源 LLM 能够在硬件配置较低的机器上运行，那么 huggingface-cli 就无法满足我们的期望了。

Ollama 是基于 llama.cpp 的，即使没有安装 Nvidia 独立显卡，只要 CPU 足够强大，内存足够多，就能运行开源 LLM。而 vLLM 是基于 CUDA 库的，必须安装 Nvidia 独立显卡，对硬件配置的要求高于 Ollama。因此我们会优先选择使用 Ollama 来部署开源 LLM。从简化教学的角度，vLLM 我们就不做展开了，感兴趣的学员们可以自行学习。

Ollama 的基本用法如下。

- 下载安装 Ollama：
  
  - curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sudo sh
  - 验证 ollama 可运行：ollama --version
- Ollama 命令行工具的使用方法
  
  - ollama pull qwen2.5:7b  下载通义千问 Qwen2.5-7B
  - ollama run qwen2.5:7b  测试通义千问 Qwen2.5-7B
  - ollama rm qwen2.5:7b 删除通义千问 Qwen2.5-7B
  - ollama pull znbang/bge:large-zh-v1.5-f16 下载开源Embedding模型
  - ollama list 查看已下载的模型列表

在上面的 ollama 命令行工具的使用例子中，下载部署了开源 Chat 模型阿里巴巴 qwen2.5、Embedding 模型 bge:large-zh-v1.5-f16。在后续课程中，我们还会使用更多的开源语言模型。

## 总结时刻

大多数 LLM 应用开发的图书、教程都只介绍了调用云端部署的商业 LLM 的 API。然而调用云端 LLM 的 API 有很多限制，除了会产生一些费用外，访问的频率、响应速度也有很多限制。

本课程中的实战环节将会以本地部署的开源 LLM 为主来开展教学，以避免上述问题。同时开源 LLM 也给了开发者极大的自由度，可以自由尝试各种奇思妙想。

既然决定基于开源 LLM 开展教学，我们就需要先学习掌握一些与开源 LLM 相关的基础知识和工具，以便顺利部署、使用开源 LLM，甚至以后对 LLM 做性能调优。我们优先选择的开源 LLM 部署工具是 Ollama，使用的开源 LLM 文件是来自 Ollama 网站的 GGUF 格式的文件。

在选择了 Ollama 之后，我们优先选择的开源 LLM 是由阿里巴巴开发的 qwen-2.5，也是目前最为强大的国产开源 LLM。当然，优秀的开源 LLM 还有不少，后续我们会根据教学需要，引入其他开源 LLM。本课程中的代码，稍作修改，同样可以使用云端商业 LLM，例如 OpenAI 的 GPT-4o 或者阿里巴巴的通义千问。

下节课我们将进入实战环节，开始学习第一个 Autonomous Agent 开发框架——MetaGPT 的基础知识。

## 思考题

1. 开发 LLM 应用有哪些自己的关注点，与做基础 LLM 研发的关注点有何区别？为何我们必须做关注点分离？
2. 开源语言模型有哪些文件格式？它们各自有何特点？

期待你的分享。如果今天的内容对你有所帮助，也期待你转发给你的同事或者朋友，大家一起学习，共同进步。我们下节课再见！
<div><strong>精选留言（13）</strong></div><ul>
<li><span>逗逼章鱼</span> 👍（3） 💬（2）<p>这个 MacBook Pro （M1Pro）可以胜任开发机嘛？</p>2025-01-06</li><br/><li><span>zhihai.tu</span> 👍（2） 💬（1）<p>请问老师， MacBookPro，芯片 M4Pro(12+16核) 24G ，这个可以胜任本课程的实验环境吗？</p>2025-01-07</li><br/><li><span>最后的替补</span> 👍（2） 💬（2）<p>老师，我不会动手部署这些开源的软件到我的电脑上，我也不是程序员，我的电脑也没有那么高的配置，我是一名内部审计人员，我的诉求就是能用这些平台或者工具解决一些应用问题，不部署这些开源平台到我的电脑上可以么？</p>2025-01-07</li><br/><li><span>种花家</span> 👍（1） 💬（1）<p>开发基于大型语言模型（LLM）的应用和从事基础 LLM 研发的关注点存在显著差异，如下：

### 开发 LLM 应用的关注点
- **应用需求与场景适配**：需要深入理解目标应用的具体需求和使用场景，如聊天机器人、文本生成、内容推荐等，确保 LLM 的功能与应用目标高度契合，满足用户的实际需求.
- **模型性能优化**：在应用层面，更关注模型的推理速度、响应时间和资源消耗等性能指标，以提升用户体验，例如通过模型压缩、加速推理等技术手段来优化模型性能.
- **数据安全与隐私**：开发应用时，必须严格遵循数据安全和隐私保护的相关法规和标准，确保用户数据的安全性和隐私性，防止数据泄露和滥用.
- **用户界面与交互设计**：良好的用户界面和交互设计对于应用的成功至关重要，需要精心设计应用的界面布局、操作流程和交互方式，使用户能够方便、直观地使用应用.
- **应用集成与兼容性**：在将 LLM 集成到应用中时，需要考虑与其他系统、平台和工具的兼容性，确保无缝集成和稳定运行，例如与企业内部系统、第三方服务等的集成.
- **应用测试与质量保证**：需要进行全面的应用测试，包括功能测试、性能测试、安全测试等，确保应用的质量和稳定性，及时发现并修复问题.

### 基础 LLM 研发的关注点
- **模型架构创新**：在基础研发中，重点关注模型架构的设计和创新，探索新的模型结构、注意力机制等，以提升模型的性能和能力.
- **算法优化与理论研究**：深入研究和优化算法，包括训练算法、优化算法等，同时开展相关的理论研究，以提高模型的训练效率和效果.
- **数据集构建与标注**：构建高质量、大规模的数据集是基础研发的重要环节，需要进行数据的收集、清洗、标注等工作，确保数据的多样性和准确性，为模型训练提供坚实的基础.
- **模型泛化能力提升**：研究和提升模型的泛化能力，使其在不同领域、不同任务中都能表现出良好的性能，减少对特定数据集的依赖.
- **可解释性与透明度**：提高模型的可解释性和透明度，使研究人员和开发者能够更好地理解模型的工作原理和决策过程，以便进行优化和改进.
- **伦理与社会影响研究**：关注模型的伦理问题和社会影响，研究如何避免模型产生偏见、歧视等问题，确保模型的公平性和伦理性.

</p>2025-01-06</li><br/><li><span>Nim</span> 👍（0） 💬（1）<p>在Linux服务器上执行
curl -fsSL https:&#47;&#47;ollama.com&#47;install.sh | sudo sh
进度百分比非常慢
请问这个该怎么解决？</p>2025-02-17</li><br/><li><span>蓝天</span> 👍（0） 💬（1）<p>2070 RTX 也是8GB，能胜任吗？</p>2025-02-11</li><br/><li><span>恩惠吴疆</span> 👍（0） 💬（1）<p>应该告诉我们具体买一台什么样G服务器，节省学生摸索的时间，或者关于百炼灵积服务通义千问LLM的选择的大致情况</p>2025-01-10</li><br/><li><span>C.</span> 👍（0） 💬（1）<p>我 GTX1650 4G  NVIDIA-SMI 566.36  Driver Version: 566.36 CUDA Version: 12.7  32G内存 跑7b是没问题的，不知道后续的课程还行不行。</p>2025-01-09</li><br/><li><span>yangchao</span> 👍（0） 💬（1）<p>T3660 I7-12700 32G  256G+2T 
RTX3060 12G 500W电源  10000元 含税，老师这款是否也可以配合教程使用</p>2025-01-09</li><br/><li><span>元气🍣 🇨🇳</span> 👍（0） 💬（1）<p>看来要买云服务器</p>2025-01-08</li><br/><li><span>李二木</span> 👍（0） 💬（1）<p>Nvidia GeForce RTX 2060 可以不</p>2025-01-08</li><br/><li><span>alue</span> 👍（0） 💬（1）<p>很想知道 员工 AI 助理 解决的是什么具体问题？</p>2025-01-06</li><br/><li><span>yangchao</span> 👍（1） 💬（0）<p>思考题1：开发 LLM 应用有哪些自己的关注点，与做基础 LLM 研发的关注点有何区别？为何我们必须做关注点分离？
回答：开发LLM应用主要关注点是微调、部署和推理，具体比如用户体验、应用场景适配、数据安全、成本控制等，不包括基础LLM研发中的训练和评估。这样关注点分离，术业有专攻，主要是因为模型训练成本可能会非常巨大，作为LLM应用开发不需要关心底层模型的训练，这也是LLM开发应用能够快速规模化的原因，方便我们LLM应用开发者低成本进行开发；风险隔离和可维护性提升

思考题2：开源语言模型有哪些文件格式？它们各自有何特点？
回答：（1）Safetensors：安全可靠、兼容性好，容易上手；（2）GPTQ：高效量化压缩，推理速度优化，广泛的量化位支持,与多数 GPU 硬件兼容；（3）GGUF：高效存储，快速加载，兼容性好，可扩展性强</p>2025-01-12</li><br/>
</ul>