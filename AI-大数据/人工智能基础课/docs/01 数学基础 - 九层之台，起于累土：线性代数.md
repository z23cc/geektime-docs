“人工智能基础课”将从数学基础开始。必备的数学知识是理解人工智能不可或缺的要素，今天的种种人工智能技术归根到底都建立在数学模型之上，而这些数学模型又都离不开线性代数（linear algebra）的理论框架。

事实上，线性代数不仅仅是人工智能的基础，更是现代数学和以现代数学作为主要分析方法的众多学科的基础。从量子力学到图像处理都离不开向量和矩阵的使用。而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：**万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察**。

线性代数中最基本的概念是集合（set）。在数学上，集合的定义是由某些特定对象汇总而成的集体。集合中的元素通常会具有某些共性，因而可以用这些共性来表示。对于集合 { 苹果，橘子，梨 } 来说， 所有元素的共性是它们都是水果；对于集合 {牛，马，羊} 来说，所有元素的共性是它们都是动物。当然 { 苹果，牛 } 也可以构成一个集合，但这两个元素并没有明显的共性，这样的集合在解决实际问题中的作用也就相当有限。

“苹果”或是“牛”这样的具体概念显然超出了数学的处理范围，因而集合的元素需要进行进一步的抽象——用数字或符号来表示。如此一来，集合的元素既可以是单个的数字或符号，也可以是多个数字或符号以某种方式排列形成的组合。

在线性代数中，由单独的数a构成的元素被称为标量（scalar）：一个标量a可以是整数、实数或复数。如果多个标量${ a\_1, a\_2, \\cdots, a\_n}$ 按一定顺序组成一个序列，这样的元素就被称为向量（vector）。显然，向量可以看作标量的扩展。原始的一个数被替代为一组数，从而带来了维度的增加，给定表示索引的下标才能唯一地确定向量中的元素。

每个向量都由若干标量构成，如果将向量的所有标量都替换成相同规格的向量，得到的就是如下的矩阵（matrix）:

$$\\left( {\\begin{array}{cc}  
{{a\_{11}}}&amp;{{a\_{12}}}&amp;{{a\_{13}}}\\cr  
{{a\_{21}}}&amp;{{a\_{22}}}&amp;{{a\_{23}}}\\cr  
{{a\_{31}}}&amp;{{a\_{32}}}&amp;{{a\_{33}}}  
\\end{array}} \\right)$$

相对于向量，矩阵同样代表了维度的增加，矩阵中的每个元素需要使用两个索引（而非一个）确定。同理，如果将矩阵中的每个标量元素再替换为向量的话，得到的就是张量（tensor）。直观地理解，张量就是高阶的矩阵。

如果把三阶魔方的每一个小方块看作一个数，它就是个3×3×3的张量，3×3的矩阵则恰是这个魔方的一个面，也就是张量的一个切片。相比于向量和矩阵，张量是更加复杂，直观性也更差的概念。

向量和矩阵不只是理论上的分析工具，也是计算机工作的基础条件。人类能够感知连续变化的大千世界，可计算机只能处理离散取值的二进制信息，因而来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，**线性代数是用虚拟数字世界表示真实物理世界的工具**。

在计算机存储中，标量占据的是零维数组；向量占据的是一维数组，例如语音信号；矩阵占据的是二维数组，例如灰度图像；张量占据的是三维乃至更高维度的数组，例如RGB图像和视频。

描述作为数学对象的向量需要有特定的数学语言，范数和内积就是代表。范数（norm）是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。通用的$L ^ p$范数定义如下：

$${\\left| {\\bf{x}} \\right|\_p} = {\\left( {\\sum\\limits\_i {{{\\left| {{x\_i}} \\right|}^p}} } \\right)^{\\frac{1}{p}}}$$

对⼀个给定向量，$L ^ 1$范数计算的是向量所有元素绝对值的和，$L ^ 2$范数计算的是通常意义上的向量长度，$L ^ {\\infty}$范数计算的则是向量中最大元素的取值。

范数计算的是单个向量的尺度，内积（inner product）计算的则是两个向量之间的关系。两个相同维数向量内积的表达式为

$$\\left\\langle {{\\bf{x,y}}} \\right\\rangle = \\sum\\limits\_i {{x\_i} \\cdot {y\_i}} $$

即对应元素乘积的求和。内积能够表示两个向量之间的相对位置，即向量之间的夹角。一种特殊的情况是内积为0，即$\\left\\langle \\bf{x,y} \\right\\rangle = 0$。在二维空间上，这意味着两个向量的夹角为90度，即相互垂直。而在高维空间上，这种关系被称为正交（orthogonality）。如果两个向量正交，说明他们线性无关，相互独立，互不影响。

在实际问题中，向量的意义不仅是某些数字的组合，更可能是某些对象或某些行为的特征。范数和内积能够处理这些表示特征的数学模型，进而提取出原始对象或原始行为中的隐含关系。

如果有一个集合，它的元素都是具有相同维数的向量（可以是有限个或无限个）， 并且定义了加法和数乘等结构化的运算，这样的集合就被称为线性空间（linear space），定义了内积运算的线性空间则被称为内积空间（inner product space）。**在线性空间中，任意一个向量代表的都是n维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示**。两者相互等效。

在线性空间上点和向量的相互映射中，一个关键问题是参考系的选取。在现实生活中，只要给定经度、纬度和海拔高度，就可以唯一地确定地球上的任何一个位置，因而经度值、纬度值、高度值构成的三维向量(x, y, h)就对应了三维物理空间中的⼀个点。

可是在直觉无法感受的高维空间中，坐标系的定义可就没有这么直观了。要知道，人工神经网络要处理的通常是数以万计的特征，对应着维度同样数以万计的复杂空间，这时就需要正交基的概念了。

在内积空间中，一组两两正交的向量构成这个空间的正交基（orthogonal basis），假若正交基中基向量的$L ^ 2$范数都是单位长度1，这组正交基就是标准正交基（orthonormal basis）。正交基的作用就是给内积空间定义出经纬度。⼀旦描述内积空间的正交基确定了，向量和点之间的对应关系也就随之确定。

值得注意的是，描述内积空间的正交基并不唯一。对二维空间来说，平面直角坐标系和极坐标系就对应了两组不同的正交基，也代表了两种实用的描述方式。

线性空间的一个重要特征是能够承载变化。当作为参考系的标准正交基确定后，空间中的点就可以用向量表示。当这个点从一个位置移动到另一个位置时，描述它的向量也会发生改变。**点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵**。

在线性空间中，变化的实现有两种方式：一是点本身的变化，二是参考系的变化。在第一种方式中，使某个点发生变化的方法是用代表变化的矩阵乘以代表对象的向量。可是反过来，如果保持点不变，而是换一种观察的角度，得到的也将是不同的结果，正所谓“横看成岭侧成峰，远近高低各不同”。

在这种情况下，矩阵的作用就是对正交基进行变换。因此，对于矩阵和向量的相乘，就存在不同的解读方式：

$${\\bf{Ax = y}}$$

这个表达式既可以理解为向量x经过矩阵A所描述的变换，变成了向量y；也可以理解为一个对象在坐标系A的度量下得到的结果为向量x，在标准坐标系 I（单位矩阵：主对角线元素为1，其余元素为0）的度量下得到的结果为向量y。

这表示矩阵不仅能够描述变化，也可以描述参考系本身。引用网络上一个精当的类比：表达式Ax就相当于对向量x做了一个环境声明，用于度量它的参考系是A。如果想用其他的参考系做度量的话，就要重新声明。**而对坐标系施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘**。

描述矩阵的⼀对重要参数是特征值（eigenvalue）和特征向量（eigenvector）。对于给定的矩阵A，假设其特征值为λ，特征向量为x，则它们之间的关系如下：

$${\\bf{Ax}} = \\lambda {\\bf{x}}$$

正如前文所述，矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化。可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果。对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值。

**矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向**。如果把矩阵所代表的变化看作奔跑的人，那么矩阵的特征值就代表了他奔跑的速度，特征向量代表了他奔跑的方向。但矩阵可不是普通人，它是三头六臂的哪吒，他的不同分身以不同速度（特征值）在不同方向（特征向量）上奔跑，所有分身的运动叠加在⼀起才是矩阵的效果。

求解给定矩阵的特征值和特征向量的过程叫做特征值分解，但能够进行特征值分解的矩阵必须是n维方阵。将特征值分解算法推广到所有矩阵之上，就是更加通用的奇异值分解。

今天我和你分享了人工智能必备的线性代数基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：

- 线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；
- 向量的实质是n维线性空间中的静止点；
- 线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；
- 矩阵的特征值和特征向量描述了变化的速度与方向。

线性代数之于人工智能如同加法之于高等数学。这样一个基础的工具集，你能想到它在人工智能中的哪些具体应用呢？

欢迎发表你的观点。

![](https://static001.geekbang.org/resource/image/e4/a2/e4111df16317c6c9a400ed9494c2f8a2.jpg)
<div><strong>精选留言（15）</strong></div><ul>
<li><span>听天由己</span> 👍（46） 💬（1）<p>今天最大的启发就是，出来混，迟早要还的。大学时候读了文学方向，考研努力了一年有余，备考了高等数学，可是现在重新捡起来真的很有难度。

科学始终都是要有理论基础的，从纯粹的构想到最终的论证过程，这是一系列的思考与解答。只是，当时却不知道为什么要学习数学，只是模糊地理解原来经济学需要扎实的数学基础，现在看来，科学都是如此。

今天的问题，我只能去搜索答案了，Google Page Rank 就是有矩阵相乘推导算法，其他的就是如今的机器学习以及游戏 3D 建模。看到那么多人都在感慨，不好好学习线性代数，怎么才能理解计算机与这个时代的各种现实问题。看来我得好好补课了。

希望老师提供其他的学习资料与辅助教材，我们才能学得更快、理解更深。</p>2017-12-25</li><br/><li><span>斌</span> 👍（9） 💬（1）<p>请问下老师，除了线性代数，是否还需要微积分的基础呢？如果需要可否指点一下具体是那几个章节的知识点呢，谢谢！</p>2017-12-24</li><br/><li><span>王天一</span> 👍（29） 💬（1）<p>@ junwen.luo 当单频的正弦波输入线性时不变系统时，输出仍然是原始频率的正弦波，改变的只是幅度和相位。所以每个单频信号都是线性时不变系统的特征向量，其幅度和相位的变化就是特征值，这就是傅立叶变换的基础。</p>2017-12-11</li><br/><li><span>王天一</span> 👍（5） 💬（0）<p>@ 夜行观星 非线性空间就要使用非线性代数了。非线性代数就是加法和数乘都不满足通常的定义，要分析就很困难。无甚必要使用这么复杂的模型。</p>2017-12-11</li><br/><li><span>王天一</span> 👍（2） 💬（0）<p>@ 秦龙君-北大 @huahua8893 每个模块结束后，会单独对参考资料做个梳理</p>2017-12-11</li><br/><li><span>aibear2018</span> 👍（29） 💬（1）<p>解释的太精彩了，高中时候就知道计算，完全不知道这些代表了什么东西，有什么意义，现在看来真是遗憾啊，高中时候要知道这些，是不是会更有学习动力和兴趣呢</p>2018-02-08</li><br/><li><span>Davilk</span> 👍（15） 💬（1）<p>王老师，27岁了转行学ai还晚吗？</p>2018-01-25</li><br/><li><span>唯一</span> 👍（9） 💬（3）<p>老师，我想问是这样吗：矩阵的特征值和特征向量描述的是变化的速度和方向，也就是这个矩阵乘以任意向量，就是让这个向量发生变化，变化速度是特征值，变化方向是特征向量。这样理解对吗？</p>2019-10-13</li><br/><li><span>超然</span> 👍（8） 💬（1）<p>老师，我是基于兴趣来学习的，没上过大学。但是我有做一个基于语音交互的应用梦想，不知道行不行</p>2018-05-28</li><br/><li><span>清音阁</span> 👍（7） 💬（1）<p>老师讲的非常好👍但其中有些举例似乎不够严谨。例如语音是一维向量？好像没这么简单。</p>2018-06-07</li><br/><li><span>Tsubasa翼</span> 👍（7） 💬（1）<p>请问极坐标系的正交基是啥？</p>2018-02-11</li><br/><li><span>Sdylan</span> 👍（6） 💬（1）<p>为啥我们很多人读书的时候，学习线性代数、高数啥的，几乎都是纯理论，没有将这些知识运用到ai或者其他领域中。在读研的时候，学习图像处理就明白高数中傅立叶变换居然可以用到图形处理。</p>2018-02-24</li><br/><li><span>Shawn.C</span> 👍（4） 💬（1）<p>文中写到：
1、定义了内积的线性空间叫做内积空间
2、内积空间中正交的向量叫做正交积
3、极坐标是一组正交积

根据1,2,3 可以推导出：

极坐标是线性空间，

但是极坐标不满足线性空间的条件。

所以三条描述应该有一条是不准确的。</p>2018-10-08</li><br/><li><span>刘強</span> 👍（4） 💬（2）<p>老师，如果人工智能的基础是数学，那么我们人类智能的基础是什么？如果不是数学，那是不是说明我们的研究方向不对，应该向生物大脑方向研究？也不知道怎么突然冒出这么个想法？希望老师点一下。</p>2018-05-10</li><br/><li><span>王庆</span> 👍（3） 💬（4）<p>矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化。可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果。对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值。  老师，我第一句就不理解。。。</p>2018-03-27</li><br/>
</ul>