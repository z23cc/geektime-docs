你好，我是独行。

这节课我们来讲讲机器人，机器人的种类有很多，ChatGPT也算是一种机器人，就是聊天机器人嘛，还有像客服机器人，甚至扫地机器人、喷漆机器人等等。不过我们这里特指人形机器人。机器人进化和人类进化很类似：首先，机器人要能站稳，能走直线，不会摔倒——对应到人脑，就是小脑部分要解决的问题；后来慢慢发展出了语音识别、图像识别，这时已经对应到了大脑中的某个特定区域；到现在大模型出来之后，相当于进化到了前额叶的部分，AI也开始能具备一些“基本智力”。

有人说，机械的进步=控制力的进步，比如第一次工业革命，诞生了气缸这样的基础组件；到第二次工业革命，有了基础控制和自动化，人类开始能够通过机械操控机械；到计算机革命时代，有了PLC（可编程控制器），开始用电脑控制机械，再后来，我们用3D视觉AI控制机械，到现在用大模型控制机械。

每代之间不是替代关系，而是补充关系，不是说有了计算机，机械控制就没用了，而是通过计算机给机械控制赋能，让机械有了更强的能力，由此诞生出了更多的产品形态，有了大模型，可以逐渐替换以前由人来控制的场景。

![图片](https://static001.geekbang.org/resource/image/d3/8f/d3f50d3yy88322729f900c3d98cfda8f.png?wh=1910x1374)

## 机器人行业的困境

首先，传统AI并不智能，训练成本高，泛化能力弱，除了在机器人视觉领域，严格意义上应该是计算机视觉（CV）领域，有较多应用之外，在机器人这样动作连续且复杂、有较多物理交互和操作因果性的领域落地情况并不多。

其次，目前的机器人很多时候只能被当作一种可编程的设备，基本只能完成特定场景下的特定工作，在人类社会的渗透率仍然很低。以使用最广泛、渗透率最高的工业机器人为例，2022年中国一共销售约30万台机器人，总体保有量在150万到200万台之间。而中国的制造业产业工人约有一亿人，通常情况下一台机器人代替0.5～2个人，取平均数1的话，工业机器人的总体渗透率在2%左右，这意味着绝大部分工厂里的生产工作仍然由人工完成。

此外，现有的机器人应用软件还不能充分发挥机器人现有机能。虽然目前机器人硬件性能距离“终结者”仍然有巨大的差距，但是整体硬件机能已经达到了很不错的可用水平。机器人是典型的机电软一体化产品，软件与硬件是相互限制又相互促进的矛盾体。机器人机能提升可以带来更好的性能，更好的性能支持更强大的软件，更强大的软件支持更多应用，更多应用扩大市场促使机器人厂商研发机能更强大的机器人，由此机器人产业才会步入良性的发展循环。

最后，价格昂贵。像人形机器人，价格起码两位数+万起。

总结起来就是：不智能、功能单一、应用软件能力有限、价格贵。实际上我认为前三个问题归根结底就一个问题：**软件智能化问题**，软件智能化根源在智能，也就是我们今天要谈的核心，大模型最大的突破就是比之前的模型更智能，那么能不能把大模型的能力嫁接到机器人上呢？答案是肯定的。

## 大模型如何赋能机器人？

我们先来看下机器人的工作流程。首先需要告诉机器人要做的任务是什么，机器人就会理解需要做的事情，然后拆分任务动作，生成应用层控制指令，并根据任务过程反馈修正动作，最终完成人类交给它的任务。整个过程不需要或者仅需少量人类的介入和确认，基本实现了机器人自主化运行。举一个例子，我们给机器人一个任务：去接一杯水，它会分解成4个动作。

1. 任务定义与描述：去接一杯水。
2. 任务分解为动作：把从拿杯子到打开水龙头到接水的过程拆分成一个一个细小的动作。
3. 根据分解动作对机器人进行编程，生成代码：可以是C++、Python，也可以是自定义的机器人编程语言。
4. 控制-执行-反馈，这是传统机器人控制的主要功能。

在大模型之前，只有第4步是由计算机完成的，前三步都是由工程师完成，那么现在前3步似乎也成为可能。

大模型强大的上下文理解能力，可以辅助机器人完成对所执行目标的理解，以及进行任务的拆解、规划。大模型强大的代码生成能力，可以快速生成各种语言的可执行代码。few-shot和zero-shot能力，使机器人能够在少量训练或者不训练的情况下，灵活地应对各种任务和指令，而不必为每个任务进行专门的训练。这让机器人在实际应用中更具通用性，能够处理多样化的场景和需求。

![图片](https://static001.geekbang.org/resource/image/3c/7e/3c04bacf1422799cd5d2fdffe073f57e.png?wh=2010x914)

基于这几点思路，业界已经有了好几个成熟的方案，我们一起看一下。

## 技术框架

### RT-2

RT-2就是Robotics Transformer-2，是谷歌DeepMind团队开发的一种结合大规模Transformer模型和强化学习的机器人技术。RT-2利用Transformer模型的强大学习能力和适应能力，使机器人在复杂任务中表现更加出色。RT-2建立在视觉-语言模型（VLM）的基础上，又创造了一种新的概念：**视觉-语言-动作（VLA）模型**，它可以从网络和机器人数据中进行学习，并将这些知识转化为机器人可以控制的通用指令。

### Q-Transformer

Q-Transformer的核心思想是在Transformer模型的基础上，引入Q-learning等强化学习算法，用于处理时序决策任务。主要步骤包括：

1. 数据输入：收集环境状态和动作数据，这些数据包括时序特征和上下文信息。
2. 特征提取：通过Transformer编码器提取特征，捕捉状态之间的复杂依赖关系。
3. Q值计算：结合强化学习算法，如Q-learning，通过预测不同动作的Q值，来评估每个动作的潜在收益。
4. 策略优化：使用强化学习策略，选择最优动作，更新Q值，从而优化决策过程。

Q-Transformer可以用于优化机器人的路径规划和任务执行。通过学习环境中的状态变化，机器人可以更智能地选择行动路径，避免障碍物，提高任务完成效率。

当然还有AutoRT、SARA-RT、RT-Trajectory、RT-H等技术框架，这里我就不一一说了，感兴趣的话可以自己了解一下。

## 创业公司

### Figure

ChatGPT一直被认为是一个聪明的大脑，直到投资Figure AI后，同时拥有了强大的身体。ChatGPT+FigureAI产生了最强大的人形机器人Figure 01。据FigureAI的官方说法，他们的目标是开发通用的类人机器人，为人类带来积极影响，为子孙后代创造更美好的生活。这些机器人可以消除对不安全和不受欢迎的工作需求——最终让我们过上更快乐、更有意义的生活。

### 1X

1X来自挪威，是1X Technologies的简称，1X是OpenAI投资的第一家硬件公司。其实1X的硬件说实话，在业界表现很普通，但是软件却很强，核心人员来自Google Brain、Deepmind等。加上OpenAI加持，号称是业界真正专注于大模型软件和机器人硬件结合的公司，将来最有可能发展为由ChatGPT控制的机器人。

除此之外，还有像Tesla的擎天柱、美国波士顿动力公司生产的Atlas等等。

## 机器人的发展障碍

### 生成任务的安全性

之前我看到过一个例子，有一个机器人在给厨房帮忙用蒸箱蒸馒头，蒸之前开蒸箱门往里面放是不需要考虑太多因素的，只要别发生碰撞即可。但是蒸好之后需要打开蒸箱拿出来的时候就需要考虑旁边是否有人，因为蒸箱打开的时候高温蒸汽喷出会对旁边的人造成烫伤，机器人是否能认识到这一点并在生成“打开刚刚使用过的蒸箱”任务动作时，考虑高温蒸汽对人的影响，没有人的话可以直接打开，有人走过来的话就要晚点开或者提醒人员离开一点，这就是基本的安全要求。

### 操作动作的安全性

在很多需要专业技能的领域，机器人还要关注细微工艺动作是否符合安全规范，譬如在机器人手术中，机器人生成的磨削骨头或者切割软组织的某个细微动作，是否符合手术手法要求，是否会对病人造成额外伤害，也是需要慎重考虑的问题。

### 缺乏高质量训练数据

像图像和自然语言处理领域，我们可以从网上获取大量训练数据，可是用于训练机器人学会执行新任务、新技能的高质量数据却非常匮乏，因为机器人执行任务时，面临的环境和交互内容模态更多，更加复杂，因此需要的数据集规模也比CV和NLP领域要大，目前业界最新的多模态大模型GPT-4虽然已经能动态地理解一些内容，但是对于需要执行各种复杂物理交互的机器人来讲，这只是基础要求。

当然除了这3点之外还有很多，比如执行速度、操作准确性，甚至伦理道德等问题，都是需要我们考量的。

## 小结

这节课我带你学习了大模型对机器人行业的赋能，大模型赋予了机器人强大的任务理解能力和代码生成能力，随着AI大模型技术的不断发展，机器人行业也会迎来春天，尤其是人形机器人，未来10～20年，肯定会有黑马杀出来。现在稍微领先一些的技术框架是 RT-2 和 Q-Transformer。如果你感兴趣的话，可以选择自己喜欢的技术框架和公司深入了解一下。

## 思考题

畅想一下，当人形机器人行业发展成熟后，可以为我们的生活带来什么变化？欢迎你把想法打在评论区，和我一起讨论！如果你觉得这节课的对你有帮助的话，也欢迎你分享给其他朋友，我们下节课再见！
<div><strong>精选留言（1）</strong></div><ul>
<li><span>石云升</span> 👍（1） 💬（2）<p>总感觉目前的人形机器人缺少人味，做不到真正的陪伴。但如果用在工业，消防等危险领域到是很好。另外就是机器人不一定是人形的，这类硬件+AI的场景反倒是有很多机会。</p>2024-09-08</li><br/>
</ul>