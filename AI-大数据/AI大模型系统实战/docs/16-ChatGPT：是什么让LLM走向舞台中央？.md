你好，我是Tyler。

在上节课，我们一起学习了 OpenAI GPT 1-3 系列的发展历程，你掌握得如何？

目前大部分人都对 GPT-1 到 GPT-3 的工作都比较熟悉，因为它们之间有很清晰的发展脉络，但是对 OpenAI 之后的工作内容却是一头雾水。

我们的认知似乎也停留在 GPT-3 上，因为在我们的印象里，自从GPT-3 问世之后，LLM 的生成内容已经开始变得真假难辨，很难区分出后续模型的细微差别了。

## 达文西诞生

然而，实际情况是，在 GPT-3（Davinci）之后，OpenAI 内部存在着众多不同的“内部版本”。你可能已经非常熟悉像 text-davinci-xxx 这样的 OpenAI API 选项。其实每一个代号的都代表着OpenAI的一次尝试，是大语言模型发展过程中积累的宝贵财富。

所以其实我们今天看到的很多能力，并不是一蹴而就的，背后藏着许多不可忽视的细节。在今天的课程中，我将会带你深入理解 OpenAI 是怎么逐步“试”出 ChatGPT 的，让你从这个经典的颠覆式创新里面得到一些启示。

正如下图所示，自GPT-3之后，OpenAI的发展经历了几个关键阶段，主要包括 CodeX、WebGPT 和 ChatGPT。

![](https://static001.geekbang.org/resource/image/c4/cf/c452febeb3999d1137a4d2faba6f56cf.jpg?wh=1867x948)

## Codex models

像大多数科技公司一样，OpenAI 在获得了 GPT-3 之后的第一件事就是寻找它的应用场景。

在寻找的过程中，他们发现 GPT-3 在代码生成和代码的静态解释上表现突出。因此，OpenAI 便基于他们的“首代模型” 特工达闻西 Davinci（其实就是GPT-3），追加了数千万个开源代码库作为语料进行 finetune 训练，得到了 CodeX 的主要模型 Code-davinci-002。

经工业界广泛验证表明，人们也发现，经过了针对性的代码语料增强训练后，大模型在逻辑推理和思维连贯能力方面都表现得更出色。很可能是因为这一点，Code-davinci-002 成为了 OpenAI 未来所有主流模型的基石。Code-davinci-002 的成功也标志着 GPT 正式进入了 3.5 时代。

![](https://static001.geekbang.org/resource/image/74/e0/74c87a817a48b01954832e14215b01e0.jpg?wh=1867x1050)

## WebGPT

还记得我们在[第2节课](https://time.geekbang.org/column/article/686408)的时候，讨论过 OpenAI 教会大语言模型使用和制造工具的一系列工作吗？尽管我们现在才看到 OpenAI 推出了联网能力、Function 和 Code Interpreter。但其实早在 code-davinci-002 刚出现的时候，OpenAI 就已经开始研究让 LLM 使用互联网工具增强回答质量的方法了。

这些工作发表在了 [WebGPT](https://arxiv.org/abs/2112.09332) 这篇论文中。你可能有点奇怪，为什么之前从没听说过这个名字？这是很正常的，因为它没有在 GPT 的主线文章中出现过，行内也有很多人没有关注过这篇论文。

但是它的作用非常重要，对我们了解 OpenAI 的发展思路和理解他们的技术选型有很大的帮助。这篇文章让我们发现了 OpenAI 很早就开始教导 LLM 使用工具，并使用强化学习来优化 LLM 的指令理解能力。所以这算得上是一篇承上启下的文章，现在我们来学习一下它的具体内容。

WebGPT 提供了一整套的方法，使 LLM 能够自主地搜索内容、浏览网页并获取知识，以使语言模型更准确地回答问题，并为回答的内容提供引用来源，从而提高回答的可信度。

在实现过程中，OpenAI 开发了一套工具，用于记录标注人员在的各种行为，其中包括使用搜索引擎辅助回答给定问题时，采取的动作（如搜索、点击链接、翻页等等）。这些数据能帮助 WebGPT 模拟人类在使用搜索引擎时的习惯行为，生成查询内容，使用查询结果。

![](https://static001.geekbang.org/resource/image/51/d3/5187e77539f2666864500e8f4fc73bd3.jpg?wh=1867x944)

值得一提的是，在 WebGPT 项目中，我们第一次看到了 RLHF（Reinforcement Learning from Human Feedback）的身影，这也是 ChatGPT 的前身。为什么这么说呢？因为WebGPT 的训练依赖以下两类数据。

- **demonstrations：**标注者使用搜索引擎回答问题的数据。
- **comparisons：**收集对同一问题模型生成的多个回复，由标注者标注哪个更好。

而 comparisons 数据其实就是 RLHF 的数据形式，下图是 comparisons 数据的标注工具，稍后你会发现它和 ChatGPT 的标注工具十分相似。此外，WebGPT 还能帮助 OpenAI 快速地改善既有语料库数据质量，为后续模型的发展提供更多有价值的训练数据。

![](https://static001.geekbang.org/resource/image/80/36/800279c28b4397ff65ccdb68687b2b36.jpg?wh=1867x1050)

## GPT-3.5 models

Code-davinci-002 在商业化上的成功，为 OpenAI 鼓舞了士气，他们在 code-davinci-002 的基础上使用了更多的文本语料，经过监督微调（Supervised Fine-Tuning，SFT）训练得到了 text-davinci-002。这里要注意的是，OpenAI 在这里再次启用了**监督微调**，而不是之前“大力出奇迹”的自监督学习。

他们通过大量的人工标注自然语言对话语料，对 code-davinci-002 进行监督微调。在微调阶段，提供的提供示例指令可以是对话片段、问题-回答对等。在这个阶段，模型会根据这些示例指令完成多次迭代训练，这让模型获得了优异的指令理解和意图识别能力，模型的输出也更符合人类的期待，胜过通用文本生成模型。

OpenAI 看到这个成果之后忍受不了这个诱惑，上下文学习（in-context Learning）的理想情怀虽说还挂在嘴边，但私下其实又偷偷把**监督学习这个祖师爷请回来了**。这让 text-davinci-002 具备了与人类进行自然对话的能力。

## ChatGPT（RLHF）

既然监督学习效果这么好，OpenAI 当然不会点到为止，以 Dota2 起家成名的 OpenAI 掏出了自己的看家本领——强化学习。

下图是早期 OpenAI 在 Dota 2 比赛中的一些珍贵影像，此时的 OpenAI 还是马斯克的“梦中情厂”。可以看出早在 2019 年，OpenAI 就已对“人类反馈”这类训练数据使用得炉火纯青了，而且他们在 Dota2 的这篇[论文](https://cdn.openai.com/dota-2.pdf)里表明他们在那时就已经开始全面使用近端策略优化算法（PPO） 了。

![](https://static001.geekbang.org/resource/image/b3/43/b3e294a400886caceb52c842d24e0f43.jpg?wh=1867x1050)

你在之前已经学习过强化学习（Reinforcement Learning，简称 RL）的概念，它是控制论学派的经典方法，通过在一个由智能体（Agent）与环境（Environment）交互的情境中进行试错学习，来解决目标问题。

在这个优化问答的任务中，我们只需要像后面这张图一样，不断地给模型关于回答好坏的反馈，就能帮助它不断进步，生成更符合人类惯例的回答。

![](https://static001.geekbang.org/resource/image/37/86/37d8b95813e3ab2fb1267f6f300d3786.jpg?wh=1867x1050)

相比监督微调，这种方法的优势在于不需要标注者编写回答，只需要为模型生成的几个回答打分。通过这种方法，OpenAI 大幅提高了标注效率。模型负责生成具体回答，人类标注者负责排序。随着迭代和反馈，模型会明显提升生成对话的准确性、流畅性和逻辑性。

![](https://static001.geekbang.org/resource/image/20/88/207258c304e4b6e151a201c4a1bd1788.jpg?wh=4000x2250)

整个训练流程如下图所示。你可以先看看流程图，再听我讲解。

![](https://static001.geekbang.org/resource/image/14/fb/149e1e46bb47950ba6c82633e243ccfb.jpg?wh=4000x2250)

首先，我们通过比较容易获得的公开无标签数据，来训练一个大语言模型，比如 GPT-3，这是我们在前几节课就已经学过的知识。然后，通过人工编写的问答对，来生成高质量的监督对话数据，来优化大语言模型的对话能力。

在得到了这个优化后模型之后，标注者便在给定问题上可以基于模型生成的答案，对回答进行排序，并用排序数据训练一个奖励模型对回答的结果排序打分，用来评估回答的质量。

看到**“排序打分”**这个词，你是不是特别熟悉？没错，这里的排序打分和你在 AIRC 系统中所学习的排序打分方法没有任何区别，只不过它使用了 Learning to Rank 方法来训练这个打分模型，这个方法早已广泛使用在了 AIRC 系统当中。

这里我们这就来看看 Learning to Rank 方法的原理。这个方法很直观，其实就是对偏序关系进行建模，借助偏序关系的传递性，进一步对全局的偏序关系建模。你可以借助这个方法的损失函数直观理解它的原理。

$$loss(\\theta)=-\\frac{1}{C\_K^2}E\_{(x,y\_w,y\_l)\\sim D}\[log(\\sigma(r\_\\theta(x,y\_w)-r\_\\theta(x,y\_l)))]$$

其中，$y\_w$代表排序排在 $y\_l$ 的所有句子。损失函数的值等于排序列表中所有排在前面项的 reward 的总和，减去排在后面项的 reward 的总和。这其实就是好答案和坏答案之间的距离。

因此，我们只要最小化损失函数，就可以得到更好的排序结果。然后，有一点需要注意的是，损失函数的目标是让值最小，所以在前面我们要加一个负号。

这里的 PPO 和刚提到 Dota 2 中用到的 PPO 其实没有什么区别，这节课你的目标是先熟悉一下监督微调，奖励模型训练和 PPO 强化学习的整个流程。

最后，也是强化学习中最重要的一步，就是用你的“奖励模型”来提升 SFT 后模型的效果。这部分的内容如下图所示，这里的 PPO 和Dota 2中用到的其实没有什么区别，这节课你的目标是先熟悉一下监督微调，奖励模型训练和 PPO 强化学习的整个流程。我会在[第20节课](https://time.geekbang.org/column/article/704029)讲模型工程时带你实操，今天我们先把重点放在深入理解前面的知识上。

![](https://static001.geekbang.org/resource/image/e5/18/e5017138e8b5dd45fa5de84d16f25418.jpg?wh=4000x2250)

## 总结

好了，今天的课程先到这里，我们做个总结吧。

今天我带你逐步了解了 OpenAI 是如何赋予 ChatGPT 智能的。从早期基于 GPT-3 的代码语料追加训练，得到了 code-davinci-002。再到后来使用人类对话语料进行监督微调，形成了 text-davinci-002。最后通过基于强化学习的 RLHF，创造出了石破天惊的 ChatGPT。其中的每一步都对应了一个“内部版本”，每个版本代表了 OpenAI 在语言模型发展过程中的不同探索和实验。

作为强化学习领域的先驱，虽然 OpenAI 略有一些拿锤子找钉子之嫌，或者说是使用强化学习技术对再次引入人工标注的事实做了一些粉饰包装。但是，无论如何 OpenAI 还是凭借着在强化学习领域的深厚积累，成功将其应用在语言模型的训练过程中，让ChatGPT在标注效率和对话生成效果方面取得了巨大突破。

在 ChatGPT 推出之后，随着大模型技术热度的不断攀升，人们对它的理解和使用也进入了新的阶段，这个新阶段是什么呢？下一节课，也就是本章的最后一节课中我将带你学习，敬请期待。

## 思考题

1. 由于 RLHF 只是单纯地根据已有的回答进行排序，是否会出现“自己吃自己”的循环，因为训练数据都是模型自己生成的，“近亲繁殖”的数据会不会对训练效果造成影响呢？
2. RLHF 和 SFT 的关系是什么？

这些问题留作你的课后思考。

恭喜完成我们第 16 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。
<div><strong>精选留言（3）</strong></div><ul>
<li><span>骨汤鸡蛋面</span> 👍（5） 💬（1）<p>sft model 相对于pretrained model 或base model，有一定的指令识别和意图识别能力，但是产生的内容可能不符合人类期待，所有有一个对齐的过程，因此，我们要如何优化sft model，尤其是符合人类期待的方式优化sft model，这就是reward model 所起的作用。从这个视角看，其实再多找一些“问题-回答对”来微调 sft model 应该也关系不大，但这样成本就比较高了，所以干脆造一个工具 reward model 给 sft model  的产出打分也是个不错的方向。就好比家长、老师会教你说话做事的正确答案，但教的总是有限的，也不一定对，到社会上没人教你，你只能通过别人的脸色、反应来判断做的好或不好。</p>2023-09-15</li><br/><li><span>lw</span> 👍（1） 💬（2）<p>webgpt是需要手动标注吗，这样工作量太大了</p>2023-10-13</li><br/><li><span>周晓英</span> 👍（3） 💬（1）<p>自我循环问题:

如果模型主要依赖自身生成的数据进行训练和优化，可能会出现自我循环（或称为自我确认偏误）的问题。这种情况下，模型可能会在一些错误或偏见上不断自我强化，而不是学习到新的、正确的或更有用的信息。
近亲繁殖的数据问题:

这种情况是指模型主要依赖与原始训练数据非常相似或重复的数据进行训练，可能会导致过拟合和泛化能力的降低。如果模型不断地只看到它自己生成的数据，它可能无法学习到新的知识或改进它的表现。
RLHF（Reinforcement Learning from Human Feedback）与 SFT（Supervised Fine-Tuning）:

RLHF 和 SFT 是两种不同的训练策略。RLHF 通常是通过从人类反馈中学习，以优化模型的性能，而 SFT 是通过有监督的方式，使用标签数据对模型进行微调。
在 RLHF 中，通常会收集人类对模型生成内容的反馈，然后利用这些反馈来指导模型的优化。而 SFT 则是基于一个预先标注的数据集来进行的，通常会有一个明确的损失函数来指导优化。
RLHF 更依赖于模型与环境的交互，而 SFT 更依赖于预先准备好的训练数据。
两者可以结合使用，以提高模型的性能和泛化能力。例如，可以首先使用 SFT 微调模型，然后使用 RLHF 进一步优化模型，或者相反。
为了解决自我循环和近亲繁殖的数据问题，通常需要确保模型在训练过程中有足够多样化的数据，并且有时会结合人类的反馈和外部数据来指导模型的训练和优化。</p>2023-10-02</li><br/>
</ul>