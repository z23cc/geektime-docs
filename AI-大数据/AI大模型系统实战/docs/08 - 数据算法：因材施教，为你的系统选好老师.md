你好，我是Tyler。

上节课我们学习了模型工程相关的知识，你掌握得如何？今天，我们来进一步学习更多的数据算法。

其实数据算法的本质是对人类智能的仿生，作为人类，我们进化出了神经反馈系统、大脑和各种感官。我们与生俱来的眼耳鼻舌身意这些高配传感器，还有大脑这个深度神经网络，让我们成为万物灵长，稍加学习就能适应外部世界。

不过作为AI系统造物主的你，就没那么轻松了。你需要先发挥自己的聪明才智，替AI系统去选择合适的输入数据，才能让你的AI系统足够智能。

你可能会问，为什么非得我们代劳，来完成选择数据的工作呢？我举个例子你就明白了。

假设，我们希望让一个智能体快速察觉到班主任的凝视，这个智能体很难自己判断哪些“数据”是重要的，它需要问遍身体的每一个传感器，才有可能找到传感信号和探测目标之间的关系。而我们只需要根据生活经验稍作思考，就知道最有价值的是听觉和视觉信号，只要将摄像头和麦克风信号供给模型，它就能判断是否有“危险”了。

在AI系统里也是同样的道理，我们利用人类的经验把数据分成了主体数据、客体数据和环境数据这三类数据。比如在无人驾驶和车联网系统中，这三类数据对应的是车辆数据、交通流数据和环境数据。

再比如我们熟悉的AIRC系统，它利用数据包括用户特征、物品特征和场景特征。这些数据都遵循了主体、客体和环境这种划分方式。

不过，给数据分类只是一小步。数据类别不同，学习方式和重点也各不相同，为了让AI系统早日成为学霸，我们还需要为它定制更合理的学习思路，说得更专业一些，就是根据不同任务的数据特点，选择最合适的数据建模方式。

## 用户特征

我们知道几乎所有商业公司，都会强调一条企业文化，那就是客户第一（Consumer Obsession），因为客户买账是商业模式延续下去的前提。所以一开始，我们先将注意力放在对用户特征的理解上，因为你所有的工作都是为了“投其所好”。

如果你不知道从何下手，你不妨和我刚才一样，从日常更容易观察到的线下场景来推演。

现在闭上眼睛（仅限听众），我们进入情景——假如你是某商场美妆区负责人，领导和你说，现在美妆区的客流量越来越少，你去发传单招揽一些客户回来。

于是，你拿着传单，站在商场门前的广场上，在川流不息的人群中风中凌乱。你发现每个人都在躲避你的眼神，所以，你意识到自己只能将传单发给有限的人，你该如何选择你的重点对象？

我猜你大概率会选年轻且爱打扮的女生，因为她才更有可能成为你产品的主要客户。现在睁开眼睛吧（仅限听众），我们回到线上的场景，你该如何发现这些对象呢？

答案是用户画像。用户画像数据被视为每个产品最宝贵的资源，同时也是AI系统中价值最高的数据。因此，AI系统必须充分利用这些有限的数据资源，以提供更有价值的信息。

### 数据管理平台（DMP）

各大科技公司为了更充分地利用这些数据，一般采用专门的数据管理平台（Data Management Platform，DMP）对用户特征进行管理，管理的数据包括三类。

1. 第一方数据，用户使用产品时产生的信息，其中包括用户个人注册信息，人群特征数据等等。
2. 第二方数据，是团队内部产品或者业务伙伴提供的数据。
3. 第三方数据，数据供应商提供的数据，例如从BlueKai、秒针这类地方购买的数据。

DMP中会将用户基本信息（例如性别、年龄、职业），使用习惯和行为记录（例如点击、购买、收藏）等信息综合起来，生成用户画像。这看似是一个简单过程，其实蕴藏了很多技术难题。为了应对这些难题，我们的系统需要建设后面这两种能力。

- 第一种是对数据做“身份对齐”的能力，比如对同一人在多设备、多账号产生的数据做身份识别。
- 第二种则是挖掘某个人群的“潜在用户”的能力，比如识别符合某个品牌产品调性的用户。

不要小瞧这两个能力，它们不仅能够大幅提高数据质量，提升你的模型效果。还是你系统的“印钞机”，可以大幅提高AI系统的营销收入。

这里提一句题外话，营销能力不仅是 AIRC 系统商业逻辑得以延续的必要条件，也是 AIGC 系统的必由之路。

### 人群扩展算法（Look-alike）

为了具备这两个能力，AI系统中的一个重要武器Lookalike就派上用场了。它有三大作用。

1. 挖掘潜在高净值用户。比如识别出与“保时捷”车主相似的用户，扩大推广活动的覆盖范围。
2. 提高风控能力。比如通过用户相似行为和特征，发现黑灰产作案团伙，识别出潜在欺诈活动。
3. 提高冷启动推荐效果。比如利用相似人的特征代表未知新用户，做出更准确的推荐决策。

实现 Lookalike，完全可以用上节课你所学到的 DeepWalk 算法。但是这节课我们来学习一个新的算法，也就是上节课预告的 GraphSAGE，当时我们提过，和两阶段的 DeepWalk 算法相比，GraphSAGE 是一个端到端的方案，能更好地保留图结构中的信息。

![](https://static001.geekbang.org/resource/image/a2/7f/a2c03702fb58a1fb5987abb8fa6cbb7f.jpg?wh=3900x1817)

我们对照前面的图解，来梳理一下GraphSAGE的具体细节。

1. 建图：将用户作为节点，如果用户之间若存在相同属性，则增加一条边。
2. 采集：随机选一个幸运节点，向它的各个方向走两步，路径会形成一棵树，如左图。你需要重复此过程，采出若干棵树，留作备用。
3. 聚合：随机取一棵树，将第三层节点和向第二层聚合（如中图），生成第二层节点表征。之后再对第二层和树根做同样的操作，每棵树都如法炮制。
4. 训练：将树根原来的表征与上一步的聚合表征拼接起来，以此来保留图中结构信息（如右图），然后与一个参数相乘，就得到了树根的融合表征。

这里先简单提一下，这些步骤背后的思想其实和大模型技术 Transformer 中的位置编码很像，到时候我会再和你详细讲解。这里我们回到 GraphSage 算法。

因为每个节点都有可能成为树根，所以每个节点都能获取自己的融合表征。算法训练的过程，其实就是在调整第四步中那个参数，使得每个人对应节点的融合表征在高维空间中的距离，和真实世界中的距离尽量一致。本质上就是让这个参数来表达高维空间映射的投影规则。

完成这些步骤之后，Lookalike的最后一步就很简单了，只要将高维空间中与高净值用户距离最接近的那群人作为潜在用户就好了。其实多设备、多场景的身份对齐也是同样的道理，我们只要找到足够相近的两个人，将他们近似识别成同一人就可以（真实场景用法更复杂些，不过是同样的道理）。

当然这其中有一个鸭子测试的假设，那就是如果它看起来像鸭子、走路像鸭子、叫声像鸭子，它就是鸭子。这里我为你准备了一张示意图，你可以点开文稿去看看。

![](https://static001.geekbang.org/resource/image/26/d0/26d5233e83e718f5d9839fd7e03275d0.jpg?wh=3900x2194)

## 物品特征

解决了用户特征的问题，接下来学习物品特征的处理方法。物品是AI系统商业目标的载体，它几乎与系统中的所有实体都有交互，可以建立丰富的连接。而这恰恰是知识图谱最喜欢的，所以AI系统往往会围绕物品的“朋友圈”来构建知识图谱。

### 知识图谱（KG）

知识图谱起源于上节课三大派别中的符号主义派，是一种用于表示和组织知识的图形结构，由多个“实体-关系-实体”的三元组所组成。

相较于其他图结构，知识图谱的边上携带了更丰富的信息。它不仅在 AIRC 系统中发挥了重要的作用，同样也是 AIGC 系统中，提示语工程的重要组成部分。

今天我们重点关注构建知识图谱的三个主要步骤：知识抽取、知识融合和知识加工。因为这三个步骤是知识图谱技术的核心特征，值得我们多花时间先搞清楚。

至于在每个步骤中所涉及的算法则纷繁复杂，相对而言，并不是那么重要。你已经学习的各种算法几乎都可以应用于知识图谱的构建过程中。

![](https://static001.geekbang.org/resource/image/05/c4/05556a0124b8ffc8125yy68772491ac4.jpg?wh=3900x2047)

首先是知识抽取，这是“**拿知识**”的过程。目标是从文本、网页、数据库等不同的地方提取数据，识别并提取出实体、属性和它们之间的关系，以便构建知识图谱。拿知识的技术包括识别实体、提取关系和属性等。

然后是知识融合，这是“**合知识**”的过程。目标是解决数据不一致和冲突的问题，将来自不同数据源的知识进行统一和一致化，从而生成更准确和完整的知识图谱。合知识的技术包括对齐实体、对齐关系和解决冲突等。

最后是知识加工，这是“**学知识**”的过程。目标是使知识图谱更可用和可靠，方便后续的知识检索、推理和应用。学知识的技术包括清洗数据、补充数据和进行知识推理等。

清楚了各个步骤的定位之后，我再带你学习一个知识图谱应用的例子，帮你建立一些直观的认识。在AIRC系统中，我们常常利用知识图谱来对物品、相关信息和用户之间的关系进行建模，从而获取“物品地图”中的各个兴趣点（Point of Interest，POI），挖掘用户各个兴趣中潜在的关联内容。

![](https://static001.geekbang.org/resource/image/13/25/136d33cb0ed14d36a76f1a81fe253825.jpg?wh=3900x1829)

如上图，如果某个用户对美白感兴趣，你可以基于 POI 实时收集相关的物料，通过组合它们来自动形成下图相应的商品创意。这种方法往往可以生成更符合用户喜好的产品介绍，更好地挖掘用户的需求，获得更高的点击率。

![](https://static001.geekbang.org/resource/image/f7/64/f7933700f3570a7ddd9d72b0e192d564.jpg?wh=3900x2104 "生成的产品使用前后对比图")

## 场景特征

最后，让我们来看一下场景特征的特征。场景特征来自于每一次流量请求中客户端提供的信息，被用于刻画用户触达应用时的全景信息，包括后面这些信息。

- 应用程序所处的界面（应用、页面、媒体位等）
- 用户的设备信息（信号强度、手机型号、电池电量等）
- 所在地点信息（城市、气温、邮编等）
- …

在处理场景特征时，我们需要将与用户长期习惯相关的数据放在用户画像中。那些随着场景变化频率较高的数据应该放在场景特征中。

### 实时特征

我想进一步说明的是，场景特征的最大价值，在于它在时间维度上的区分性和敏感性。例如“用户最近30分钟的商品点击数量”或“用户最近1小时浏览商品数量”这些实时特征都是非常重要的，它们会对推荐结果产生很大影响。为什么这么说呢？

这是因为用户画像和物料特征数据相对稳定，更新频率不高，如果不增加场景特征，模型的输入值很可能在一段时间内没有任何变化。因此，场景特征的输入可以让模型变得更加敏感。

## 小结

今天的学习到这里告一段落，我们做一个回顾。

在这节课上，我们学习了如何根据具体的业务场景选择适合AI系统的数据。具体内容包括这三个方面。

1.用户画像的构成，以及相似人群扩展的技术。  
2.物品特征的构成，知识图谱技术的关键步骤。  
3.场景特征的定位，以及实时特征的独特价值。

除此之外，我还是想提醒你，对生活的观察和体验是数据智能工作中很重要的一个部分，只有长时间和数据“泡在一起”，才能更深入地理解用户使用系统的习惯，清楚知道我们有哪些能利用的原材料，得到最符合直觉的重要特征。

更值得一提的是，这节课讲到的图神经网络、知识图谱和跨模态预训练模型技术都是AI大模型系统最重要的前序知识，一定要熟练掌握。

在下节课中，我会带你结合这一章的全部知识，构建完整的AI系统，敬请期待！

## 课后思考

为了更好地巩固今天所学内容，请你尝试回答后面的问题。

你认为ChatGPT中的主体、客体和环境数据分别是什么？如何基于用户、内容和场景的特征来优化ChatGPT的内容生成质量？答出大概思路即可。  
如何区分场景特征和用户特征，比如“用户最近30分钟内，观看的运动类视频数量”是哪类特征？  
如果让你选择三个算法，放入知识图谱的三个步骤中，你会选哪三个？

恭喜你完成我们第 8 次打卡学习，期待你在留言区和我交流互动。也欢迎你把这节课分享给身边朋友，和 TA 一起学习进步。
<div><strong>精选留言（4）</strong></div><ul>
<li><span>GAC·DU</span> 👍（18） 💬（1）<p>主体在对话中，主体通常指的是模型或参与对话的用户。
客体指的是主体在对话中讨论的主题、事物或概念。在对话中，主体通常会涉及一个或多个客体，以其问题、回答或讨论内容来定义对话的方向和主题。
环境数据是在对话过程中提供背景信息和上下文的数据。这些数据可能包括对话历史、之前的提问、回答，以及在对话中引入的其他文本。环境数据有助于理解对话的脉络，确保主体对当前对话情境有准确的理解。
用户特征
提供明确的指导，例如要求ChatGPT以特定的语气、风格或格式回答问题。这有助于模型生成更符合用户预期的内容。在对话中提供反馈，指出哪些回答是有用的，哪些回答需要改进。这有助于模型逐步调整生成内容。参考之前的对话历史，以便模型可以更好地理解用户的偏好、兴趣和问题，有助于生成更连贯和相关的内容。
内容特征
提供准确详细的信息，以便模型可以从中获取更具信息价值的内容。在问题中提供清晰的背景和上下文，以确保模型理解问题的背景，从而生成更恰当的回答。避免使用模棱两可的语句，特别是当涉及到多个可能的含义时，这可以减少模型误解意图的可能性。
场景特征
如果对话涉及特定领域的知识，提供相关的专业术语和背景信息，以确保模型在生成内容时具有正确的专业性。考虑到文化差异和语境，以便模型可以生成适合特定文化和背景的内容，避免可能的误解或冒犯。如果需要特定风格的回答，可以明确指示模型使用哪种语气、语言风格或情感色彩。

场景特征更关注于对话的背景和环境，而用户特征更关注于对话中的用户需求和个性化，场景特征通常包括对话主题、领域和文化等，而用户特征包括用户指令、历史记录和风格偏好等。而“用户最近 30 分钟内，观看的运动类视频数量”属于和时间相关的用户行为特征。

三种算法包括拿知识的实体识别算法，合知识的实体关系抽取算法，学知识的图数据学习算法。

</p>2023-08-28</li><br/><li><span>周晓英</span> 👍（3） 💬（0）<p>在 ChatGPT 的设计中，可以将 &quot;主体&quot; 理解为模型自身，&quot;客体&quot; 理解为与模型交互的用户，而 &quot;环境&quot; 则是交互的上下文环境，包括但不限于用户的输入、对话的历史记录以及外部信息等。

1. 基于用户、内容和场景的特征优化内容生成:
用户特征: 用户的行为、偏好、历史交互记录等。例如，用户的年龄、性别、喜好、之前的查询等。
内容特征: 输入的内容、模型的回复、外部知识源等。例如，文本的复杂度、情感、主题等。
场景特征: 对话的场景、时间、地点等。例如，对话的目的（如咨询、购物、娱乐等）、时间、地点等。
优化步骤：
特征工程：首先需要进行特征工程，提取和构建与用户、内容和场景相关的特征。
模型定制：根据这些特征定制模型结构，例如，通过添加特征嵌入层或特征条件层来整合这些特征。
训练和调优：使用带有这些特征的数据进行模型训练和调优，以改善内容生成的质量。
2. 特征分类:
“用户最近30分钟内，观看的运动类视频数量”这个特征应该是一个用户特征，因为它反映了用户的实时行为。
3. 知识图谱构建三个步骤中的算法选择:
构建知识图谱通常包括三个基本步骤：实体识别、关系抽取和知识融合。

实体识别: 可以选择条件随机场 (CRF) 算法，它是一种有效的序列标注算法，常用于实体识别任务。
关系抽取: 卷积神经网络 (CNN) 或 循环神经网络 (RNN) 可以被用于关系抽取，通过学习文本中的模式来识别实体间的关系。
知识融合: 同源性检测算法 (如 SimRank) 可以用于知识融合，以识别和合并来自不同源的重复或相似的知识。</p>2023-10-02</li><br/><li><span>顾琪瑶</span> 👍（1） 💬（0）<p>1. 数据:
1.1 主体: GPT本身, 或者说是模型自身
1.2 客体: 不同的用户
1.3 环境: 不同用户的上下问数据
2. 质量:
2.1 用户: 
2.1.1 时区或位置(IP): 由于不同地区的发展程度不一致, 相同的问题更合适的答案应该也是不同的
2.2.2 特征: 根据提问内容检索提出相似问题的人群, 如更倾向于科普类, 或专业类
2.2 内容: 可以考虑在响应用户回答后, 再选择几个相似度高的答案, 作为备选, 拓展用户的提问思路引导用户
2.3 场景: 如果是非通用型大语言模型, 可以在应用层就提示模型, 当前是属于什么场景下的提问, 如购物, 检测等, 提高模型的准确度
3. 区分: 更偏向于用户特征
3.1 场景特征: 可适用于观看视频的场景太多, 几乎任何和平地区且有网络的地区都可以</p>2023-08-28</li><br/><li><span>Seachal</span> 👍（0） 💬（0）<p>学完这课，对数据在AI系统里的应用有了点新体会。用户画像、物品特征和场景特征，这三块都得搞明白。用户画像嘛，就是得挖掘用户数据，用Look-alike算法找找相似人群。物品特征，得靠知识图谱，把物品都连起来。场景特征，就是得结合对话的上下文，理解用户的真实需求。

还有啊，别忘了多观察生活，多和数据打交道，这样才能真正理解用户习惯，找到关键特征。图神经网络、知识图谱、跨模态预训练模型这些技术，都是AI大模型的基石，得好好学。

另外，特征优化这块也挺有意思。用户特征、内容特征、场景特征，都得考虑进去，这样才能生成更符合用户预期的内容。特征工程、模型定制、训练和调优，这些步骤都不能少。

说到算法，实体识别可以用CRF，关系抽取CNN或RNN都行，知识融合可以试试SimRank。这些都是构建知识图谱的好帮手。</p>2024-11-23</li><br/>
</ul>