你好，我是金伟。

通过之前的项目和课程我相信你已经了解到，大模型最根本的价值就是替代人的工作，而在训练大模型的过程中，还有很多操作是需要人工的。比如数据工程里的数据质量检查、模型评估阶段的人工测评等。

我们之前提出过类似的问题：这些人工操作真的离不开人吗？可以用大模型替代吗？

这节课我们就开始探讨这个问题。

关于这个问题，我的答案是**使用评估专家模型技术**，让这些过程实现完全无人化。你可以把评估专家模型看做是大模型项目里技术难度最高的部分，它的思想也完全适用于你将来利用大模型解决其他行业问题。

# ChatGPT的奖励模型

在开始说评估专家模型到底是什么之前，我想先带你回顾一下ChatGPT在RLHF强化学习阶段里奖励模型的概念。通过这个概念的回顾，我们能搞清楚评估专家模型的来龙去脉。

先理解**第一个层次**，Reward Modeling奖励模型最终实现了全自动地评估ChatGPT的输出并为其打分，以便和人类价值观对齐。这个自动化的evaluate评估 - 打分能力实际上已经等同于人类的能力。

我们举一个例子，实际上大模型输出的时候，如果要两个结果2选1，有的时候人都不一定能选得很好，因为每个人其实评估标准并不一样，但Reward Modeling奖励模型需要做出良好的选择符合大多数人的期望。

再说**第二个层次**，要实现这个奖励模型，实际上还是需要人类业务专家先评估语料的质量，真实过程是用小样本的人类专家的数据标注，模型经过训练后获得全功能的输出评估能力。

这个过程就是RLHF阶段的step 2 ，如下图所示，也就是奖励模型的实现过程。

![图片](https://static001.geekbang.org/resource/image/70/91/70ffddd48a45f3b7ce352de5568c7491.png?wh=1920x1132)

从图中可见，奖励模型是怎么获得结果评估的能力呢？是人类评估专家提前对结果做A、B、C、D的评估和排序，最终通过单独的模型训练让奖励模型获得评估能力。我们也可以说，这个奖励模型就是一个评估专家模型。其作用是评估多个大模型输出的质量好坏。

好了，现在再回过头看一看，实际上ChatGPT最后的人类对齐阶段的结果评估已经是对事物评估最难的一个级别了，毕竟ChatGPT的输入输出可是千变万化的。

相比较而言，在数据工程里对数据质量的评估，在模型测评里对某类问题的结果评估则比RLHF里的评估专家要简单。所以，**这样的评估任务完全可以借鉴ChatGPT奖励模型的思路。**

# 评估专家模型实例

我们一起来思考一个问题，不管是对数据质量的判断，对模型结果的判断，实际上都是一种数据评估能力，而GPT的奖励模型就是一种典型的评估能力，也就是对结果好坏做评估排序。数据质量的判断则可以简化为采用-丢弃这样的二值化评估，对特定的问题的模型输出结果的判断可以简化为评估比对前后两次结果的好坏。从原理上来说，都可以被GPT奖励模型的结果排序评估覆盖。

具备这些能力的评估模型就可以认为是一个评估专家模型，那具体要怎么实现评估专家模型呢?

## ChatGPT论文

要自己实现一个评估专家，可以分为两个层次思考，**首先在战略设计上可以直接参考ChatGPT的论文。**实际上就是RLHF强化学习的三个阶段。

![图片](https://static001.geekbang.org/resource/image/fe/d3/fe152935a6cf3e0b131e2e0782dd78d3.png?wh=1920x1132)

在第一阶段，预训练的大语言模型首先使用人类标注的高质量数据集进行监督微调，第二阶段就是之前分析的基于人类专家数据标注的奖励模型训练，第三阶段是采用PPO算法让大模型在奖励模型评分的基础上保持稳定的数据质量。

实际上，其他开源的大模型都会借鉴这个总体设计的思想。

然而，ChatGPT的技术报告不包含技术细节，大方向有参考性，但是可操作性低。其中模型结构、反馈结构、使用方法有很高的参考价值。总之，ChatGPT的论文只能作为一种指导思想，我这里的目的也不是分析论文，我们要从一个更落地的评估专家模型入手，也就是我**们思考的第二个层次，从战术上找一个可对标的开源评估专家。**

我这里以Llama模型里的一个专门用于内容安全性评估的模型: Llama Guard 3‍模型为例。

## Llama Guard 3‍模型

不知道你有没有这样的经验，当你向大模型的提问带有某些风险的时候，大模型的输出会明确提示这个风险，甚至你让大模型输出某些不文明用语都会被阻止。这其实要归功于Llama Guard 3‍这类防止滥用防护评价模型。

![图片](https://static001.geekbang.org/resource/image/a7/75/a7cf4cbd5c153954933a20b2e54ba375.png?wh=1920x1005)

当用户向Llama 3提问的时候，每次Llama 3都会让Llama Guard 3‍来做安全评估，其实现过程也是一次大模型调用。

你可以看看每次安全评估的提示词模版。

![图片](https://static001.geekbang.org/resource/image/93/43/931976e46cfde2a056d419b69d6c6f43.png?wh=1682x1202)  
显然，Llama Guard 3‍接收的提示词是一个典型的大模型提示词，我们只要关注最核心的几个信息。

{{unsafe\_categories}}表示要让Llama Guard 3‍识别的具体安全分类。具体安全分类有14种。

![图片](https://static001.geekbang.org/resource/image/f3/d2/f3a9a070aaab2dd3035a2ba92553bdd2.png?wh=1696x954)

提示词中的 user\_message 和 model\_answer 就是Llama 3和用户之间的问答上下文。有了这样一个提示词，Llama Guard 3‍ 最终会给出一个二值的输出，也就是 `safe` 或 `unsafe`，如果输出是 `unsafe` 则还会给出具体的安全分类。下面是一个具体的输出例子，你一看就知道了。

![图片](https://static001.geekbang.org/resource/image/8a/00/8a27b383553338d9f2b54bfb0c2daa00.png?wh=808x400)

我们可以把Llama Guard 3‍ 看做一个典型的评估专家模型，他的作用是给输出内容做安全评估和分类。在具体实现上，Llama Guard 3‍ 其实就是一个经过微调的大模型，它是基于Llama 3.1 8B小模型微调训练的。

![图片](https://static001.geekbang.org/resource/image/29/71/295f0a9df5777aa8a82245aa315dd871.png?wh=1920x851)

我们看完Llama Guard 3‍ 之后，对评估专家模型的基本模式已经有认识了，现在需要思考一个问题，如何构建这样的专家模型呢？其实核心的工作仍然是数据集的设计。

# 评估专家模型设计

先明确两点，其一是评估专家的模型训练方法就是大模型微调训练，这个之前课程已经详细讨论过。其二需要为特定的评估专家准备特定的数据集。核心思想是人类整理数据集，让大模型学会人类的评估方法。

那什么场景下用什么数据集呢？这个才是核心问题。如果总结一下，其实一共就3类评估专家，分别适合3种场景。接下来我带你一一细分。

## 数据集设计

我们先从数据集格式出发比对不同数据集的异同，最后自然可以推导出什么场景适用哪个数据集格式。

![图片](https://static001.geekbang.org/resource/image/6b/e4/6b08500301179326879293de00d088e4.png?wh=1920x663)

我想这张图已经比较清楚地表明了这3类数据集的区别，KTO的输出是绝对的0-1这样二值判断，RLHF则是类似ChatGPT输出的排序评估，DPO也很简单，是对A-B两种结果的倾向性评估。

RLHF输出是多个输出对比排序，也就是ChatGPT奖励模型的数据格式，自不必多说。那KTO训练和DPO训练有什么不同？

训练过程中，KTO方法需要对输出结果进行二元判断，也就是输出：符合预期/不符合预期，所以其收集的数据为Prompt+Chosen或Rejected；DPO训练依赖人类反馈，需要对输出结果进行偏好性判断，输出是两个结果的倾向程度，所以其收集的数据为Prompt+Chosen和Rejected。

下面是KTO和DPO的具体数据例子。

![图片](https://static001.geekbang.org/resource/image/93/be/933d19fbea14577482fee57aca3339be.png?wh=1920x1019)  
实际上，KTO和DPO的数据集都表示在一个提示词下，用户和AI问答数据的优选策略，`chosen` 表示优选，`rejected` 表示次优或丢弃。

接下来更重要的问题是搞清楚KTO和DPO适用的场景。

**KTO（Knowledge Trained Optimization）**更适合客观问题，因为它依赖于对输出结果进行明确的二元判断（符合预期或不符合预期）。这意味着它的应用场景主要是有明确标准答案的领域，例如数学问题、技术操作或事实陈述等。在这些场景中，输出结果要么正确，要么错误，没有太多的主观性。

举一个简单的例子，现在假设我们要评估一个数值，正数留下，负数丢弃，你已经用KTO设计了一个基于正数和负数的奖励模型，接下来可以通过以下方式来设计提示词（Prompt）让模型对输入数字进行判断。

提示词模版如下。

```plain
请根据以下输入的数字判断其是否符合数据质量标准。返回一个评分，范围从0到1，表示数字的质量。
输入数字：X
```

提示词例子如下。

```plain
请对以下输入的数字进行评分，表示该数字是否符合标准。
输入数字：100
```

此时奖励模型会返回高分，例如 `0.95`，因为100是正数，符合之前的KTO标准。如果输入数字是负数，比如：

```plain
输入数字：-4
```

此时奖励模型会返回较低分数，比如 `0.05`，因为负数被判定为"不符合标准"。

再进一步看看DPO的适用场景。

**DPO（Direct Preference Optimization）** 则更加适合主观问题，因为它依赖人类的偏好进行相对判断（两个输出的倾向性）。在这类问题中，可能不存在绝对的“正确答案”，而是更多取决于人类的判断和偏好，例如内容创作、文学作品、设计审美等。在这些场景中，输出的好坏并不取决于客观标准，而是根据用户的偏好来选择更优的输出。

还是用刚才的简单数字场景来举例，假设任务是对两个候选值（即 `x` 和 `y`）进行偏好性判断，偏向是正数。根据需求，你需要构建一个 prompt 来让奖励模型在两个数字之间做出选择。

提示词构建如下。

```plain
请根据数据质量标准，对以下两个数字进行偏好性判断，并返回哪个更符合标准。


数字x：1
数字y：-4
```

此时模型会比较这两个数字，基于你训练的数据，`1` 会被认为符合标准（chosen），而 `-4` 会被判定为不符合标准（rejected），因此模型可能会给出“选择数字1”。

DPO 的重点是比较两个输出的相对好坏，所以每次输入两个候选数字，让奖励模型进行相对判断。总结一下，KTO非常适用于有明确客观标准的问题，而DPO则更适用于涉及主观判断和个人偏好的问题。

最后可以整理出3个评估专家的场景和数据集的应对关系。

![图片](https://static001.geekbang.org/resource/image/50/44/50360957d84998c30d3dc9802c013444.png?wh=1920x688)

## 人类专家的价值

当然，数据集仍然要靠人类专家来准备和标注，和其他大模型数据集一样，人类准备的数据集实际上是对大模型起到一个引导的作用。

具体到评估专家模型，就是对评分、分类、排序能力的引导。

![图片](https://static001.geekbang.org/resource/image/9d/06/9d7b1c6e4c2a08e03a2dcd4083f22706.png?wh=1920x853)

## 工程经验

我们以数字孪生中的数据质量评估为例，看看实际工程中我们是如何根据需求设计数据集的。

先说结论，在数据孪生的数据质量判断中，需要评估专家按照业务使用场景对数据进行快速精选（也就是二义分类KTO），只需要用小数据集就可以做得很好。

当然，我们也尝试过打分制，但是很难泛化。在我们的经验下，打分制整体不如二义分类。当然，有比较多精准数据，并且有比较强烈的需求下可以尝试训练打分制模型。这也意味着需要比较大的数据集。

数据孪生的数据质量判断场景下，二义分类比较好，离散度高，有比较可靠的参考经验。实际上这跟数据的情况有很大的关系，所谓数据质量判断，实际上就是把“坏数据”评估出来，那什么是“坏数据”呢?

“坏数据”通常是指不符合物理现实或预期结果的输出，可能有数据不准确、数据不完整、超出合理范围、不一致的时序、违反物理规律这几个原因。

在这种应用场景下，KTO 作为优化方法，会根据这些预定义的质量标准（如数据的准确性、完整性、合理范围等）来判断数据是否为“坏数据”。而且判断的结果不是基于偏好，而是明确的“对”或“错”，“适合”或“不适合”的判断。

最后还要说一个工程经验，做评估专家模型的时候，无论如何，不要重头开始训练一个新模型，而是基于预训练模型如Llama 3 ，ChatGLM 3来训练，这样才能更好利用这些模型已经内化的能力。

# 小结

仔细想想，ChatGPT在RLHF强化学习阶段的奖励模型是一个比大模型本身更难的模型，大模型的基本原理只是根据概率预测下一个字符，而奖励模型则要对千变万化的大模型输入输出做出类似人类水平的评估。

奖励模型就是我们说的评估专家模型，其数据格式是对多个结果的评估排序，类似C&gt;D&gt;A&gt;B。评估专家的另外两个数据形式则分别是2元数值的倾向性格式DPO，类似B&gt;A，以及2元数值的1-0评估，也就是KTO数据格式。

类似Llama Guard 3‍ 的内容安全评估模型就是借鉴了奖励模型的设计思路。需要注意的是，Llama Guard 3‍ 是基于Llama 3.1 8B模型微调训练的，交互形式仍然是大模型标准的Prompt交互。

那怎么根据场景选择合适的评估模型呢？以数据孪生的数据质量检测为例，因为这个场景是把数据中的好数据，坏数据挑出来，是一个二值化的对错判断，因此适合KTO的评估专家模型。

![图片](https://static001.geekbang.org/resource/image/50/44/50360957d84998c30d3dc9802c013444.png?wh=1920x688)

那怎么实现数字孪生的数据质量评估专家模型呢，我们下节课会重点说一说实现过程。

# 思考题

为什么Llama Guard 3‍ 防止滥用防护评价模型只用8b大模型这样的小参数模型做基础呢？请你思考一下。

欢迎你在留言区和我交流。如果觉得有所收获，也可以把课程分享给更多的朋友一起学习。我们下节课见！

[&gt;&gt;戳此加入课程交流群](https://jsj.top/f/hm26hN)
<div><strong>精选留言（1）</strong></div><ul>
<li><span>石云升</span> 👍（1） 💬（1）<p>防护评价通常需要快速响应。8B 模型可以提供更低的延迟，同时，其部署也简单。 </p>2024-09-23</li><br/>
</ul>