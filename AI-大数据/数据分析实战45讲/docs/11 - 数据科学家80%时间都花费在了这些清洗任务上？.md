我们在上一节中讲了数据采集，以及相关的工具使用，但做完数据采集就可以直接进行挖掘了吗？肯定不是的。

就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的20%，剩下80%的时间都是在做准备，比如买菜、择菜、洗菜等等。

在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据“**整理裁剪**”成自己想要或需要的样子。

为什么呢？因为我们采集到的数据往往有很多问题。

我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？

![](https://static001.geekbang.org/resource/image/5e/23/5e69b73b96c0d824240ac8035fe69723.png?wh=522%2A334)

你刚看到这些数据可能会比较懵，因为这些数据缺少标注。

我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。

我简单解释下这些数据代表的含义。

这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。

列坐标中，第0列代表的是序号，第1列代表的会员的姓名，第2列代表年龄，第3列代表体重，第4~6列代表男性会员的三围尺寸，第7~9列代表女性会员的三围尺寸。

了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么“脏乱差”啊，有很多值是空的（NaN），还有空行的情况。

是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如100个指标，数据量通常都是超过TB、EB级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。

我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，**好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了80%**。

## 数据质量的准则

在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？

准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。有研究说一个人的短期记忆，最多可以记住7条内容或信息，超过7条就记不住了。而数据清洗要解决的问题，远不止7条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？

在这里，我将数据清洗规则总结为以下4个关键点，统一起来叫“**完全合一**”，下面我来解释下。

1. **完**整性：单条数据是否存在空值，统计的字段是否完善。
2. **全**面性：观察某一列的全部数值，比如在Excel表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
3. **合**法性：数据的类型、内容、大小的合法性。比如数据中存在非ASCII字符，性别存在了未知，年龄超过了150岁等。
4. 唯**一**性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

在很多数据挖掘的教学中，数据准则通常会列出来7~8项，在这里我们归类成了“完全合一”4项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得**数据标准、干净、连续**，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。

## 清洗数据，一一击破

了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。

这里你就需要Python的Pandas工具了。这个工具我们之前介绍过。它是基于NumPy的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。

这里我补充说明一下，如果你对Python还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下Python是很有必要的。Python拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以Pandas为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。

下面，我们就依照“完全合一”的准则，使用Pandas来进行清洗。

**1. 完整性**

**问题1：缺失值**

在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：

- 删除：删除数据缺失的记录；
- 均值：使用当前列的均值；
- 高频：使用当前列出现频率最高的数据。

比如我们想对df\[‘Age’]中缺失的数值用平均年龄进行填充，可以这样写：

```
df['Age'].fillna(df['Age'].mean(), inplace=True)
```

如果我们用最高频的数据进行填充，可以先通过value\_counts获取Age字段最高频次age\_maxf，然后再对Age字段中缺失的数据用age\_maxf进行填充：

```
age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True)
```

**问题2：空行**

我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read\_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。

```
# 删除全空的行
df.dropna(how='all',inplace=True) 
```

**2. 全面性**

**问题：列数据的单位不统一**

观察weight列的数值，我们能发现weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。

这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）：

```
# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
	# 截取从头开始到倒数第三个字符之前，即去掉lbs。
	weight = int(float(lbs_row['weight'][:-3])/2.2)
	df.at[i,'weight'] = '{}kgs'.format(weight) 
```

**3. 合理性**

**问题：非ASCII字符**

我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非ASCII问题，这里我们使用删除方法：

```
# 删除非 ASCII 字符
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

**4. 唯一性**

**问题1：一列有多个参数**

在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname两个字段。我们使用Python的split方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。

```
# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)
```

**问题2：重复数据**

我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop\_duplicates() 来删除重复数据。

```
# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)
```

这样，我们就将上面案例中的会员数据进行了清理，来看看清理之后的数据结果。怎么样？是不是又干净又标准？

![](https://static001.geekbang.org/resource/image/71/fe/71001f8efb2692e77fa1285bcf7f91fe.png?wh=1441%2A398)

## 养成数据审核的习惯

现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有6处错误。所以我们常说，现实世界的数据是“肮脏的”，需要清洗。

第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说**没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。**

当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有“数据审核”的“职业病”。这就好比编辑非常在意文章中的错别字、语法一样。

数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多7秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。

刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。

好了，最后我们来总结下今天的内容，你都收获了什么？

![](https://static001.geekbang.org/resource/image/87/dd/87c0f81b493e99e715e9129cd3134bdd.jpg?wh=2249%2A1724)

学习完今天的内容后，给你留个小作业吧。下面是一个美食数据，如果你拿到下面的数据，按照我们今天讲的准则，你能找到几点问题？如果你来清洗这些数据，你打算怎样清洗呢？

![](https://static001.geekbang.org/resource/image/34/fd/347fcfd86d83ff92923cbd01707a35fd.png?wh=523%2A355)

欢迎在留言区写下你的思考，如果你对今天“数据清洗”的内容还有疑问，也欢迎留言和我讨论。也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事。
<div><strong>精选留言（15）</strong></div><ul>
<li><span>third</span> 👍（47） 💬（3）<p>自己不知道有没有什么好的工具，所以就把图片上一个一个敲进去了。
数据.csv格式
链接：https:&#47;&#47;pan.baidu.com&#47;s&#47;1jNnUpntrlxFSubmna3HtXw 
提取码：e9hc</p>2019-02-05</li><br/><li><span>wonderland</span> 👍（46） 💬（5）<p>一、首先按照所讲的数据质量准则，数据存在的问题有：
1.&quot;完整性&quot;问题：数据有缺失，在ounces列的第三行存在缺失值
处理办法：可以用该列的平均值来填充此缺失值
2.“全面性”问题：food列的值大小写不统一
处理办法：统一改为小写
3.“合理性”问题：某一行的ounces值出现负值
处理办法：将该条数据记录删除
4.“唯一性”问题：food列大小写统一后会出现同名现象，
处理办法：需要将food列和animal列值均相同的数据记录进行合并到同一天记录中国</p>2019-01-10</li><br/><li><span>滢</span> 👍（38） 💬（6）<p>原始数据链接：https:&#47;&#47;github.com&#47;onlyAngelia&#47;Read-Mark&#47;blob&#47;master&#47;数据分析&#47;geekTime&#47;data&#47;accountMessage.xlsx    （课程中讲解原始数据-点击view Raw即可下载）
课后练习原始数据: https:&#47;&#47;github.com&#47;onlyAngelia&#47;Read-Mark&#47;blob&#47;master&#47;数据分析&#47;geekTime&#47;data&#47;foodInformation.xlsx （点击View Raw下载）</p>2019-04-11</li><br/><li><span>Geek_5hxn61</span> 👍（27） 💬（2）<p>import pandas as pd
&quot;&quot;&quot;利用Pandas清洗美食数据&quot;&quot;&quot;

# 读取csv文件
df = pd.read_csv(&quot;c.csv&quot;)

df[&#39;food&#39;] = df[&#39;food&#39;].str.lower()  # 统一为小写字母
df.dropna(inplace=True)  # 删除数据缺失的记录
df[&#39;ounces&#39;] = df[&#39;ounces&#39;].apply(lambda a: abs(a))  # 负值不合法，取绝对值

# 查找food重复的记录，分组求其平均值
d_rows = df[df[&#39;food&#39;].duplicated(keep=False)]
g_items = d_rows.groupby(&#39;food&#39;).mean()
g_items[&#39;food&#39;] = g_items.index
print(g_items)

# 遍历将重复food的平均值赋值给df
for i, row in g_items.iterrows():
    df.loc[df.food == row.food, &#39;ounces&#39;] = row.ounces
df.drop_duplicates(inplace=True)  # 删除重复记录

df.index = range(len(df))  # 重设索引值
print(df)</p>2019-02-19</li><br/><li><span>西南偏北</span> 👍（16） 💬（1）<p>这些东西，大家都一定要上手去实现一遍。最简单的就是，搞一个文本，把这些数据放进去，用Python读这个文本，转成dataframe，把老师讲的那些清洗相关的API都一个一个试一下，才会有体会，光看一遍真的没啥用的！
现在只是很少的几十条数据，等你真正去搞那些上亿的数据的时候，就知道核对数据是个多么复杂的事情了……</p>2019-01-07</li><br/><li><span>滢</span> 👍（9） 💬（2）<p>觉得完全合一原则挺好，不过有些操作顺序是不是得更改一下，比如数值补全要在删除全空行之后，否则在补全的时候全空行也会补全。接下来总结在清洗过程中的问题：（1） 不知道Python2 执行情况如何，在用Python3进行数据清理的时候，对于女性三围数据补全的时候因为列中有空字符的存在，会提示‘must str not int’,需要自己过滤含有数值的有效数据进行mean()计算。  (2)生成的新列一般会自动补到后面，但first_name,last_name需要在第一列和第二列，所以要进行列移动或列交换。(3)在删除数据之后默认加载的索引会出现问题，需要自己更新索引</p>2019-04-11</li><br/><li><span>周飞</span> 👍（6） 💬（1）<p>完整性：ounces 列数据中存在NAN
全面性：food列数据中存在大小写不一致问题
合法性：ounces列数据存在负值
唯一性：food列数据存在重复
# -*- coding: utf-8 -*
import pandas as pd
import numpy as np
from pandas import Series, DataFrame
df = pd.read_csv(&#39;.&#47;fooddata.csv&#39;)
# 把ounces 列中的NaN替换为平均值
df[&#39;ounces&#39;].fillna(df[&#39;ounces&#39;].mean(), inplace=True)
# 把food列中的大写字母全部转换为小写
df[&#39;food&#39;] = df[&#39;food&#39;].str.lower()
# 把ounces 列中的负数转化为正数
df[&#39;ounces&#39;]= df[&#39;ounces&#39;].apply(lambda x: abs(x))
#删除food列中的重复值
df.drop_duplicates(&#39;food&#39;,inplace=True)
print (df)
</p>2019-01-12</li><br/><li><span>上官</span> 👍（6） 💬（1）<p>weight = int(float((lbs_row[&#39;weight&#39;][:-3])&#47;2.2)
老师好，这行代码中[：-3]的作用是什么啊？


</p>2019-01-08</li><br/><li><span>北方</span> 👍（3） 💬（1）<p>#!&#47;usr&#47;bin&#47;env python
# -*- coding:utf8 -*-
# __author__ = &#39;北方姆Q&#39;
# __datetime__ = 2019&#47;1&#47;11 15:53


import pandas as pd

# 导入
df = pd.read_csv(&quot;.&#47;s11.csv&quot;)
# 去除完全的空行
df.dropna(how=&#39;all&#39;, inplace=True)
# 食物名切分并去掉原本列
df[[&quot;first_name&quot;, &quot;last_name&quot;]] = df[&quot;food&quot;].str.split(expand=True)
df.drop(&quot;food&quot;, axis=1, inplace=True)
# 名称首字母大写
df[&quot;first_name&quot;] = df[&quot;first_name&quot;].str.capitalize()
df[&quot;last_name&quot;] = df[&quot;last_name&quot;].str.capitalize()
# 以食物名为标准去重
df.drop_duplicates([&quot;first_name&quot;, &quot;last_name&quot;], inplace=True)

print(df)</p>2019-01-11</li><br/><li><span>qinggeouye</span> 👍（2） 💬（2）<p>https:&#47;&#47;github.com&#47;qinggeouye&#47;GeekTime&#47;blob&#47;master&#47;DataAnalysis&#47;11_data_clean.py

import pandas as pd

# 读取数据
data_init = pd.read_csv(&quot;.&#47;11_clothingStoreMembers.csv&quot;)

# 清洗数据

# 删除 &#39;\t&#39; 列, 读取 csv 文件多了一列
data_init.drop(columns=&#39;\t&#39;, inplace=True)
# 重命名列名
data_init.rename(
    columns={&quot;0&quot;: &quot;SEQ&quot;, &quot;1&quot;: &quot;NAME&quot;, &quot;2&quot;: &quot;AGE&quot;, &quot;3&quot;: &quot;WEIGHT&quot;, &quot;4&quot;: &quot;BUST_M&quot;, &quot;5&quot;: &quot;WAIST_M&quot;, &quot;6&quot;: &quot;HIP_M&quot;,
             &quot;7&quot;: &quot;BUST_F&quot;, &quot;8&quot;: &quot;WAIST_F&quot;, &quot;9&quot;: &quot;HIP_F&quot;}, inplace=True)
print(data_init)

# 1、完整性
# 删除空行
data_init.dropna(how=&#39;all&#39;, inplace=True)

# 4、唯一性
# 一列多个参数切分
data_init[[&quot;FIRST_NAME&quot;, &quot;LAST_NAME&quot;]] = data_init[&quot;NAME&quot;].str.split(expand=True)
data_init.drop(&quot;NAME&quot;, axis=1, inplace=True)
# 删除重复数据
data_init.drop_duplicates([&quot;FIRST_NAME&quot;, &quot;LAST_NAME&quot;], inplace=True)

# 2、全面性
# 列数据单位统一, 体重 WEIGHT 单位统一（lbs 英镑， kg 千克）
rows_with_lbs = data_init[&quot;WEIGHT&quot;].str.contains(&quot;lbs&quot;).fillna(False)
print(rows_with_lbs)
# lbs 转为 kg
for i, lbs_row in data_init[rows_with_lbs].iterrows():
    weight = int(float(lbs_row[&quot;WEIGHT&quot;][:-3]) &#47; 2.2)
    data_init.at[i, &quot;WEIGHT&quot;] = &quot;{}kgs&quot;.format(weight)
print(data_init)

# 3、合理性
# 非 ASCII 字符转换，这里删除处理
data_init[&quot;FIRST_NAME&quot;].replace({r&#39;[^\x00-\x7F]+&#39;: &#39;&#39;}, regex=True, inplace=True)
data_init[&quot;LAST_NAME&quot;].replace({r&#39;[^\x00-\x7F]+&#39;: &#39;&#39;}, regex=True, inplace=True)

# 1、完整性
# 补充缺失值-均值补充
data_init[&quot;AGE&quot;].fillna(data_init[&quot;AGE&quot;].mean(), inplace=True)
# 体重先去掉 kgs 的单位符号
data_init[&quot;WEIGHT&quot;].replace(&#39;kgs$&#39;, &#39;&#39;, regex=True, inplace=True)  # 不带单位符号 kgs
data_init[&quot;WEIGHT&quot;] = data_init[&quot;WEIGHT&quot;].astype(&#39;float&#39;)
data_init[&quot;WEIGHT&quot;].fillna(data_init[&quot;WEIGHT&quot;].mean(), inplace=True)

data_init.replace(&#39;-&#39;, 0, regex=True, inplace=True)  # 读取的 csv 数据多了&#39;-&#39;
data_init[&quot;WAIST_F&quot;] = data_init[&quot;WAIST_F&quot;].astype(&#39;float&#39;)
data_init[&quot;WAIST_F&quot;].fillna(data_init[&quot;WAIST_F&quot;].mean(), inplace=True)

# 用最高频的数据填充
age_max_freq = data_init[&quot;AGE&quot;].value_counts().index[0]
print(age_max_freq)
data_init[&quot;AGE&quot;].fillna(age_max_freq, inplace=True)
print(data_init)</p>2019-11-08</li><br/><li><span>Chino</span> 👍（2） 💬（1）<p>有一个问题 就是代码最后那里to_excel 如果参数的路径是指定的那种 就会报错显示filenotfound 搜了很久都没找到是什么原因 求解
另外感觉这一讲有好多点都没讲深入呀 下面代码是对课程中的样例进行清洗 感觉只能做到几小点了. 特别是在填充nan值的时候 一开始想着遍历每一个nan值 然后再特判列的类型进行填充的. 但是发现三围那里有个大问题 按理说三围应该是int类型 但是因为有- 这个东西的存在 搞的三围是object类型 一开始赋值的时候报错提示需要str 后来想把列的类型转换成int也失败了 还有好多地方都卡着了...
import pandas as pd
import numpy as np
from pandas import Series, DataFrame

data = DataFrame(pd.read_excel(&#39;~&#47;Desktop&#47;data.xlsx&#39;))

print(data)

# 更改列名
data.rename(columns={0:&#39;序号&#39;,1:&#39;姓名&#39;,2:&#39;年龄&#39;,3:&#39;体重&#39;,4:&#39;男三围1&#39;,5:&#39;男三围2&#39;,6:&#39;男三围3&#39;,7:&#39;女三围1&#39;,8:&#39;女三围2&#39;,9:&#39;女三围3&#39;},inplace = True)

# 去掉重复行
data = data.drop_duplicates()

# 1.完整性
# 填充缺失值
col = data.columns.values.tolist()
row = data._stat_axis.values.tolist()

# 先把姓名的数据类型改成字符串
data[&#39;姓名&#39;] = data[&#39;姓名&#39;].astype(&#39;str&#39;) 

# 1.1 先清除空行
data.dropna(how = &#39;all&#39;, inplace = True)

# 1.2 填充缺失值
age_maxf = data[&#39;年龄&#39;].value_counts().index[0]

# 以年龄频率最大值来填充
data[&#39;年龄&#39;].fillna(age_maxf, inplace=True)

# 2.全面性
# 把体重单位为lbs的转化为kgs 2.2lbs = 1kgs
# 把所有体重单位为lbs的记录存放在一起 (如果体重是nan则不要)
rows_with_lbs = data[&#39;体重&#39;].str.contains(&#39;lbs&#39;).fillna(False)

for i,lbs_row in data[rows_with_lbs].iterrows():
    weight = int(float(lbs_row[&#39;体重&#39;][:-3]) &#47; 2.2)
    # 第一个参数是y坐标(竖) 第二个参数是x坐标(横) 
    data.at[i,&#39;体重&#39;] = &#39;{}kgs&#39;.format(weight)

print(data)

# 把清洗后的数据输出
data.to_excel(&#39;CleanData.xlsx&#39;)
# 会报错
# data.to_excel(&#39;~&#47;Desktop&#47;CleanData.xlsx&#39;)</p>2019-01-17</li><br/><li><span>一语中的</span> 👍（1） 💬（1）<p>#pd读取数据
df = pd.read_excel(&#39;testdata11.xlsx&#39;)
#1.完整性，ounces列NA值用平均值填充
df[&#39;ounces&#39;].fillna(df[&#39;ounces&#39;].mean(), inplace=True)
#2.全面性，统一food列大小写
df[&#39;food&#39;] = df[&#39;food&#39;].str.lower()
#3.合法性，ounces列负值取绝对值
df[&#39;ounces&#39;] = df[&#39;ounces&#39;].apply(lambda x: abs(x))
#4.唯一性.animal列有重复值
df.drop_duplicates(&#39;food&#39;, inplace=True)
#5.重新排序显示
df.reset_index(drop=True, inplace=True)
print(df)

</p>2019-02-26</li><br/><li><span>littlePerfect</span> 👍（1） 💬（1）<p>import pandas as pd

df = pd.read_excel(&quot;E:\data_analys_work&#47;food_data.xlsx&quot;)

# 1. 完整性问题: 缺失值
# df[&#39;ounces&#39;].fillna(df[&#39;ounces&#39;].mean(),inplace=True) # 平均值
ounces_maxf = df[&#39;ounces&#39;].value_counts().index[0] # 出现频率最高的值
df[&#39;ounces&#39;].fillna(ounces_maxf, inplace=True)

# 2. 全面性问题: 列首字母不统一
df[&#39;food&#39;] = df[&#39;food&#39;].str.title()

# 3. 合理性问题: 列数据的单位不统一
df[&#39;ounces&#39;] = df[&#39;ounces&#39;].apply(lambda x: abs(x))

# 4. 唯一性问题: 食物重复出现,求和合并,并删除多余行
# 没想出来.....
</p>2019-02-18</li><br/><li><span>Geek_hve78z</span> 👍（1） 💬（1）<p>1、完整性：
在ounces一列，存在缺失值。处理步骤：删除，因为有重复值
2、全面性：
food列首字母大小写不统一。处理步骤：利用DataFrame.columns.str.lower()全部换成小写
3、合法性：
在重量一列，存在负数。处理步骤，删除
4、唯一性：
在食品名称一列，统一大小写后，存在重复值。处理：求和合并</p>2019-02-12</li><br/><li><span>Chino</span> 👍（1） 💬（1）<p>作业
import pandas as pd
import numpy as py
from pandas import Series, DataFrame

data = DataFrame(pd.read_excel(&#39;~&#47;Desktop&#47;HomeworkData.xlsx&#39;))

# 把食物的名字统一大小写
data[&#39;food&#39;] = data[&#39;food&#39;].str.title()

# 完整性
# 填充空值
data[&#39;ounces&#39;].fillna(data[&#39;ounces&#39;].mean(), inplace = True)

# 全面性
# 没有发现问题

# 合理性
# ounces值应大于等于0 负值取绝对值
data[&#39;ounces&#39;][data[&#39;ounces&#39;] &lt; 0] = data[&#39;ounces&#39;][data[&#39;ounces&#39;] &lt; 0] * -1

# 唯一性
# 清除食物名重复的数据
data.drop_duplicates(&#39;food&#39;,inplace = True)

data.to_excel(&#39;HomeworkCleanData.xlsx&#39;)
</p>2019-01-17</li><br/>
</ul>