我们采集的数据经常会有冗余重复的情况。举个简单的例子，假设你是一个网络综艺节目的制片人，一共有12期节目，你一共打算邀请30位明星作为节目的嘉宾。你知道这些明星影响力都很大，具体在微博上的粉丝数都有标记。于是你想统计下，这些明星一共能直接影响到微博上的多少粉丝，能产生多大的影响力。

然后你突然发现，这些明星的粉丝数总和超过了20亿。那么他们一共会影响到中国20亿人口么？显然不是的，我们都知道中国人口一共是14亿，这30位明星的影响力总和不会覆盖中国所有人口。

那么如何统计这30位明星真实的影响力总和呢？这里就需要用到数据集成的概念了。

数据集成就是将多个数据源合并存放在一个数据存储中（如数据仓库），从而方便后续的数据挖掘工作。

据统计，大数据项目中80%的工作都和数据集成有关，这里的数据集成有更广泛的意义，包括了数据清洗、数据抽取、数据集成和数据变换等操作。这是因为数据挖掘前，我们需要的数据往往分布在不同的数据源中，需要考虑字段表达是否一样，以及属性是否冗余。

## 数据集成的两种架构：ELT和ETL

数据集成是数据工程师要做的工作之一。一般来说，数据工程师的工作包括了数据的ETL和数据挖掘算法的实现。算法实现可以理解，就是通过数据挖掘算法，从数据仓库中找到“金子“。

什么是ETL呢？ETL是英文Extract、Transform和Load的缩写，顾名思义它包括了数据抽取、转换、加载三个过程。ETL可以说是进行数据挖掘这项工作前的“备菜”过程。

我来解释一下数据抽取、转换、加载这三个过程。

抽取是将数据从已有的数据源中提取出来。

转换是对原始数据进行处理，例如将表输入1和 表输入2 进行连接形成一张新的表。

![](https://static001.geekbang.org/resource/image/31/68/31a88246298e317c90412bc9c03eee68.png?wh=527%2A174)

如果是三张表连接的话，可以怎么操作呢？先将表输入1和表输入2进行连接形成表输入1-2，然后将表输入1-2和表输入3进行连接形成新的表。然后再将生成的新表写入目的地。

![](https://static001.geekbang.org/resource/image/bd/f7/bd39e4f480f92a1794ccc43f51acf9f7.png?wh=717%2A273)

根据转换发生的顺序和位置，数据集成可以分为 ETL 和 ELT 两种架构。

ETL 的过程为提取(Extract)——转换(Transform)——加载(Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取(Extract)——加载(Load)——变换(Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如Spark来完成转换的步骤。

![](https://static001.geekbang.org/resource/image/90/c2/90df6593871fa5974e744907bb145bc2.jpg?wh=2420%2A1329)

目前数据集成的主流架构是ETL，但未来使用ELT作为数据集成架构的将越来越多。这样做会带来多种好处：

- ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面ELT允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
- 在ELT架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

**ETL工具有哪些？**

介绍完了这两种架构，你肯定想要知道ETL工具都有哪些？

典型的ETL工具有:

- 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services等
- 开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop等

相对于传统的商业软件，Kettle是一个易于使用的，低成本的解决方案。国内很多公司都在使用Kettle用来做数据集成。所以我重点给你讲解下Kettle工具的使用。

## Kettle工具的使用

Kettle是一款国外开源的ETL工具，纯Java编写，可以在Window和Linux上运行，不需要安装就可以使用。Kettle 中文名称叫水壶，该项目的目标是将各种数据放到一个壶里，然后以一种指定的格式流出。

Kettle在2006年并入了开源的商业智能公司Pentaho, 正式命名为Pentaho Data Integeration，简称“PDI”。因此Kettle现在是Pentaho的一个组件，下载地址：[https://community.hitachivantara.com/docs/DOC-1009855](https://community.hitachivantara.com/docs/DOC-1009855)

在使用Kettle之前还需要安装数据库软件和Java运行环境（JRE）。

Kettle采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation转换和Job作业。

- Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。
- Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。

接下来，我分别讲下这两个脚本的创建过程。

**如何创建Transformation（转换）**

Transformation可以分成三个步骤，它包括了输入、中间转换以及输出。

![](https://static001.geekbang.org/resource/image/de/ea/de0b5c9d489a591fa1f478cfd09585ea.png?wh=352%2A76)

在Transformation中包括两个主要概念：Step和Hop。Step的意思就是步骤，Hop就是跳跃线的意思。

- Step（步骤）：Step是转换的最小单元，每一个Step完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这4个步骤；
- Hop（跳跃线）：用来在转换中连接Step。它代表了数据的流向。

![](https://static001.geekbang.org/resource/image/ba/f7/bab04c4d05f3a5dbd02d8abd147630f7.jpg?wh=1179%2A622)

**如何创建Job（作业）：**

完整的任务，实际上是将创建好的转换和作业串联起来。在这里Job包括两个概念：Job Entry、Hop。

如何理解这两个概念呢？

- Job Entry（工作实体）：Job Entry是Job内部的执行单元，每一个Job Entry都是用来执行具体的任务，比如调用转换，发送邮件等。
- Hop：指连接Job Entry的线。并且它可以指定是否有条件地执行。

在Kettle中，你可以使用Spoon，它是一种一种图形化的方式，来让你设计Job和Transformation，并且可以保存为文件或者保存在数据库中。下面我来带你做一个简单的例子。

**案例1：如何将文本文件的内容转化到MySQL数据库中**

这里我给你准备了文本文件，这个文件我上传到了GitHub上，你可以自行下载：[http://t.cn/E4SzvOf](http://t.cn/E4SzvOf)，数据描述如下：

![](https://static001.geekbang.org/resource/image/3b/97/3b6ad903051b066bfba1a4cf3a0d3197.png?wh=298%2A105)

下面我来教你，如何将文本文件的内容转化到MySQL数据库中。

Step1：创建转换，右键“转换→新建”；

Step2：在左侧“核心对象”栏目中选择“文本文件输入”控件，拖拽到右侧的工作区中；

Step3：从左侧选择“表输出”控件，拖拽到右侧工作区；

Step4：鼠标在“文本文件输入”控件上停留，在弹窗中选择图标，鼠标拖拽到“表输出”控件，将一条连线连接到两个控件上；

这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。

Step5：双击“文本文件输入”控件，导入已经准备好的文本文件；

Step6：双击“表输出”控件，这里你需要配置下MySQL数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个wucai数据库，以及score数据表。字段包括了name、create\_time、Chinese、English、Math，与文本文件的字段一致）。

具体操作可以看下面的演示：

![](https://static001.geekbang.org/resource/image/6f/c9/6fb632fe0f3a2cce4169b9633a86c0c9.gif?wh=744%2A404)

Step7：创建数据库字段的对应关系，这个需要双击“表输出”，找到数据库字段，进行字段映射的编辑；

![](https://static001.geekbang.org/resource/image/8f/04/8fee5a90cab86200102a7006efe88104.png?wh=485%2A186)  
Step8：点击左上角的执行图标，如下图：

![](https://static001.geekbang.org/resource/image/f9/6d/f9678df2fa2d1684a03d43f90cce716d.png?wh=733%2A526)

这样我们就完成了从文本文件到MySQL数据库的转换。

Kettle的控件比较多，内容无法在一节课内容中完整呈现，我只给你做个入门了解。

另外给你推荐一个Kettle的开源社区：[http://www.ukettle.org](http://www.ukettle.org) 。

在社区里，你可以和大家进行交流。因为Kettle相比其他工具上手简单，而且是开源工具，有问题可以在社群里咨询。因此我推荐你使用Kettle作为你的第一个ETL工具。

当然除了Kettle工具，实际工作中，你可能也会接触到其他的ETL工具，这里我给你简单介绍下阿里巴巴的开源工具DataX和Apache的Sqoop。

**阿里开源软件：DataX**

在以往的数据库中，数据库都是两两之间进行的转换，没有统一的标准，转换形式是这样的：

![](https://static001.geekbang.org/resource/image/cf/50/cf2a017d2055afb1f8236bd6518e1c50.jpg?wh=1465%2A1251)

但DataX 可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。

![](https://static001.geekbang.org/resource/image/ad/ac/ad35fb37962b548f6bf7a00b291ec9ac.jpg?wh=1429%2A937)

DataX的模式是基于框架+插件完成的，DataX的框架如下图：

![](https://static001.geekbang.org/resource/image/8c/67/8cac4b4298187dcd4621c9a1f9551a67.jpg?wh=2373%2A981)

在这个框架里，Job作业被Splitter分割器分成了许多小作业Sub-Job。在DataX里，通过两个线程缓冲池来完成读和写的操作，读和写都是通过Storage完成数据的交换。比如在“读”模块，切分后的小作业，将数据从源头装载到DataXStorage，然后在“写”模块，数据从DataXStorage导入到目的地。

这样的好处就是，在整体的框架下，我们可以对Reader和Writer进行插件扩充，比如我想从MySQL导入到Oracle，就可以使用MySQLReader和OracleWriter插件，装在框架上使用即可。

**Apache开源软件:Sqoop**

Sqoop是一款开源的工具，是由Apache基金会所开发的分布式系统基础架构。Sqoop在Hadoop生态系统中是占据一席之地的，它主要用来在Hadoop和关系型数据库中传递数据。通过Sqoop，我们可以方便地将数据从关系型数据库导入到HDFS中，或者将数据从HDFS导出到关系型数据库中。

Hadoop实现了一个分布式文件系统，即HDFS。Hadoop的框架最核心的设计就是HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。

## 总结

今天我介绍了数据集成的两种架构方式，以及Kettle工具的基本操作。不要小看了ETL，虽然它不直接交付数据挖掘的结果，但是却是数据挖掘前重要的工作，它包括了抽取各种数据、完成转化和加载这三个步骤。

因此除了数据科学家外，还有个工作职位叫ETL工程师，这份工作正是我们今天介绍的从事ETL这种架构工作的人。如果你以后有机会从事这份工作，你不仅要对今天介绍的数据集成概念有所了解，还要掌握至少一种ETL开发工具，如Kettle、DataX、 Sqoop等；此外还需要熟悉主流数据库技术，比如SQL Server、PostgeSQL、Oracle等。

![](https://static001.geekbang.org/resource/image/07/a0/0767b2fa01ea526d19857e9b95bc1ba0.jpg?wh=3073%2A1324)

这是我操作kettle的流程视频，你可以看一下。

今天我给你讲了数据集成的两种架构，以及帮助我们实现ETL的工具Kettle。纸上得来终觉浅，绝知此事要躬行。你不妨尝试下如何使用Kettle将MySQL数据库内容转化到文本文件？

另我想让你来谈谈，你对数据集成的理解，如果你之前做过ETL的工具，也请你来谈一谈你对ETL的工具选择和使用经历。

欢迎在评论区与我分享你的想法。如果你觉得这篇文章对你有帮助，欢迎点击“请朋友读”，分享给你的朋友和同事。
<div><strong>精选留言（15）</strong></div><ul>
<li><span>Monica</span> 👍（22） 💬（3）<p>在“数据分析实战交流群”，老师分享了额外干货资料：“Kettle的操作视频”，有入群需求的，可加我的微信：imonica1010，和老师及同学们交流数据分析的学习心得。

由于申请人数太多，进群免费但设置了一道小门槛，欢迎加我，了解入群规则。</p>2019-01-09</li><br/><li><span>云深不知处</span> 👍（27） 💬（1）<p>大约三年大数据工程师工作，从最开始的数据集成（sqoop、代码、商用软件ETL工具等），将数据汇聚到数据仓库，理解业务，清洗应用需要的数据。数据集成是将多源（多系统）、多样（结构化、非结构化、半结构化）、多维度数据整合进数据仓库，形成数据海洋，更好的提供业务分析系统的数据服务，通过数仓的数据集成，达到数据共享的效果，降低对原始业务系统的影响，同时加快数据分析工作者的数据准备周期。数据集成最开始就是原始系统的数据，照样搬到数据仓库，这种类型工作长期实施，容易疲劳失去兴趣，理解业务需求，通过自己的数据集成、清洗、数据分析，提供有意思的数据，就是挖金子过程，应该也是一件有趣的事情。</p>2019-06-09</li><br/><li><span>奔跑的徐胖子</span> 👍（14） 💬（2）<p>希望有如我一般的使用Mac的屌丝注意，安装完了Kettle之后，要去mysql官网下载驱动，这个驱动不能用最新版本的，要用老版本的才能连接数据库，我用的是5.1.46</p>2019-03-22</li><br/><li><span>JingZ</span> 👍（14） 💬（1）<p>#2019&#47;1&#47;9 Kettle数据集成

1、安装jdk：官网http:&#47;&#47;www.oracle.com&#47;technetwork&#47;java&#47;javase&#47;downloads&#47;jdk8-downloads-2133151.html，下载mac版的JDK，下载后，直接安装。终端验证java-version~

2、安装Kettle：https:&#47;&#47;sourceforge.net&#47;projects&#47;pentaho&#47;files&#47;Data%20Integration&#47;
下载最新pdi-ce-8.2.0.0-342.zip压缩包，直接生成data integration文件夹

3、下载数据库驱动：https:&#47;&#47;dev.mysql.com&#47;downloads&#47;connector&#47;j&#47;
mysql-connector-java-5.1.41.tar.gz解压，但是出现了query-cache-size问题，重新下载最新mysql-connector-java-8.0.13.tar.gz，又出现找不到jar文件。重启也不行～最后两个文件都放在了data integration&#47;lib里面，貌似就可以了，这块还需再探索

4、打开终端，启动Kettle：sh spoon.sh打开Spoon，开始文本输入和表输出了

5、安装MySQL，同时安装了MySQL WorkBench建立数据库wucai和表score，目前出现表输出Unexpected batch update error committing the database connection和org.pentaho.di.core.exception.KettleDatabaseBatchException问题，可能是对SQL设置问题，还需debug

接触新工具，还需多实践</p>2019-01-09</li><br/><li><span>veical</span> 👍（6） 💬（1）<p>加载就是把转换后的数据从中间层(stage层，通常是一个数据库或数据库集群)导入数据分析层，然后才能在模型中用这些干净的数据进行数据分析</p>2019-01-10</li><br/><li><span>qinggeouye</span> 👍（3） 💬（1）<p>1、搭环境(open jdk , mysql 8.0 , mysql-connector for java, kettle)
2、启动 kettle , 实操 ...</p>2019-11-12</li><br/><li><span>羊小看</span> 👍（2） 💬（1）<p>目前我们做的业务需求比较多，一个需求有时会关联五六张表，所以我们特别希望可以先做转换，做成大宽表，入仓，可以直接用。
老师说的先加载再转换，是适用于做数据挖掘时吗？</p>2019-08-27</li><br/><li><span>Sandy</span> 👍（1） 💬（2）<p>我现在每天的工作就是写kettle job</p>2019-04-08</li><br/><li><span>旭霁</span> 👍（1） 💬（1）<p>数据库 MySQL 操作
本地登录
mysql -u root -p

创建数据库 wucai
CREATE DATABASE wucai;

查询数据库
show databases;

切换&#47;进入数据库 wucai
use wucai;

创建数据库表 score。包含 create_time、name、Chinese、English、Math 五个字段。
CREATE TABLE score (create_time VARCHAR(255) NULL, name VARCHAR(255) NULL, Chinese VARCHAR(255) NULL, English VARCHAR(255) NULL, Math VARCHAR(255) NULL);

查询数据库表
show tables;</p>2019-03-19</li><br/><li><span>lemonlxn</span> 👍（0） 💬（1）<p>通过pandas预处理数据，然后用 pd.to_sql 的方式直接写到 数据库里面，这个方式感觉会更快。
或者通过 Koalas 的 to_spark 后的overwrite模式，可以直接 将数据写入到 DBFS里面的</p>2020-09-18</li><br/><li><span>hegw</span> 👍（0） 💬（1）<p>数据集成：数据抽取、数据转换、数据加载
工具：DataX、Sqoop</p>2020-05-08</li><br/><li><span>timeng27</span> 👍（0） 💬（1）<p>直接用pandas进行数据转换应该也可以。</p>2020-03-31</li><br/><li><span>Geek_樗</span> 👍（0） 💬（1）<p>今天学习收获良多，以前没有ETL和ELT的概念，现在清楚多了。</p>2020-03-31</li><br/><li><span>北房有佳人</span> 👍（0） 💬（2）<p>老师,kettle的教程不太详细，github的数据集也没办法下载啊</p>2019-11-28</li><br/><li><span>建强</span> 👍（0） 💬（1）<p>数据集成理解：数据集成是对各种业务数据进行清洗、按数据仓库模型转换后形成一套跨业务的集成数据，这套集成数据可以从各个维度、全方位地展示业务数据，为BI分析和数据挖掘提供可靠、丰富的数据来源。
我曾经用过的ETL工具是IBM的DataStage，DataStage的数据抽取效率还是挺高的，它是在抽取过程中对数据进行分区，各个分区并行抽取和转换，最后再对各个分区进行收集和合并，这种机制极大提高了ETL的效率。
DataStage采用图形化界面进行流程设计，不需要编程，上手比较容易，但各个组件需要进行大量配置，配置比较繁琐，另外，调试起来也不是很直观方便，但总体来说还是一款不错的ETL工具。</p>2019-08-06</li><br/>
</ul>