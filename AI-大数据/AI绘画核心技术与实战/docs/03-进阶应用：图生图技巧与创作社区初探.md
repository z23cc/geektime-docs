你好，我是南柯。

上一讲我们学习了很多文生图的AI绘画技巧。如果说文本生图是一种无中生有的绘画形式，那么图像生图无疑是锦上添花的绘画技巧。

今天这一讲，我会带你探索图生图的功能，了解图生图能力能够帮我们完成哪些任务，并通过控制重绘强度影响图生图的绘画效果。另外，我还会带你了解Civitai和Hugging Face这两个开源社区，用不同风格的模型帮我们进行AI绘画。学完这一讲，你就可以使用开源社区丰富的模型宝库，对自己手中的照片进行魔改了！

## 图生图可以做哪些事情？

图生图，顾名思义，算法的输入是一张图片，输出也是一张图片。AI绘画给出的图片需要和用户输入的图片之间存在某种关联。这种关联包括后面这些情况。

1. 输入一张真实拍摄的照片，保持图像构图输出一张风格化的绘画结果。
2. 输入一张低分辨率的照片，输出一张高分辨率的清晰照片。
3. 输入一件衣服，输出一个模特穿着这件衣服。
4. 输入一张局部涂抹的照片，输出一张AI算法补全后的照片。
5. 输入一张图片，输出这种图片向外延展之后的效果等等。

![](https://static001.geekbang.org/resource/image/d1/60/d14366c1a0c2bd8060c5ee0012143c60.jpg?wh=4409x2480)  
![](https://static001.geekbang.org/resource/image/4c/0f/4c36e6b6e4f20f05c60d776795252a0f.jpg?wh=4409x2480)

你可以翻看前面的[先导篇](https://time.geekbang.org/column/article/676007)，了解更多文生图的效果。那么，如何通过WebUI实现这些神奇的图生图能力呢？

## 通过重绘强度控制图生图效果

我们选择img2img这个标签，prompt和negative prompt的操作与之前相同（详见第二讲），在下方drop image here区域内，可以点击选择需要修改的图像，或者拖拽一张新图像进来。

![](https://static001.geekbang.org/resource/image/65/45/65decb7056d8035bfd8964dcb0202f45.jpg?wh=2858x1408)

与文生成图不同，图生成图需要输入prompt和原始图像。首先，原始图像会经过加噪处理，转变为Stable Diffusion模型可以处理的“噪声图像”。然后，再根据prompt的引导生成图像。

因此，只要我们仔细控制参数，生成的图像就能同时包含原始图像的特征和prompt语句相关的内容。这种机制的本质其实就是**用prompt引导原始图像产生新的图像**。

光说理论太过抽象，我们结合一个具体例子来看看。我们输入一张小女孩的照片，然后在prompt写上“一张女孩的照片，迪士尼风格，卡通”，输出效果就是后面图里这样。

![](https://static001.geekbang.org/resource/image/54/96/5422b8af883ca7f1bb6016f3313a2096.jpg?wh=4409x2480)

通过观察你可以发现，这种方式生成的图像与原图相似，但风格变成了迪斯尼风格。同时，一个新变量——即Denoising strength（重绘强度）出现了。

重绘强度仅在图生成图（img2img）或高清修复过程中被应用，这个变量表示图像加噪的程度。它代表了生成的图像相对于原始输入图像内容的变化程度，数值越高，AI对原图的参考程度就越低，生成的图像和原图的区别就越大。

我们以之前的生成图像为例，对应prompt语句为 “a dog, cartoon style”，其它参数不变。

![](https://static001.geekbang.org/resource/image/fb/5c/fba81a4dcd1dc22af7yy5c2a5859365c.jpg?wh=4409x2480)

我们可以观察到，生成的图像在一定程度上保留了原始图像的特征。当重绘强度的值较高时，生成的图像变化较大，但整体轮廓仍然与原始图像相似。

那如果我们希望在图生图的过程中，对内容做出更大的调整，又该怎么做呢？针对上面这个例子，我们不妨修改prompt语句为：“a cat, cartoon style”。生成结果是后面这样。

![](https://static001.geekbang.org/resource/image/0e/65/0ecfb96dc7e828b9c5b280a009bc1365.jpg?wh=2814x1470)

![](https://static001.geekbang.org/resource/image/4d/77/4db06dbd96468a1b98d6441095fbd977.jpg?wh=4409x2480)

你有没有感觉非常有趣？当重绘强度为0.5时，生成的图像没有明显的变化，并且与新修改的“cat”没有太多相关性。然而，当重绘强度为0.85时，图像基本上都转变成了猫的图像，靠近了prompt语句，并且保留了原始图像的结构。

这个观察结果表明，重绘强度参数对图像生成的影响非常显著。**较高的重绘强度可以帮助模型更好地理解并生成与prompt相关的内容，同时保留原始图像的整体结构。**这进一步证明了参数对图生图的关键作用。

我们回到最初的任务，尝试使用不同的重绘强度和prompt变化，将一位可爱的姑娘转变为卡通或迪士尼风格。

![](https://static001.geekbang.org/resource/image/6b/b6/6be1399232841e17bf1b42be545b72b6.jpg?wh=4409x2188)

看到了么？目标达成！只要灵活应用prompt和重绘强度，我们就能随心所欲地进行图像生图。你可以课后自己亲自动手练习一下，这样体会更深。

我再给你稍微总结一下重点。重绘强度在图生成图过程中起到重要的作用，它表示生成图像相对于原始输入图像内容的变化程度。较高的重绘强度会导致生成的图像更大程度地与prompt相关，同时有些偏离原始图像的特征。效果就是生成的图像更加创新、有趣，但与原始图像之间的相似性较低。

相反，较低的重绘强度会更倾向保留原始图像的整体特征和结构，让生成的图像更接近原始图像，但可能在与prompt相关性方面稍显不足，说白了就是只能针对原始图像做小修小补。

因此，适当调整重绘强度可以控制生成图像的变化程度和与原始图像的相似性。我们根据具体需求，可以选择合适的重绘强度，来达到所期望的图像生成效果。比如，采样步数为30，重绘强度取0.6便是不错的选择。更多的参数组合，你可以课后做更多探索。

## 开源社区的魅力：Civitai和Hugging Face

掌握了WebUI这个有用的工具，如何获取到各种风格的AI绘画模型就非常关键了。事实上，除了我们经常听到的Stable Diffusion 1.4、1.5、2.0这样的模型，我们在开源社区中还能找到成千上万的有趣模型，为我们所用。

[Civitai](https://civitai.com/) 和 [Hugging Face](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery) 是AI绘画领域两个非常重要的开源社区。它们吸引了来自全球各地的网友们参与其中。这些社区成为了一个宝藏般的资源库，提供了大量且多样的风格模型。通过这些社区，人们可以相互交流、分享和发现新的AI绘画技巧，不断推动AI绘画领域的发展。

![](https://static001.geekbang.org/resource/image/61/a3/61a54c313979dea1de9a0cd228e34da3.jpg?wh=2558x1268 "Hugging Face")

![](https://static001.geekbang.org/resource/image/3f/ca/3faef9e20aaa9e904c889c2fbfee4dca.jpg?wh=2708x1370 "Civitai")

在使用上，这两个社区却有点区别。对于Hugging Face，你需要先注册官方账号并安装diffusers库，这样你就可以使用社区中提供的各种模型。以SD1.5模型为例，你调用相关的API即可用它完成图像生成任务，脚本如下。

```python
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]  
    
image.save("astronaut_rides_horse.png")
```

大多数情况下，模型的更换就是更换 “model\_id”。但如果我们希望有良好的界面管理，而且希望联合使用多种能力的话，WebUI配合Civitai或许是你最佳的选择。通过界面上的选项，就能选择不同的模型和参数设置，实现定制化的AI绘图。

在使用Civitai网站时，很重要的一步是找到并且选择适合自己的模型。你需要仔细查看模型的类型和使用方式，以确保正确地安装和配置模型，这样WebUI才能顺利调用它。

![](https://static001.geekbang.org/resource/image/e6/99/e6ee559d3733719b662348ddyy83a799.jpg?wh=2490x1232)

尝试不同的风格化模型，可以让我们探索多样的创作风格和特征。每个模型都有其独特的艺术表达方式，选到合适的模型有助于我们精准去控制图像风格。最后，在大量风格化模型，巧妙设计prompt以及合理调整参数的配合下，相信你的AI绘画能力将会得到显著提升。

![](https://static001.geekbang.org/resource/image/15/y7/15cdc7736090e40c85d218edca50byy7.jpg?wh=4409x2253)

## Midjourney：风头无两的画师

除了上面提到的Civitai和Hugging Face两个开源社区，Midjourney也是AI绘画的一大杀器。不同于WebUI，我们可以在聊天应用程序Discord中使用Midjourney，不需要本地GPU资源，也不需要安装任务WebUI类的工具。相比前面的两个开源社区，Midjourney模型AI绘画的效果在精致度、图像文本关联性上都有更显著的优势。

这里我们一起看看Midjourney的三种常见用法。

首先是**文字生图**，在Midjourney的Discord论坛中，使用/imagine可以触发文生图功能。

![](https://static001.geekbang.org/resource/image/2c/90/2c022c22a87d9acbca6519456b6c2d90.jpg?wh=1866x220)

比如，我们可以通过简单的描述，让Midjourney帮我们生成各种美观有趣的卡通头像、可口诱人的美食、靓丽的风景、可爱的小动物，或者是有特色的艺术作品，再或者是现实生活中完全不存在的事物。我放了一些自己用Midjourney生成图片的效果示例，你可以点开图片感受一下。

![](https://static001.geekbang.org/resource/image/54/11/54bc4127ba930650f2734260b48f1511.jpg?wh=4409x2480)  
![](https://static001.geekbang.org/resource/image/94/ba/94aac3bc2842a9c967c0a6b2405eb2ba.jpg?wh=4409x2480)  
![](https://static001.geekbang.org/resource/image/6f/96/6f68158c70da3c05b860d1ac638fd096.jpg?wh=4409x2480)

然后是**图生图**。在Midjourney的Discord论坛中，首先将图像发送给Midjourney获得图像的链接，图片链接粘贴完成后，必须空一格，再输入描述语，按下回车键，最终新生成的图片就会参考原图。

![](https://static001.geekbang.org/resource/image/d4/a9/d455151597c34d85163a248fca7bbca9.jpg?wh=2038x604)

比如我们输入一张可乐的无背景图，希望将可以放到一张桌子上。我们便可以得到下面的效果。

![](https://static001.geekbang.org/resource/image/d5/16/d5e75c87ed259f639f6984e7a845f316.jpg?wh=4409x2480)

prompt语句中使用 --iw命令可以控制新图片与参考图的相似程度，Midjourney V5.2版本这个数值的范围为0 ~ 2， 默认值为0.25。数值越大，参考原图的比重越高。

认真观察上面的图片你会发现，可乐的瓶身似乎有一些变化。你可以试试将 --iw 参数再调整大一些。你也许也听过Nijijourney这个模型，它是在Midjourney原始模型的基础上，使用大量动漫图像进行模型微调后得到的二次元风格专属模型。如果我们使用Nijijourney模型进行上述图生图人物，--iw 设置0.2，你会得到下面的结果。

![](https://static001.geekbang.org/resource/image/de/b4/de89ee38821f8b5b53e4c0027eca1cb4.jpg?wh=4409x2480)

最后是**多图融合**。Midjourney支持将N张图进行图像融合，N最大支持5张。比如下面这个例子中，我们将一张女生照片和一张水墨画进行融合，得到的效果你可以点开图片查看。

![](https://static001.geekbang.org/resource/image/fc/e8/fcd538377e6f4e3f5b7704b4709c9de8.jpg?wh=2004x1030)

![](https://static001.geekbang.org/resource/image/69/e9/69b67e73712b0493746b5af01212f2e9.jpg?wh=4026x2480)

Midjourney当前并不支持LoRA用法，但类似WebUI的部分功能还是可以实现的，比如使用负面描述词、文本权重调整的技巧，不过在具体用法上和WebUI略有不同。比如负面描述词，可以通过 --no来指定；文本权重调整，可以通过 ::数值来指定。

为了帮你加深理解，我们结合后面这两个例子对比一下。

Midjourney描述词1，创作一幅温馨真挚微笑的年轻亚洲女孩的写实肖像画。

```plain
Create a realistic portrait of a young asian girl with a warm, genuine smile --ar 2:3
```

Midjourney描述词2，我们要求图像里避免出现眼镜和卷发，而且对写实、年轻和微笑都设置了不同的文本权重。

```plain
Create a realistic::-1.0 portrait of a young::1.5 asian girl with a warm, genuine smile::1.4 --no glasses curly hair  --ar 2:3
```

你可以看一下这两个描述得到的图像，负面描述词和文本权重都发挥了各自的作用。

![](https://static001.geekbang.org/resource/image/a9/af/a96821416101e3a844aa1021cd2d28af.jpg?wh=4409x2480)

不得不说，当前Midjourney在AI绘画文字生图的效果上要领先于开源社区的模型。遗憾的是Midjourney的模型并没有开源，我们只能通过账号付费的形式来使用。如果你想使用Midjourney，可以访问这个[链接](https://www.midjourney.com/app/)，如何用较低的成本使用Midjourney提供的服务，也欢迎同学们在评论区留下自己的小技巧。

## 总结时刻

这一讲我们从文生图过渡到图生图，探索了重绘强度和prompt咒语的巧妙使用。不同于文生图这种无中生有的用法，图生图能够针对我们提供的图片实现各种趣味编辑，比如图像风格化、老照片修复、图像内容补全等。

重绘强度是图生成图模型中的一个关键参数，它表示图像的重绘程度。较高的重绘强度可以使生成图像更多样化，但仍保持原始图像的整体轮廓。通过调整重绘强度，你可以控制生成图像与原始输入图像内容的变化程度，实现图生图的风格化、编辑等过程。

另外，我们一起探索了Hugging Face和Civitai这两大重要的开源模型社区，通过这些社区，你可以发掘大量的开源模型和资源，实现多样化的图像生成，大幅提升自己的AI绘画能力。Civitai可以直接在WebUI中进行管理和使用，而Hugging Face社区中的模型，我们也可以直接注册HuggingFace账号，通过diffusers库来调用。

最后我们还了解了Midjourney这个专业的AI绘画工具，介绍了Midjourney的三种常见用法：文字生图、图像生图和多图融合。未开源的Midjourney模型在生成效果、图文一致性上都有不俗的表现，本地无需GPU资源即可体验Midjourney的生成效果。

![](https://static001.geekbang.org/resource/image/4d/fc/4d6bf262d6eba7e848903fc81811a3fc.jpg?wh=3600x1697)

## 思考题

假设你使用图生图模式生成了一张卡通风格的猫的图片，你调整了重绘强度和prompt，尝试了不同的参数组合，但始终无法获得满意的结果。你观察到生成的图像要么失去了猫的特征，要么与原始图像过于相似。请思考可能的原因，并提出解决方案。

欢迎你在留言区和我交流互动，也推荐你把这一讲分享给更多朋友，和他一起交流开源AI绘画工具的乐趣。
<div><strong>精选留言（14）</strong></div><ul>
<li><span>五木老祖</span> 👍（3） 💬（2）<p>奇怪，本身sd就是一个预训练模型了，还需要加载其他模型和lora</p>2023-07-21</li><br/><li><span>妹的银拆纳</span> 👍（2） 💬（1）<p>老师~催更啦~</p>2023-07-22</li><br/><li><span>zhihai.tu</span> 👍（1） 💬（2）<p>colab安装sd课程里面什么时候会讲啊？</p>2023-07-21</li><br/><li><span>我就是我，烟火不一样的我</span> 👍（0） 💬（1）<p>老师在c站下载了模型之后，在webui怎么使用，怎么查看模型使用的基础模型，例如下载的是一个lora模型，怎么查看他使用的基础模型是啥，感觉使用了lora之后，画出来的画风和老师讲的不太一样</p>2024-01-14</li><br/><li><span>xingliang</span> 👍（0） 💬（1）<p>失去了猫的特征：调整prompt，可以用mj通过图片推出描述词，作为图片描述词参考，看看是否有优化的地方。
要么与原始图像过于相似：可以调整描述词，追加灯光、视角、或者相似度参数。</p>2023-08-12</li><br/><li><span>EnidYin</span> 👍（0） 💬（1）<p>这个问题有点问题，如何定义满意的效果，究竟要生成一张什么样的图片，可以用&#47;blend 融合原图和一张卡通风格的图片</p>2023-07-30</li><br/><li><span>AI悦创</span> 👍（0） 💬（1）<p>思考题，能不能给出解决方案？一时间没有啥解决方法</p>2023-07-24</li><br/><li><span>AI悦创</span> 👍（0） 💬（2）<p>在 Midjourney 讲解部分前部分很清晰，但是后面部分不够清晰，比如：多张图片合成，具体怎么实现？不使用特殊命令，直接发送两个图片链接就可以？打马赛克可以理解，比如老师的账户名称马赛克打马赛克保护隐私，那么图片打马赛克是为什么呢？望解惑。</p>2023-07-24</li><br/><li><span>AI悦创</span> 👍（0） 💬（1）<p>操作的图片原图不知道能不能提供？现在的操作是截图然后使用图生图😂，希望完善配套资料。（部分涉及的案例，参数能完整就更好的复现了）当然本身课程质量很好了。</p>2023-07-24</li><br/><li><span>peter</span> 👍（0） 💬（1）<p>请教老师几个问题：
Q1：对于上传的图片为什么要加噪声？
Q2：webUI可以用于手机端吗？比如安卓。
Q3：模型的训练时间长，但训练完成后，模型确定了，类似于有了一个确定的函数，所以运算就很快，是这样吗？</p>2023-07-22</li><br/><li><span>Rainbow^</span> 👍（0） 💬（4）<p>电脑配置太低用不了SD...我这课是不是白听了呜呜</p>2023-07-21</li><br/><li><span>王彦军</span> 👍（1） 💬（0）<p>可以用阿里云或者腾讯云的函数计算服务</p>2023-07-23</li><br/><li><span>hallo128</span> 👍（0） 💬（0）<p>文中链接：
1. Civitai：https:&#47;&#47;civitai.com&#47;
2. Hugging Face：https:&#47;&#47;huggingface.co&#47;spaces&#47;huggingface-projects&#47;diffusers-gallery
3. Midjourney：https:&#47;&#47;www.midjourney.com&#47;showcase</p>2024-04-11</li><br/><li><span>Chengfei.Xu</span> 👍（0） 💬（0）<p>思考题：
1、重绘强度可能太高，与prompt组合使用后导致出现与原图太过相似现象
2、原图可能与猫的特征不太相关，需要研究如何prompt以相配</p>2023-09-05</li><br/>
</ul>