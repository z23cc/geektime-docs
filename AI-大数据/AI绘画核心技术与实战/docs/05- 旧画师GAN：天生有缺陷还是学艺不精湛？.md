你好，我是南柯。

今天开始，我们正式进入AI绘画理论阶段的学习。我会带你理解图像生成模型背后的算法原理，掌握AI绘画主流算法方案背后通用的算法模块，并带你从零到一训练一个扩散模型。

基于扩散模型的AI绘画技术是我们这门课的主题，但其实在22年以前，GAN才是业界公认的AI绘画技术首选。在老一辈的AI画图中，GAN（生成对抗网络）可以说是唯一的选择。相信你也在各种社交软件上见到过各种变小孩、变老、性别变换的视觉特效，这类效果通常就是靠GAN完成的。

然而，随着22年DALL-E 2、Stable Diffusion的推出，扩散模型技术逐渐成为了AI绘画的主流技术。无论是绘画细节的精致度还是内容的多样性，扩散模型似乎都要优于GAN。

即便如此，对于入门AI绘画知识体系而言，GAN仍然是绕不开的话题，值得我们深入了解。因为搞懂了GAN的长处和短板，才能理解后来扩散模型解决了GAN的哪些痛点。而且今天我们要学的各种算法模型，也是面试中常常会问到的。

在正式探索基于扩散模型的AI绘画技术之前，我们用这一讲来重温旧画师GAN，探讨GAN如何从兴起到高光，并简要回顾GAN发展史上那些里程碑式的技术。

## GAN的起源

下面我放了两张例子。第一个例子是张大千模仿石涛的画作，第二个例子是贝特莱奇14岁时仿照毕加索的画作。假如你是艺术鉴赏家，能否发现这些仿作的破绽呢？

![](https://static001.geekbang.org/resource/image/b9/30/b90db198yy2937a8e5a697006d5e2930.jpg?wh=3891x2303)

![](https://static001.geekbang.org/resource/image/fd/c4/fdfaae42d842f7cf88e0d2efe4750dc4.jpg?wh=4409x2187)

事实上，名画伪造家和艺术鉴赏家之间的较量，酷似GAN中生成器与判别器之间的对抗。接下来，就让我们一起揭开GAN背后的奥秘。

故事还要从“遥远”的2014年说起。那时候，Ian Goodfellow等人提出了生成对抗网络——也就是GAN这个全新的概念。

当时的深度神经网络通常需要收集图像样本和目标标签，比如分类任务的标签就是类别信息、年龄回归任务的标签就是年龄数值。通常通过交叉熵损失来训练分类任务，通过数值误差损失（比如L1损失和L2损失）来训练回归任务。

而GAN的思路则完全不同。GAN模型由两个模块构成，也就是常说的生成器（Generator）和判别器（Discriminator）。可以这样类比，生成器是一位名画伪造家，目标是创作出逼真的艺术品，判别器是一位艺术鉴赏家，目标是从细节中找出伪造破绽。生成器与判别器在模型训练的过程中持续更新与对抗，最终达到平衡。

你可以看下面的伪代码，加深对GAN这种对抗训练思想的理解。

```python
for epoch in range(num_epochs):
    for batch_data in data_loader:
        # 更新判别器
        real_images = batch_data.to(device)
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = generator(z).detach()
        d_loss_real = discriminator(real_images)
        d_loss_fake = discriminator(fake_images)
        # 判别器损失
        d_loss = -(torch.mean(d_loss_real) - torch.mean(d_loss_fake))
        discriminator.zero_grad()
        d_loss.backward()
        discriminator_optimizer.step()
        
        # 更新生成器
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = generator(z)
        g_loss = -torch.mean(discriminator(fake_images))
        generator.zero_grad()
        g_loss.backward()
        generator_optimizer.step()
```

在每个训练周期内，对于每个批次的数据是这样处理的。

1.首先更新判别器，将真实图像和生成器生成的假图像输入到判别器中，计算真实图像的损失和生成图像的损失。通过反向传播更新判别器的参数，也就是利用梯度下降类的算法更新模型的权重。

2.接着更新生成器。生成一批随机噪声输入到生成器中生成图像，再将生成的图像输入到判别器中计算损失，之后反向传播更新生成器的参数。

3.重复以上步骤进行多个训练周期，直到达到预定的训练次数。

在上面代码中，关于判别器损失的计算你可能会有疑问，我这就为你解释一下。我们已经知道，判别器的目标是区分真实图像和生成图像，因此损失函数的设计是通过**最大化真实图像的损失（d\_loss\_real）和最小化生成图像的损失（d\_loss\_fake）**来实现的。

torch.mean(d\_loss\_real)计算了真实图像的平均损失，而torch.mean(d\_loss\_fake)计算了生成图像的平均损失。在这里，我们用减号将两个损失相减，是为了实现最大化真实图像损失和最小化生成图像损失的效果。通过这样的设计，我们希望判别器能够更好地区分真实图像和生成图像，从而提高生成器生成逼真图像的能力。

GAN最初的故事咱们就说到这里，它的精髓在于对抗训练思想。GAN通过生成器和判别器的竞争和学习，使得生成的图像逐渐趋近于真实图像。在现实世界中，GAN的应用场景广泛，包括图像合成、图像修复、图像风格转换等。

## 走向高光的GAN

最初的GAN并没有走进大众的视野，主要是因为GAN模型存在一些问题，比如同时训练生成器和判别器的过程并不稳定，最初的生成器生成内容不能被指定，生成的图像分辨率较低，模型推理在手机等设备上用时过长等等。

从14年GAN被提出以来，随着上面提到的这些问题逐一得到解决，GAN的发展经历了一系列的重要改进，终于迎来了它的高光时刻。那GAN是如何从平凡到卓越的呢？我们这就来看看。

### 图像生成能力的进化：DCGAN/CGAN/WGAN

最初的GAN模型使用全连接神经网络，对于图像生成任务来说，学习图像的空间结构和局部特征是非常困难的。

但2015年由Radford等人提出的[深度卷积GAN（DCGAN）](https://arxiv.org/pdf/1511.06434.pdf)给GAN带来了进化可能。主要创新就是引入卷积神经网络（CNN）结构，通过卷积层和反卷积层替代全连接层，使得生成器和判别器能够感知和利用图像的局部关系，更好地处理图像数据，从而生成更逼真的图像。

DCGAN的优点在于它的稳定性和生成效果。通过使用卷积神经网络，DCGAN能够更好地保持图像的空间结构和细节信息，生成的图像质量更高。此外，DCGAN的架构设计也为后续的GAN改进工作提供了重要的基础。

![](https://static001.geekbang.org/resource/image/9e/58/9e670cdc2d08d1409e10a5e9df656258.jpg?wh=3936x2083)

[条件GAN](https://readpaper.com/pdf-annotate/note?pdfId=4500170052153794561&noteId=1848651293424178944)，简称cGAN，允许我们在生成图像的过程中引入额外的条件信息。这样一来，我们可以控制生成图像的特征，比如生成特定类别的图像。比如在上面的数字图中，普通的GAN无法提前指定生成的数字是0到9中的哪一个，而cGAN便可以轻松控制要生成的数字是几。

[Wasserstein GAN](https://readpaper.com/pdf-annotate/note?pdfId=4665126035419840513&noteId=1848652981780710144)，简称wGAN，是另一个重要的改进，它通过使用Wasserstein距离（瓦瑟斯坦距离，也被称为地面距离）来衡量生成图像和真实图像之间的差异，这样就能提升训练的稳定性和生成图像的质量。

Wasserstein距离用于比较两个概率分布之间的差异，量化了将一个分布转换为另一个分布所需的最小工作量。

这么说有点抽象，我再举个形象的例子帮你理解，假设我们有两堆沙子，一堆沙子分布在一个地方，另一堆沙子分布在另一个地方。现在我们想将第一堆沙子移动到第二堆沙子的位置，但我们只能以一定的速度和固定的容器大小来移动沙子。Wasserstein距离就是将第一堆沙子移动到第二堆沙子所需的最小总移动成本。在wGAN中，这两堆沙子就是真实数据分布和生成数据分布。

cGAN和wGAN生成图像的分辨率很低，分辨率提升是图像生成领域一个持续研究的方向，后来的 [PGGAN](https://readpaper.com/pdf-annotate/note?pdfId=4500186031072108545&noteId=1848670213424382720)、[BigGAN](https://arxiv.org/abs/1809.11096)、[StyleGAN](https://arxiv.org/abs/1812.04948) 等工作，将生成图像的分辨率提高了1024x1024分辨率之上。这个我们之后再讲。

### 手机端实时特效：从Pix2Pix到CycleGAN

[Pix2Pix](https://readpaper.com/pdf-annotate/note?pdfId=4500178853162541057&noteId=1848681503787931392) 系列工作延续了cGAN的思想，将cGAN的条件换成了与原图尺寸大小相同的图片，可以实现类似轮廓图转真实图片、黑白图转彩色图等效果。是不是听起来很熟悉？没错，就是GAN时代的ControlNet！

![](https://static001.geekbang.org/resource/image/eb/98/ebe7925deea4ff2f11063a35600fc598.jpg?wh=4409x1850)

Pix2Pix最大的缺点就是训练需要大量目标图像与输入图像的图像对，优点是模型可以做到很轻很快，甚至能在很低端的手机上也能达到实时效果。从18年至今，我们在短视频平台上看到的各种实时变脸特效，比如年龄转换、性别编辑等特效，都是基于这个技术。

那么问题来了，获取成对的数据是困难且耗时的，那大量成对数据该怎么来呢？答案就是大名鼎鼎的 [CycleGAN](https://junyanz.github.io/CycleGAN/)。2017年Jun-Yan Zhu等人提出了CycleGAN，也就是循环一致性生成对抗网络。

![](https://static001.geekbang.org/resource/image/56/a2/56927b7896feb8fe104d95da335f20a2.jpg?wh=4409x2086)

CycleGAN的核心要点就是让两个不同领域的图像可以互相转换。它有两个生成器，分别是G（A→B）和G（B→A），它们的任务是把A领域的图像变成B领域的，反之亦然。同时，还有两个判别器，D\_A和D\_B，负责分辨A和B领域里的真实图像和生成的图像。

CycleGAN的关键点在于循环一致性损失。这个方法把原图像转换到目标领域，然后再转换回原来的领域，就可以确保生成的图像跟原图像差别不大。这种循环一致性约束让图像转换有了双向的一致性。我举个例子你就明白了，先把马变成斑马，再恢复成马，最后的图像应该跟原来的马图像很相似。

CycleGAN的优势是不需要成对的训练数据便可以实现图像转换，在很多图像转换任务上都表现得非常出色，比如风景、动物、风格等转换。再加上Pix2Pix，CycleGAN简直是制作短视频特效的神器。

### 高分辨率的生成：StyleGAN系列工作

之后，英伟达在2018年提出的生成对抗网络模型StyleGAN，彻底改变了GAN在图像合成和风格迁移方面的应用前景。与传统的GAN模型相比，StyleGAN在图像生成的质量、多样性和可控性方面取得了显著的突破。

StyleGAN的核心思想是用风格向量来控制生成图像的各种属性特点，并通过自适应实例归一化（AdaIN）把风格向量和生成器的特征图结合在一起。另外，用渐进式的生成器结构逐渐提高分辨率，这样可以提高训练的稳定性和生成图像的质量。

![](https://static001.geekbang.org/resource/image/9d/d2/9d9e2667a3a45e692aac942884f13bd2.jpg?wh=3501x2170)

StyleGAN的应用非常广泛。它不仅可以用于生成高分辨率的逼真图像，还可以用于风格迁移、图像编辑和人脸合成等任务。StyleGAN生成的图像质量非常高，具有细致的纹理、自然的细节和丰富的变化，可用于各种创作、设计和研究领域。

StyleGAN 2和StyleGAN 3是StyleGAN的改进版本。它在StyleGAN的基础上引入了一系列重要的改进，进一步提升了图像生成的质量、稳定性和控制性。

另外还有一种叫做超分辨率生成对抗网络（SRGAN）的模型，它的目标是将低分辨率图像转换成高分辨率的图像。

讲到这估计你也发现了，GAN类型的生成模型非常多，我这里给你分享的是最有影响力的模型，对于其他GAN模型，有兴趣的话你可以了解下BigGAN、StarGAN、Progressive GAN等模型。

## GAN的应用场景

无论过去还是现在，在图像生成、编辑和风格化领域，GAN都占据着非常重要的地位，而且是生成模型发展的重要里程碑。

### 图像生成

GAN可以生成各种类型的图像，包括自然风景、人脸、动物等。通过训练生成器网络，GAN能够从随机噪声中生成逼真的图像，为艺术创作、虚拟场景生成、游戏开发等领域提供了强大的工具。

![](https://static001.geekbang.org/resource/image/66/38/66f88e6b6968e006549b4b7610466938.jpg?wh=4173x2281)

### 图像局部编辑

GAN可以通过生成器网络实现对图像局部的编辑。通过将输入图像和编辑向量结合，可以精确地控制生成器网络，在特定区域编辑图像，比如改变图像的颜色、纹理或形状。这为图像编辑和修复提供了一种更加灵活和高效的方式。

![](https://static001.geekbang.org/resource/image/4c/41/4cf074db7468143ca084518d39ed0041.jpg?wh=4409x1620)

### 图像风格化

GAN可以将图像转换为具有不同艺术风格的图像。通过训练一个生成器网络，可以将输入图像转换为特定风格的图像，如印象派、油画、水彩画等。这种图像风格化技术，广泛应用于艺术创作、图像处理和社交媒体滤镜等领域。

![](https://static001.geekbang.org/resource/image/25/f8/25104ece4c97c9c0ef27e0351100b4f8.jpg?wh=4409x1772)

### 老照片修复

另外，GAN可以用于修复老照片中的损坏或模糊的部分。通过训练生成器网络，GAN可以学习恢复损坏图像的细节和纹理，并生成高质量的修复结果。这在数字文化遗产保护和历史文档修复等领域具有重要的应用意义。

![](https://static001.geekbang.org/resource/image/b5/75/b52acb79c9d887790a176eac55cbcb75.jpg?wh=4409x1392)

## 与扩散模型狭路相逢

尽管GAN逐渐走向高光，高分辨率生成、可控编辑能力等问题也得到了解决，GAN仍然存在着局限性。GAN的局限性主要表现在训练不稳定性、生成图像模糊、难以评估和控制生成质量等问题。此外，在图像风格化、图像编辑等任务中，通常是每个任务一个GAN。训练成本、数据需求量、使用场景局限性都是实际工作中的痛点。

而扩散模型在很大程度上解决了GAN的痛点。其实扩散模型并不是这两年的新鲜事，实际上，早在2015年就有人提出了图像扩散模型的概念。而GAN是2014年！二者几乎是前后脚同时提出的。

2021年之前GAN一直在图像生成领域处于制霸地位，直到2021年10月，一篇名为“[扩散模型在图像生成领域击败了GAN](https://arxiv.org/abs/2105.05233)” 的文章横空出世，扩散模型在图像生成领域的潜力才广为人知。

后来OpenAI的Glide、DALL-E 2，Google的Imagen、Parti，还有广为人知的Stable Diffusion、Midjourney，更是把基于扩散模型的AI绘画推向了新的高度。关于扩散模型，下一讲我们再深入探讨。

## GAN能否东山再起？

有意思的是，热衷于GAN的研究人员并没有放弃。就在2023年3月，Adobe的学者提出了GigaGAN, 一个新的GAN架构。一听这个名字，就有一种大模型的味道。

![](https://static001.geekbang.org/resource/image/4f/f0/4f06153da946ec53e171353d56bac3f0.jpg?wh=3314x2261)

GigaGAN是一种具有突破性的GAN模型，它通过扩大模型规模，在多个方面展现了卓越的优势。比如，对于512分辨率图像的合成，仅需要0.13秒的推理速度，这比现有的工作在推理速度上高出了一个数量级。并且GigaGAN可以合成更高分辨率的图像，生成1600万像素的图像仅需3.66秒。

![](https://static001.geekbang.org/resource/image/50/c2/503238a43497e42ce61039e5c7a552c2.jpg?wh=4104x2299)

我们再来看看GAN领域的另一位明星——DragGAN。实际上，DragGAN是一种交互式图像操作方法，为各种GAN开发提供了一种神奇的功能，我们用鼠标简单拉伸图像，就能够生成全新的图像。

使用DragGAN非常简单，用户只需要设置一个起始点、一个目标点，以及希望修改的区域。接下来，模型会进行运动监督和点跟踪这两个步骤的迭代，然后修改原始图像。这种交互式的操作方式让图像的编辑变得非常直观和有趣。

## 总结时刻

这一讲，我们认识了生成对抗网络（GAN），了解了GAN的基本算法原理，还学习了经典的GAN算法和它的应用场景，比如图像生成、局部编辑、图像风格化、老照片修复等。

之后我们也探讨了GAN的局限性，这对我们后续学习和理解扩散模型也很有帮助。即便在扩散模型风靡的今天，GAN的改进版例如GigaGAN和DragGAN仍展示出令人惊叹的创新和功能。在AI绘画这个快速发展的领域中，我们也期待GAN技术能够取得更大的突破和进步，为我们带来更加出色的图像生成和编辑能力。

我把今天的重点内容梳理成了知识导图，供你参考复习。

![](https://static001.geekbang.org/resource/image/71/c9/7117a324d7fc5902a3edeb10228213c9.jpg?wh=2084x1619)

## 思考题

在基于扩散模型的AI绘画时代到来之前，你还见过哪些有意思的GAN的应用？背后的技术原理是怎样的？

欢迎你在留言区和我交流互动，如果这一讲对你有启发，别忘了分享给身边更多朋友。
<div><strong>精选留言（10）</strong></div><ul>
<li><span>Chengfei.Xu</span> 👍（1） 💬（1）<p>GAN的原理总结：

生成对抗网络（GAN）由两个部分组成：生成器（Generator）与判别器（Discriminator），它们在模型训练的过程中会持续更新和对抗，最终达到平衡

生成器的任务是根据输入的随机噪声，生成看起来像真实样本的新数据
判别器的任务是辨别给定的真实数据是真样本还是生成器生成的伪造样本（它会收到一组真实数据和一组生成数据）

在训练过程中，生成器和判别器互相对抗。生成器试图生成更逼真的样本来迷惑判别器，而判别器则努力辨别出生成器生成的伪造样本。它们之间会不断重复这个过程，持续更新自己的参数，达到相互改进和提升


而随着训练的进行，生成器和判别器逐渐“学会”了“博弈”，最终会达到一个平衡状态，即生成器的样本会越来越逼真，判别器识别的准确率也会越来越高。通过这种对抗式的训练方式，GAN可以生成非常逼真的数据，使用场景有图像合成、图像修复、图像风格转换等等。</p>2023-09-26</li><br/><li><span>Wiliam</span> 👍（0） 💬（1）<p>老师请教一下：
1. GAN 的局限性主要表现在训练不稳定性、生成图像模糊、难以评估和控制生成质量等问题，那么GigaGAN具体是解决了哪个问题呢？
2. 抛开GAN的劣势，相比Diffusion，GAN有什么优点吗？有考虑过GAN的优点与Diffusion的优点强强联合吗？</p>2023-09-13</li><br/><li><span>xixi</span> 👍（0） 💬（1）<p>giga读错了</p>2023-09-01</li><br/><li><span>xingliang</span> 👍（0） 💬（1）<p>BigGAN：BigGAN 是为了实现高分辨率、高质量的图像生成而设计的。它的特点是在模型中引入了大量的参数，利用更大的批次大小和更多的特征通道。这样可以实现高分辨率且内容丰富的图像生成。但同时，由于模型的复杂性，它需要大量的计算资源和时间来训练。

StarGAN：不同于其他 GANs 专注于单一领域或任务，StarGAN 能够在多个领域之间进行图像转换，例如，它可以在一个模型中实现人脸属性（如头发颜色、性别等）的多种转换。StarGAN 的核心是使用一个共同的生成器和判别器，以及域标签，使其可以对多个域进行学习和转换。

Progressive GAN (逐步增长的 GAN，也被称为 PGGAN)：Progressive GAN 的主要特点是它从低分辨率开始训练，然后逐渐添加更多的层来增加分辨率。这种逐步的方法使得模型在初期的训练更稳定，并且能够逐渐适应更高分辨率的图像。这使得模型能够生成高质量且高分辨率的图像。</p>2023-08-14</li><br/><li><span>xueerfei007</span> 👍（0） 💬（2）<p>一口气看完，催更</p>2023-07-27</li><br/><li><span>AI悦创</span> 👍（0） 💬（1）<p>这句：下面我放了两张例子。第一个例子是张大千模仿石涛的画作，第二个例子是贝特莱奇 14 岁时仿照毕加索的画作。假如你是艺术鉴赏家，能否发现这些仿作的破绽呢？

所以破绽是什么？（到这里，我还没往下看，想先提问一下）</p>2023-07-26</li><br/><li><span>peter</span> 👍（0） 💬（1）<p>请教老师几个问题：
Q1：GAN有具体产品吗？
扩散模型有具体产品，比如SD，GAN有具体产品吗？
Q2：GAN的生成器加随机噪声，请问有非随机噪声吗？
Q3：模型的训练一般用什么语言？
Q4：GAN或webUI能制作技术文档上的图吗？比如写一个技术文档，上面有数据链应用图等，可以制作这一类的技术图片吗？</p>2023-07-26</li><br/><li><span>yanyu-xin</span> 👍（5） 💬（0）<p>看不懂伪代码。将代码复制给极客的&quot;AI学习助手“，回答如下：
这段代码是一个用于训练生成对抗网络（GAN）的循环。下面是对代码的逐行分析：

for epoch in range(num_epochs):：外层循环，用于控制训练的轮数。

for batch_data in data_loader:：内层循环，用于遍历训练数据集中的每个批次。

real_images = batch_data.to(device)：将真实图像数据移动到指定的设备（如GPU）上。

z = torch.randn(batch_size, latent_dim).to(device)：从标准正态分布中生成一组随机噪声向量z，并将其移动到指定的设备上。

fake_images = generator(z).detach()：使用生成器网络（generator）和随机噪声z生成一组假图像，并将其与生成器的参数的计算图分离。

d_loss_real = discriminator(real_images)：将真实图像输入判别器网络（discriminator）中，计算真实图像的判别器损失。

d_loss_fake = discriminator(fake_images)：将假图像输入判别器网络中，计算假图像的判别器损失。

d_loss = -(torch.mean(d_loss_real) - torch.mean(d_loss_fake))：计算判别器的总损失，这里使用了最大化判别器对真实图像的预测概率和最小化判别器对假图像的预测概率的策略。

discriminator.zero_grad()：清除判别器的梯度。

d_loss.backward()：反向传播，计算判别器的梯度。

discriminator_optimizer.step()：更新判别器的参数，使用优化器进行梯度下降。

z = torch.randn(batch_size, latent_dim).to(device)：重新生成一组随机噪声向量z，并将其移动到指定的设备上。

fake_images = generator(z)：使用生成器网络和新的随机噪声z生成一组新的假图像。

g_loss = -torch.mean(discriminator(fake_images))：计算生成器的损失，这里使用了最大化判别器对假图像的预测概率的策略。

generator.zero_grad()：清除生成器的梯度。

g_loss.backward()：反向传播，计算生成器的梯度。

generator_optimizer.step()：更新生成器的参数，使用优化器进行梯度下降。

这段代码的作用是通过交替地训练判别器和生成器，使生成器能够生成逼真的假图像，同时判别器能够准确地区分真实图像和假图像。</p>2023-07-29</li><br/><li><span>大将</span> 👍（0） 💬（0）<p>最大化真实图像的损失（d_loss_real）和最小化生成图像的损失（d_loss_fake），个人感觉这个描述并不准确，最大化和最小化容易引起歧义</p>2024-02-06</li><br/><li><span>Geek_0bac2d</span> 👍（0） 💬（0）<p>AI生成的代码解释
for epoch in range(num_epochs):
    for batch_data in data_loader:
        # 更新判别器
        real_images = batch_data.to(device)  # 获取真实图像数据
        z = torch.randn(batch_size, latent_dim).to(device)  # 生成随机噪声向量
        fake_images = generator(z).detach()  # 通过生成器生成假图像，并将其与生成器的梯度计算图分离

        d_loss_real = discriminator(real_images)  # 判别器对真实图像的判别结果
        d_loss_fake = discriminator(fake_images)  # 判别器对假图像的判别结果

        # 判别器损失
        d_loss = -(torch.mean(d_loss_real) - torch.mean(d_loss_fake))  # 计算判别器的损失

        discriminator.zero_grad()  # 清空判别器的梯度
        d_loss.backward()  # 反向传播计算判别器的梯度
        discriminator_optimizer.step()  # 利用优化器更新判别器的参数

        # 更新生成器
        z = torch.randn(batch_size, latent_dim).to(device)  # 生成新的随机噪声向量
        fake_images = generator(z)  # 通过生成器生成新的假图像

        g_loss = -torch.mean(discriminator(fake_images))  # 判别器对新的假图像的判别结果作为生成器的损失

        generator.zero_grad()  # 清空生成器的梯度
        g_loss.backward()  # 反向传播计算生成器的梯度
        generator_optimizer.step()  # 利用优化器更新生成器的参数
</p>2023-07-30</li><br/>
</ul>