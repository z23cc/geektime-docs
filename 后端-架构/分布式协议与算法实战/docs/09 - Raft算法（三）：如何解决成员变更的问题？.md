你好，我是韩健。

在日常工作中，你可能会遇到服务器故障的情况，这时你就需要替换集群中的服务器。如果遇到需要改变数据副本数的情况，则需要增加或移除集群中的服务器。总的来说，在日常工作中，集群中的服务器数量是会发生变化的。

讲到这儿，也许你会问：“老韩，Raft是共识算法，对集群成员进行变更时（比如增加2台服务器），会不会因为集群分裂，出现2个领导者呢？”

在我看来，的确会出现这个问题，因为Raft的领导者选举，建立在“大多数”的基础之上，那么当成员变更时，集群成员发生了变化，就可能同时存在新旧配置的2个“大多数”，出现2个领导者，破坏了Raft集群的领导者唯一性，影响了集群的运行。

而关于成员变更，不仅是Raft算法中比较难理解的一部分，非常重要，也是Raft算法中唯一被优化和改进的部分。比如，最初实现成员变更的是联合共识（Joint Consensus），但这个方法实现起来难，后来Raft的作者就提出了一种改进后的方法，单节点变更（single-server changes）。

为了帮你掌握这块内容，今天我除了带你了解成员变更问题的本质之外，还会讲一下如何通过单节点变更的方法，解决成员变更的问题。学完本讲内容之后，你不仅能理解成员变更的问题和单节点变更的原理，也能更好地理解Raft源码实现，掌握解决成员变更问题的方法。

在开始今天内容之前，我先介绍一下“配置”这个词儿。因为常听到有同学说，自己不理解配置（Configuration）的含义，从而不知道如何理解论文中的成员变更。

的确，配置是成员变更中一个非常重要的概念，我建议你这么理解：它就是在说集群是哪些节点组成的，是集群各节点地址信息的集合。比如节点A、B、C组成的集群，那么集群的配置就是\[A, B, C]集合。

理解了这一点之后，咱们先来看一道思考题。

假设我们有一个由节点A、B、C组成的Raft集群，现在我们需要增加数据副本数，增加2个副本（也就是增加2台服务器），扩展为由节点A、B、C、D、E， 5个节点组成的新集群：

![](https://static001.geekbang.org/resource/image/85/04/853b678cb8a088ce1bc9f91fc62bde04.jpg?wh=1142%2A400)

那么Raft算法是如何保障在集群配置变更时，集群能稳定运行，不出现2个领导者呢？带着这个问题，我们正式进入今天的学习。

老话说得好，“认识问题，才能解决问题”。为了帮你更好地理解单节点变更的方法，我们先来看一看，成员变更时，到底会出现什么样的问题？

## 成员变更的问题

在我看来，在集群中进行成员变更的最大风险是，可能会同时出现2个领导者。比如在进行成员变更时，节点A、B和C之间发生了分区错误，节点A、B组成旧配置中的“大多数”，也就是变更前的3节点集群中的“大多数”，那么这时的领导者（节点A）依旧是领导者。

另一方面，节点C和新节点D、E组成了新配置的“大多数”，也就是变更后的5节点集群中的“大多数”，它们可能会选举出新的领导者（比如节点C）。那么这时，就出现了同时存在2个领导者的情况。

![](https://static001.geekbang.org/resource/image/82/9e/827a4616e65633015c1f77f3425b1a9e.jpg?wh=1142%2A293)

如果出现了2个领导者，那么就违背了“领导者的唯一性”的原则，进而影响到集群的稳定运行。你要如何解决这个问题呢？也许有的同学想到了一个解决方法。

因为我们在启动集群时，配置是固定的，不存在成员变更，在这种情况下，Raft的领导者选举能保证只有一个领导者。也就是说，这时不会出现多个领导者的问题，那我可以先将集群关闭再启动新集群啊。也就是先把节点A、B、C组成的集群关闭，然后再启动节点A、B、C、D、E组成的新集群。

**在我看来，这个方法不可行。** 为什么呢？因为你每次变更都要重启集群，意味着在集群变更期间服务不可用，肯定不行啊，太影响用户体验了。想象一下，你正在玩王者荣耀，时不时弹出一个对话框通知你：系统升级，游戏暂停3分钟。这体验糟糕不糟糕？

既然这种方法影响用户体验，根本行不通，那到底怎样解决成员变更的问题呢？**最常用的方法就是单节点变更。**

## 如何通过单节点变更解决成员变更的问题？

单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将3节点集群扩容为5节点集群，这时你需要执行2次单节点变更，先将3节点集群变更为4节点集群，然后再将4节点集群变更为5节点集群，就像下图的样子。

![](https://static001.geekbang.org/resource/image/7e/55/7e2b1caf3c68c7900d6a7f71e7a3a855.jpg?wh=1142%2A790)

现在，让我们回到开篇的思考题，看看如何用单节点变更的方法，解决这个问题。为了演示方便，我们假设节点A是领导者：

![](https://static001.geekbang.org/resource/image/25/40/25cabfbad4627ec4c39b8d32a567d440.jpg?wh=1142%2A670)

目前的集群配置为\[A, B, C]，我们先向集群中加入节点D，这意味着新配置为\[A, B, C, D]。成员变更，是通过这么两步实现的：

- 第一步，领导者（节点A）向新节点（节点D）同步数据；
- 第二步，领导者（节点A）将新配置\[A, B, C, D]作为一个日志项，复制到新配置中所有节点（节点A、B、C、D）上，然后将新配置的日志项应用（Apply）到本地状态机，完成单节点变更。

![](https://static001.geekbang.org/resource/image/7f/07/7f687461706f3b226d79a55b618e4c07.jpg?wh=1142%2A486)

在变更完成后，现在的集群配置就是\[A, B, C, D]，我们再向集群中加入节点E，也就是说，新配置为\[A, B, C, D, E]。成员变更的步骤和上面类似：

- 第一步，领导者（节点A）向新节点（节点E）同步数据；
- 第二步，领导者（节点A）将新配置\[A, B, C, D, E]作为一个日志项，复制到新配置中的所有节点（A、B、C、D、E）上，然后再将新配置的日志项应用到本地状态机，完成单节点变更。

![](https://static001.geekbang.org/resource/image/7d/43/7d3b5da84db682359ab82579fdd2e243.jpg?wh=1142%2A427)

这样一来，我们就通过一次变更一个节点的方式，完成了成员变更，保证了集群中始终只有一个领导者，而且集群也在稳定运行，持续提供服务。

我想说的是，在正常情况下，**不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的。** 也就是说，不会同时存在旧配置和新配置2个“大多数”：

![](https://static001.geekbang.org/resource/image/5f/b8/5fe7c8d90857737d7314263eae2166b8.jpg?wh=1142%2A906)![](https://static001.geekbang.org/resource/image/4a/27/4a00b7e1b89922cd9f785c6f153aca27.jpg?wh=1142%2A899)

从上图中你可以看到，不管集群是偶数节点，还是奇数节点，不管是增加节点，还是移除节点，新旧配置的“大多数”都会存在重叠（图中的橙色节点）。

需要你注意的是，在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现2个领导者的情况。

如果你遇到这种情况，可以在领导者启动时，创建一个NO\_OP日志项（也就是空日志项），只有当领导者将NO\_OP日志项应用后，再执行成员变更请求。这个解决办法，你记住就可以了，可以自己在课后试着研究下。具体的实现，可参考Hashicorp Raft的源码，也就是runLeader()函数中：

```
noop := &logFuture{
        log: Log{
               Type: LogNoop,
        },
}
r.dispatchLogs([]*logFuture{noop})
```

当然，有的同学会好奇“联合共识”，在我看来，因为它难以实现，很少被Raft实现采用。比如，除了Logcabin外，未见到其他常用Raft实现采用了它，所以这里我就不多说了。如果你有兴趣，可以自己去阅读论文，加深了解。

## 内容小结

以上就是本节课的全部内容了，本节课我主要带你了解了成员变更的问题和单节点变更的方法，我希望你明确这样几个重点。

1. 成员变更的问题，主要在于进行成员变更时，可能存在新旧配置的2个“大多数”，导致集群中同时出现两个领导者，破坏了Raft的领导者的唯一性原则，影响了集群的稳定运行。
2. 单节点变更是利用“一次变更一个节点，不会同时存在旧配置和新配置2个‘大多数’”的特性，实现成员变更。
3. 因为联合共识实现起来复杂，不好实现，所以绝大多数Raft算法的实现，采用的都是单节点变更的方法（比如Etcd、Hashicorp Raft）。其中，Hashicorp Raft单节点变更的实现，是由Raft算法的作者迭戈·安加罗（Diego Ongaro）设计的，很有参考价值。

除此之外，考虑到本节课是Raft算法的最后一讲，所以在这里，我想多说几句，帮助你更好地理解Raft算法。

有很多同学把Raft当成一致性算法，其实Raft不是一致性算法而是共识算法，是一个Multi-Paxos算法，实现的是如何就一系列值达成共识。并且，Raft能容忍少数节点的故障。虽然Raft算法能实现强一致性，也就是线性一致性（Linearizability），但需要客户端协议的配合。在实际场景中，我们一般需要根据场景特点，在一致性强度和实现复杂度之间进行权衡。比如Consul实现了三种一致性模型。

- default：客户端访问领导者节点执行读操作，领导者确认自己处于稳定状态时（在leader leasing时间内），返回本地数据给客户端，否则返回错误给客户端。在这种情况下，客户端是可能读到旧数据的，比如此时发生了网络分区错误，新领导者已经更新过数据，但因为网络故障，旧领导者未更新数据也未退位，仍处于稳定状态。
- consistent：客户端访问领导者节点执行读操作，领导者在和大多数节点确认自己仍是领导者之后返回本地数据给客户端，否则返回错误给客户端。在这种情况下，客户端读到的都是最新数据。
- stale：从任意节点读数据，不局限于领导者节点，客户端可能会读到旧数据。

一般而言，在实际工程中，Consul的consistent就够用了，可以不用线性一致性，只要能保证写操作完成后，每次读都能读到最新值就可以了。比如为了实现幂等操作，我们使用一个编号(ID)来唯一标记一个操作，并使用一个状态字段（nil/done）来标记操作是否已经执行，那么只要我们能保证设置了ID对应状态值为done后，能立即和一直读到最新状态值就可以了，也就通过防止操作的重复执行，实现了幂等性。

总的来说，Raft算法能很好地处理绝大部分场景的一致性问题，我推荐你在设计分布式系统时，优先考虑Raft算法，当Raft算法不能满足现有场景需求时，再去调研其他共识算法。

比如我负责过多个QQ后台的海量服务分布式系统，其中配置中心、名字服务以及时序数据库的META节点，采用了Raft算法。在设计时序数据库的DATA节点一致性时，基于水平扩展、性能和数据完整性等考虑，就没采用Raft算法，而是采用了Quorum NWR、失败重传、反熵等机制。这样安排不仅满足了业务的需求，还通过尽可能采用最终一致性方案的方式，实现系统的高性能，降低了成本。

## 课堂思考

在最后，我给你留了一个思考题，强领导者模型会限制集群的写性能，那你想想看，有什么办法能突破Raft集群的写性能瓶颈呢？欢迎在留言区分享你的看法，与我一同讨论。

最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。
<div><strong>精选留言（15）</strong></div><ul>
<li><span>每天晒白牙</span> 👍（62） 💬（5）<p>可以参考Kafka的分区和ES的主分片副本分片这种机制，虽然写入只能通过leader写，但每个leader可以负责不同的片区，来提高写入的性能</p>2020-03-02</li><br/><li><span>XHH</span> 👍（35） 💬（2）<p>1、leader可以合并请求
2、leader提交日志和日志复制RPC两个步骤可以并行和批量处理 
3、leader可以并发和异步对所有follower 并发日志复制，同时可以利用pipeline的方式提高效率
4、每个节点启动多个raft实例，对请求进行hash或者range后，让每个raft实例负责部分请求
</p>2020-03-02</li><br/><li><span>竹马彦四郎的好朋友影法師</span> 👍（27） 💬（2）<p>韩老师您好~ (A,B,C) 三个旧配置A是leader，然后加入D，然后网络分区为(A,B)和(C,D), 那么A依旧赢得大多数选票而是leader.  C因为分区，所以也开始发起选举，赢得了C、D的选票，注意哦~ ，C也是旧配置哦~ 那么C不也就成为leader了么?  所以不就出现了2个leader了么?  也就是D作为新配置固然无法成为leader，但是C作为旧配置还是可以成为leader的呀~ 希望您能指点一下. 但是我说一下我的看法，我的看法是——您一开始在&quot;成员变更的问题&quot;中举的例子貌似有点问题——应该是C、D、E中的D或者E会成为新配置中的leader而不是C节点会成为新配置中的leader，因为C的（旧）配置中原本就没有D、E，它即便获取到D、E的选票也不能认为自己得票过半，这样就能解释的通了。</p>2020-05-04</li><br/><li><span>kylexy_0817</span> 👍（18） 💬（1）<p>韩老师，有个问题，其实在实际生产环境中，是不是应该尽量避免网络分区才是重点，例如把某个集群的机器，尽量放在同一个内网中。举个我想到的例子，ABC三个节点，A在网络1，BC在网络2，初始化时，A成为了领导者，后来在网络2的单节点D加入集群，此时正好出现网络分区，BCD重新重新选举，得到B是领导者，后面网络通讯恢复了，这样即使采用单节点变更的方式，不也同样会出现了脑裂了吗？不知道我的理解正不正确，求解答。</p>2020-04-19</li><br/><li><span>黄海峰</span> 👍（14） 💬（2）<p>老师，这种共识算法只是用于p2p的分布式系统吧，像hadoop&#47;spark这些大数据分布式系统都是主从模式，部署就决定了谁是master，根本就不用这些共识算法了。。。相对比主从模式更可靠更可控啊，因为没有这些这么复杂的选举逻辑。。除了区块链，其他系统用p2p是不是有什么不可取代的必要性呢？</p>2020-03-03</li><br/><li><span>朱东旭</span> 👍（9） 💬（2）<p>一致性算法与共识算法的区别是啥，raft以领导者的日志为准，不就是保证了数据的最终一致吗。</p>2020-03-03</li><br/><li><span>Geek_zbvt62</span> 👍（9） 💬（7）<p>一般而言，在实际工程中，Consul 的 consistent 就够用了，可以不用线性一致性，

这句话是不是笔误了？</p>2020-03-02</li><br/><li><span>static</span> 👍（8） 💬（7）<p>老师好，想请教一个困扰我很久的关于Raft算法的一个问题。
在分布式锁场景下（使用Raft算法），A客户端向leader申请获取锁（set lock），此时leader应用lock信息日志，并RPC复制日志信息给follower节点，此时follower节点还没应用到状态机，leader收到大部分follower成功信息，自己应用了状态机并返回客户端说set lock成功，但此时leader宕机了，其中一个follower变为leader，此时客户端B来获取锁，发现leader没有lock信息（因为follower将lock信息应用到状态机靠leader心跳传递，但刚刚leader宕机了没来得及传递），客户端B此时获取锁也成功了，这不就破坏了锁的同步性吗？Raft算法是如何保证这种场景下的强一致性（线性一致性）？</p>2020-03-21</li><br/><li><span>Kvicii.Y</span> 👍（7） 💬（2）<p>NO_OP这个空日志项该怎么理解呢，为了防止出现多个领导者？怎么防止的呢</p>2020-06-21</li><br/><li><span>ξ！</span> 👍（6） 💬（1）<p>老师，在配置单节点加入的时候，是怎么发现当前集群的呢，难道是在配置的时候就将集群的节点信息写入了么，即便这样的话，当前节点是怎么发现当前集群的领导者呢，在新节点加入的时候他是怎么知道当前集群的领导者的呢，这个发现领导者的过程是新节点主动发起的rpc还是领导者的心跳发现的呢</p>2020-11-17</li><br/><li><span>慢动作</span> 👍（5） 💬（3）<p>老师好，有个疑问，集群加入节点都是通过leader处理的，那文章开头3节点到5节点，为什么a还是旧配置，c却是新配置？</p>2020-06-04</li><br/><li><span>刘学</span> 👍（5） 💬（2）<p>韩老师你好，我想到的可以解决因为强领导者导致的写性能瓶颈的办法是多分片，这样多个raft流程并发执行，不同的分片的master落在不同的机器上就可以很好的解决这个问题。在加入新节点后的第一步时，主节点向新加入的节点同步数据，那就意味着主节点需要支持向非本组成员的节点同步数据的功能对么？</p>2020-03-11</li><br/><li><span>华子</span> 👍（3） 💬（2）<p>1. 所谓的＂配置＂就是指集群中的节点要知道新加入节点的IP地址等信息吗？
2. 而之所以不能一次性加入两台或以上的节点，是因为无法保证＂同时＂加入？</p>2020-04-18</li><br/><li><span>旅途</span> 👍（3） 💬（1）<p>老师  麻烦解答下 如果旧配置的大多数和新的大多数数量相等 并且有重叠的 这时候为什么不会产生两个领导者呢 是因为只能选重叠的做领导者吗?如果新的大多数数量大于旧配置 ,领导者就会在新的大多数中产生吗?</p>2020-03-09</li><br/><li><span>Infinite_gao</span> 👍（3） 💬（2）<p>老师讲的深入浅出，可是有个疑问，为什么配置从老的阶段到中间阶段再到新阶段的这个过程没有进行阐述呢，配置的自动转换过程对于理解细节非常重要。C(old)-&gt;C(old,new)-&gt;C(new)。
还有就是新加入的节点开始是通过什么类型的消息与原leader通信的，通信的信息细节是什么，是选举请求吗？</p>2020-03-07</li><br/>
</ul>