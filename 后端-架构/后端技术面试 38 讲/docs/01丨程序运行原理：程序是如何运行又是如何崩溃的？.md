软件的核心载体是程序代码，软件开发的主要工作产出也是代码，但是代码被存储在磁盘上本身没有任何价值，软件要想实现价值，代码就必须运行起来。那么代码是如何运行的？在运行中可能会出现什么问题呢？

## 程序是如何运行起来的

软件被开发出来，是文本格式的代码，这些代码通常不能直接运行，需要使用编译器编译成操作系统或者虚拟机可以运行的代码，即可执行代码，它们都被存储在文件系统中。不管是文本格式的代码还是可执行的代码，都被称为**程序**，程序是静态的，安静地呆在磁盘上，什么也干不了。要想让程序处理数据，完成计算任务，必须把程序从外部设备加载到内存中，并在操作系统的管理调度下交给CPU去执行，去运行起来，才能真正发挥软件的作用，程序运行起来以后，被称作**进程**。

进程除了包含可执行的程序代码，还包括进程在运行期使用的内存堆空间、栈空间、供操作系统管理用的数据结构。如下图所示：

![](https://static001.geekbang.org/resource/image/89/98/89c6e3bbc44cdc042e7a8bcddb3b4398.png?wh=920%2A852)  
操作系统把可执行代码加载到内存中，生成相应的数据结构和内存空间后，就从可执行代码的起始位置读取指令交给CPU顺序执行。指令执行过程中，可能会遇到一条跳转指令，即CPU要执行的下一条指令不是内存中可执行代码顺序的下一条指令。编程中使用的循环for…，while…和if…else…最后都被编译成跳转指令。

程序运行时如果需要创建数组等数据结构，操作系统就会在进程的**堆空间**申请一块相应的内存空间，并把这块内存的首地址信息记录在进程的栈中。堆是一块无序的内存空间，任何时候进程需要申请内存，都会从堆空间中分配，分配到的内存地址则记录在栈中。

栈是严格的一个后进先出的数据结构，同样由操作系统维护，主要用来记录函数内部的局部变量、堆空间分配的内存空间地址等。

我们以如下代码示例，描述函数调用过程中，栈的操作过程：

```
void f(){
  int x = g(1);
  x++; //g函数返回，当前堆栈顶部为f函数栈帧，在当前栈帧继续执行f函数的代码。
}
int g(int x){
  return x + 1;
}
```

每次函数调用，操作系统都会在栈中创建一个栈帧（stack frame）。正在执行的函数参数、局部变量、申请的内存地址等都在当前栈帧中，也就是堆栈的顶部栈帧中。如下图所示：

![](https://static001.geekbang.org/resource/image/f0/f7/f08d6fca893da5cac926a23f1f1aa7f7.png?wh=1450%2A468)  
当f函数执行的时候，f函数就在栈顶，栈帧中存储着f函数的局部变量，输入参数等等。当f函数调用g函数，当前执行函数就变成g函数，操作系统会为g函数创建一个栈帧并放置在栈顶。当函数g()调用结束，程序返回f函数，g函数对应的栈帧出栈，顶部栈帧变又为f函数，继续执行f函数的代码，也就是说，真正执行的函数永远都在栈顶。而且因为栈帧是隔离的，所以不同函数可以定义相同的变量而不会发生混乱。

## 一台计算机如何同时处理数以百计的任务

我们自己日常使用的PC计算机通常只是一核或者两核的CPU，我们部署应用程序的服务器虽然有更多的CPU核心，通常也不过几核或者几十核。但是我们的PC计算机可以同时编程、听音乐，而且还能执行下载任务，而服务器则可以同时处理数以百计甚至数以千计的**并发**用户请求。

那么为什么一台计算机服务器可以同时处理数以百计，以千计的计算任务呢？这里主要依靠的是操作系统的CPU分时共享技术。如果同时有很多个进程在执行，操作系统会将CPU的执行时间分成很多份，进程按照某种策略轮流在CPU上运行。由于现代CPU的计算能力非常强大，虽然每个进程都只被执行了很短一个时间，但是在外部看来却好像是所有的进程都在同时执行，每个进程似乎都独占一个CPU执行。

所以虽然从外部看起来，多个进程在同时运行，但是在实际物理上，进程并不总是在CPU上运行的，一方面进程共享CPU，所以需要等待CPU运行，另一方面，进程在执行I/O操作的时候，也不需要CPU运行。进程在生命周期中，主要有三种状态，运行、就绪、阻塞。

- **运行**：当一个进程在CPU上运行时，则称该进程处于运行状态。处于运行状态的进程的数目小于等于CPU的数目。
- **就绪**：当一个进程获得了除CPU以外的一切所需资源，只要得到CPU即可运行，则称此进程处于就绪状态，就绪状态有时候也被称为等待运行状态。
- **阻塞**：也称为等待或睡眠状态，当一个进程正在等待某一事件发生（例如等待I/O完成，等待锁……）而暂时停止运行，这时即使把CPU分配给进程也无法运行，故称该进程处于阻塞状态。

不同进程轮流在CPU上执行，每次都要进行进程间CPU切换，代价是非常大的，实际上，每个用户请求对应的不是一个进程，而是一个线程。线程可以理解为轻量级的进程，在进程内创建，拥有自己的线程栈，在CPU上进行线程切换的代价也更小。线程在运行时，和进程一样，也有三种主要状态，从逻辑上看，进程的主要概念都可以套用到线程上。我们在进行服务器应用开发的时候，通常都是多线程开发，理解线程对我们设计、开发软件更有价值。

## 系统为什么会变慢，为什么会崩溃

现在的服务器软件系统主要使用多线程技术实现多任务处理，完成对很多用户的并发请求处理。也就是我们开发的应用程序通常以一个进程的方式在操作系统中启动，然后在进程中创建很多线程，每个线程处理一个用户请求。

以Java的web开发为例，似乎我们编程的时候通常并不需要自己创建和启动线程，那么我们的程序是如何被多线程并发执行，同时处理多个用户请求的呢？实际中，启动多线程，为每个用户请求分配一个处理线程的工作是在web容器中完成的，比如常用的Tomcat容器。

如下图所示：

![](https://static001.geekbang.org/resource/image/d4/9a/d40cc1e9a2a5ce3913670743f0543b9a.png?wh=1168%2A808)  
Tomcat启动多个线程，为每个用户请求分配一个线程，调用和请求URL路径相对应的Servlet（或者Controller）代码，完成用户请求处理。而Tomcat则在JVM虚拟机进程中，JVM虚拟机则被操作系统当做一个独立进程管理。真正完成最终计算的，是CPU、内存等服务器硬件，操作系统将这些硬件进行分时（CPU）、分片（内存）管理，虚拟化成一个独享资源让JVM进程在其上运行。

以上就是一个Java web应用运行时的主要**架构**，有时也被称作**架构过程视图**。需要注意的是，这里有个很多web开发者容易忽略的事情，那就是**不管你是否有意识，你开发的web程序都是被多线程执行的，web开发天然就是多线程开发**。

CPU以线程为单位进行分时共享执行，可以想象代码被加载到内存空间后，有多个线程在这些代码上执行，这些线程从逻辑上看，是同时在运行的，每个线程有自己的线程栈，所有的线程栈都是完全隔离的，也就是每个方法的参数和方法内的局部变量都是隔离的，一个线程无法访问到其他线程的栈内数据。

但是当某些代码修改内存堆里的数据的时候，如果有多个线程在同时执行，就可能会出现同时修改数据的情况，比如，两个线程同时对一个堆中的数据执行+1操作，最终这个数据只会被加一次，这就是人们常说的**线程安全**问题，实际上线程的结果应该是依次加一，即最终的结果应该是+2。

多个线程访问共享资源的这段代码被称为**临界区**，解决线程安全问题的主要方法是使用锁，将临界区的代码加锁，只有获得锁的线程才能执行临界区代码，如下：

```
lock.lock();  //线程获得锁
i++;  //临界区代码，i位于堆中
lock.unlock();  //线程释放锁
```

如果当前线程执行到第一行，获得锁的代码的时候，锁已经被其他线程获取并没有释放，那么这个线程就会进入阻塞状态，等待前面释放锁的线程将自己唤醒重新获得锁。

锁会引起线程阻塞，如果有很多线程同时在运行，那么就会出现线程排队等待锁的情况，线程无法并行执行，系统响应速度就会变慢。此外I/O操作也会引起阻塞，对数据库连接的获取也可能会引起阻塞。目前典型的web应用都是基于RDBMS关系数据库的，web应用要想访问数据库，必须获得数据库连接，而受数据库资源限制，每个web应用能建立的数据库的连接是有限的，如果并发线程数超过了连接数，那么就会有部分线程无法获得连接而进入阻塞，等待其他线程释放连接后才能访问数据库，并发的线程数越多，等待连接的时间也越多，从web请求者角度看，响应时间变长，**系统变慢**。

被阻塞的线程越多，占据的系统资源也越多，这些被阻塞的线程既不能继续执行，也不能释放当前已经占据的资源，在系统中一边等待一边消耗资源，如果阻塞的线程数超过了某个系统资源的极限，就会导致系统宕机，**应用崩溃**。

解决系统因高并发而导致的响应变慢、应用崩溃的主要手段是使用**分布式系统架构**，用更多的服务器构成一个集群，以便共同处理用户的并发请求，保证每台服务器的并发负载不会太高。此外必要时还需要在请求入口处进行**限流**，减小系统的并发请求数；在应用内进行业务**降级**，减小线程的资源消耗。高并发系统架构方案将在专栏的第三模块中进一步探讨。

## 小结

事实上，现代CPU和操作系统的设计远比这篇文章讲的要复杂得多，但是基础原理大致就是如此。为了让程序能很好地被执行，软件开发的时候要考虑很多情况，为了让软件能更好地发挥效能，需要在部署上进行规划和架构。软件是如何运行的，应该是软件工程师和架构师的常识，在设计开发软件的时候，应该时刻以常识去审视自己的工作，保证软件开发在正确的方向上前进。

## 思考题

线程安全的临界区需要依靠锁，而锁的获取必须也要保证自己是线程安全的，也就是说，不能出现两个线程同时得到锁的情况，那么锁是如何保证自己是线程安全的呢？或者说，在操作系统以及CPU层面，锁是如何实现的？

你不妨思考一下这个问题，把你的思考写在下面的评论区里，我会和你一起交流。也欢迎你把这篇文章分享给你的朋友或者同事，一起交流一下。
<div><strong>精选留言（15）</strong></div><ul>
<li><span>斐波那契</span> 👍（106） 💬（5）<p>在java里 锁是通过cas把当前线程id刷新到对象的头信息里 在获取锁时会去头信息里拿这个信息 如果没有 则会cas刷新进去 刷新成功就获取到锁 刷新失败就表明有别的线程也在尝试刷新这个信息 在操作系统层面 有pv操作保证原子性 而pv操作也是利用cpu中原语指令 在获取锁时保证不会被别的指令打断（或被重排序）</p>2019-11-18</li><br/><li><span>Luciano李鑫</span> 👍（40） 💬（9）<p>“不管你是否有意识，你开发的 web 程序都是被多线程执行的，web 开发天然就是多线程开发。”这个会不会绝对了一些，比如go或者c++开发的没有额外创建线程的web程序呢？</p>2019-11-18</li><br/><li><span>Allen_</span> 👍（24） 💬（7）<p>有些地方可能不是很到位，欢迎补充
小结：
1. 我们平时开发出来的程序是文本格式代码，但只是在硬盘中还只是一个程序，只有加载到内存里面通过cpu执行成为进程才是发挥了程序作用。
2.进程里面有堆，栈，可执行代码和进程数据结构。
3.cpu分时共享技术进行并发操作，进程切换效率不高，所以有了线程切换
4.因为线程安全问题引入锁，不过也引入了更多造成阻塞的可能
5.线程阻塞可能是I&#47;O,锁，网络请求，数据库链接获取
6利用分布式系统架构来减缓高并发的性能不佳</p>2019-11-18</li><br/><li><span>蚂蚁内推+v</span> 👍（15） 💬（2）<p>李老师好：我想请教下，一个JVM 是一个进程。JVM 上跑 tomcat，tomcat 可以部署多个应用？那每个跑在tomcat 上的应用是一个线程吗？那一个应用crash了，其他应用也会crash.这块感觉有点问题。不知道老师方便解释下吗？</p>2019-11-18</li><br/><li><span>我爱布丁</span> 👍（12） 💬（1）<p>老师，看完文章，联想到两个关于协程的问题：

1. 使用协程在出现IO等待时，程序会自己调度去执行其他的(CPU)任务。理论上这样可以避免额外的IO等待导致的线程间切换。我的问题是从系统的角度上看，使用协程可以抢占到更多的CPU时间片吗？

2. 感觉系统崩溃（除人为Bug外) 主要是系统资源不足导致的。那么即使用轻量级的协程也不会变得更好。因为当协程数量过多，导致event loop过大，变慢，系统还是要崩溃的对吗？
</p>2019-11-21</li><br/><li><span>雷咏</span> 👍（12） 💬（3）<p>我们用文本格式书写的程序有三种执行方式:
1.解释执行。例子是脚本语言书写的程序或类似于BASIC语言书写的程序。著名的PYTHON也属于这种情况。
2.编译执行。通常C&#47;C++程序属于这种情况。文本格式书写的程序称之为源程序，需要编译器编译成机器语言代码，称之为可执行程序一或目标程序。
3.虚拟机执行。将文本格式的程序先编译成一种中间代码，然后由驻留在计算机中的虚拟机解释执行。例子是通常的JAVA程序。
</p>2019-11-19</li><br/><li><span>探索无止境</span> 👍（6） 💬（1）<p>希望老师在第二节课可以谈谈上一节课留下的思考题，您是怎么理解分析的</p>2019-11-19</li><br/><li><span>peter</span> 👍（4） 💬（2）<p>一直以为tomcat是一个独立的进程。根据本文所述，tomcat只是一个线程，是虚拟机进程中的一个线程。是这样吗？</p>2019-11-24</li><br/><li><span>Heidi</span> 👍（4） 💬（3）<p>你好，想提个问题。文章中大部分知识点都掌握，但是遇到问题的时候没有从这些角度出发，只是跟着一些关联去分析问题，对遇到的问题反应比较慢。这种情况是不是知识没有成体系？那么怎样建立比较完整的知识体系呢？</p>2019-11-20</li><br/><li><span>Tobe24</span> 👍（4） 💬（3）<p>您好，老师，这节课太适合我这种新手了，这里有三个不太明白的地方，希望老师能够解惑。
问题1：CPU 分时共享技术同时执行进程的数量，取决于什么？
问题2：为什么线程切换的代价更小？
问题3：进程切换是不是必须要等到线程切换完毕后进行？如果不是，优先级是由什么决定的？
一点小建议：
有一些表达程度的词，如果能用数据举例简单说明一下，对于我们理解会更有帮助。比如2问题中，代价更小，小到什么程度，是进程切换速度的几倍?
思考题：
作为小白，我的思路是这样，锁是在线程的临界区，线程是在进程的线程栈，而 一个 cpu 同时只能运行一个进程，所以本质上都是轮流执行的……于是，只要保证在获取锁的时候，锁不在正在获取或已经被获取的状态即可，进而推断线程中会有一片内存区域用来存这些状态信息。
😂不知道这个思路对不对。
最后谢谢老师。</p>2019-11-20</li><br/><li><span>布拉姆</span> 👍（1） 💬（2）<p>“被阻塞的线程越多，占据的系统资源也越多，这些被阻塞的线程既不能继续执行，也不能释放当前已经占据的资源”被阻塞的线程并不消耗cpu（或者说时间片）对吧？占据的资源是IO类的，比如文件描述符或者IP port之类的吗？</p>2021-02-19</li><br/><li><span>AidenLiang</span> 👍（1） 💬（2）<p>老师，别人常说java和go等是带有运行时的语言，难道c和cpp没有吗？运行时是指垃圾回收这些功能吗？</p>2019-11-18</li><br/><li><span>李郝</span> 👍（0） 💬（1）<p>很多年前看过智慧老师的那本大型网络技术架构，大致读了两遍，每次都获益匪浅，确实因为个人水平不足，很需要智慧老师这种能帮我将很多杂乱的知识言简意赅的融会贯通起来，让我对于未来遇到的问题可以更自信的去解决。同时，通过这些文章，也让我学会了如何提高自己学习的效率，以及如何去训练自己以达到目标等等，非常感谢！</p>2020-11-13</li><br/><li><span>志江</span> 👍（0） 💬（1）<p>李老师,
我理解tomcat就是一个jvm进程吧, tomcat启动的时候实际上是启动一个jvm进程(包含)tomcat字节码文件)
这么理解对么</p>2020-09-09</li><br/><li><span>静水流深</span> 👍（0） 💬（1）<p>老师，您好，闭包函数的执行栈帧如何描述？</p>2019-11-29</li><br/>
</ul>