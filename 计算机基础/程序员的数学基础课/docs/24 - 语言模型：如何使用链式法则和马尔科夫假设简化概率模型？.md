你好，我是黄申。

之前我给你介绍了用于分类的朴素贝叶斯算法。我们讲了，朴素贝叶斯算法可以利用贝叶斯定理和变量之间的独立性，预测一篇文章属于某个分类的概率。除了朴素贝叶斯分类，概率的知识还广泛地运用在其他机器学习算法中，例如语言模型、马尔科夫模型、决策树等等。

今天我就来说说，基于概率和统计的语言模型。语言模型在不同的领域、不同的学派都有不同的定义和实现，因此为了避免歧义，我这里先说明一下，我们谈到的语言模型，都是指基于概率和统计的模型。

## 语言模型是什么？

在解释语言模型之前，我们先来看两个重要的概念。第一个是链式法则，第二个是马尔科夫假设及其对应的多元文法模型。为什么要先说这两个概念呢？这是因为链式法则可以把联合概率转化为条件概率，而马尔科夫假设通过变量间的独立性来减少条件概率中的随机变量，两者结合就可以大幅简化计算的复杂度。

### 1.链式法则

链式法则是概率论中一个常用法则。它使用一系列条件概率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。

![](https://static001.geekbang.org/resource/image/de/0c/de5d37d9392a18225b4f9d522b4f180c.png?wh=554%2A42)

其中，$x\_{1}$到$x\_{n}$表示了n个随机变量。

这个公式是怎么来的呢？你还记得联合概率、条件概率和边缘概率之间的“三角”关系吗？我们用这三者的关系来推导一下，最终我们可以得到链式法则。

![](https://static001.geekbang.org/resource/image/35/a2/35bbc4bd80d8fba7bd6a63c1cdbd86a2.png?wh=475%2A149)

推导的每一步，都是使用了三种概率之间的关系，这个应该不难理解。

### 2.马尔科夫假设

理解了链式法则，我们再来看看马尔可夫假设。这个假设的内容是：任何一个词$w\_{i}$出现的概率只和它前面的1个或若干个词有关。基于这个假设，我们可以提出**多元文法（Ngram）模型**。Ngram中的“N”很重要，它表示任何一个词出现的概率，只和它前面的N-1个词有关。

我以二元文法模型为例，来给你解释。按照刚才的说法，二元文法表示，某个单词出现的概率只和它前面的1个单词有关。也就是说，即使某个单词出现在一个很长的句子中，我们也只需要看前面那1个单词。用公式来表示出来就是这样：

![](https://static001.geekbang.org/resource/image/b0/e9/b0bc4bb97ba4b233e0ae9db0291059e9.png?wh=263%2A39)

如果是三元文法，就说明某个单词出现的概率只和它前面的2个单词有关。即使某个单词出现在很长的一个句子中，它也只看相邻的前2个单词。用公式来表达就是这样：

![](https://static001.geekbang.org/resource/image/a6/94/a619504d5c698f0c519904cd6eddf094.png?wh=302%2A37)

你也许会好奇，那么一元文法呢？按照字面的意思，就是每个单词出现的概率和前面0个单词有关。这其实说明，每个词的出现都是相互独立的。用公式来表达就是这样的：

![](https://static001.geekbang.org/resource/image/52/25/523ff06450418d4d91d8e6d2756e1025.png?wh=400%2A40)

弄明白链式法则和马尔科夫假设之后，我们现在来看语言模型。

假设我们有一个统计样本文本$d$，$s$表示某个有意义的句子，由一连串按照特定顺序排列的词$w\_{1}，w\_{2},…,w\_{n}$组成，这里$n$是句子里单词的数量。现在，我们想知道根据文档d的统计数据，$s$在文本中出现的可能性，即$P(s|d)$，那么我们可以把它表示为$P(s|d)=P(w\_{1}, w\_{2}, …, w\_{n} | d$)。假设我们这里考虑的都是在集合d的情况下发生的概率，所以可以忽略$d$，写为$P(s)=P(w\_{1}, w\_{2}, …, w\_{n})$。

到这里，我们碰到了第一个难题，就是如何计算$P(w\_{1}, w\_{2}, …, w\_{n})$要在集合中找到一模一样的句子，基本是不可能的。这个时候，我们就需要使用链式法则。我们可以把这个式子改写为：

![](https://static001.geekbang.org/resource/image/2f/23/2fe51ac1be8efbad2082627860aada23.png?wh=733%2A40)

乍一看，问题似乎是解决了。因为通过文档集合C，你可以知道$P(w\_{1})$，$P(w\_{2}|w\_{1})$这种概率。不过，再往后看，好像$P(w\_{3}|w\_{1},w\_{2})$出现概率很低，$P(w\_{4}|w\_{1},w\_{2},w\_{3})$出现的概率就更低了。一直到$P(w\_{n}|w\_{1}, w\_{2}, …, w\_{n-1})$，基本上又为0了。我们可以使用上一节提到的平滑技巧，减少0概率的出现。不过，如果太多的概率都是通过平滑的方式而得到的，那么模型和真实的数据分布之间的差距就会加大，最终预测的效果也会很差，所以平滑也不是解决0概率的最终办法。

除此之外，$P(w\_{1}, w\_{2}, …, w\_{n})和P(w\_{n}|w\_{1}, w\_{2}, …, w\_{n-1})$还不只会导致0概率，它还会使得模型存储空间的急速增加。

为了统计现有文档集合中$P(w\_{1}, w\_{2}, …, w\_{n})$这类值，我们就需要生成很多的计数器。我们假设文档集合中有m个不同的单词，那么从中挑出$n$个单词的可重复排列，数量就是$m^{n}$。此外，还有$m^{n-1}$, $m^{n-2}$等等。这也意味着，如果要统计并存储的所有$P(w\_{1}, w\_{2}, …, w\_{n})$或$P(w\_{n}|w\_{1}, w\_{2}, …, w\_{n-1})$这类概率，就需要大量的内存和磁盘空间。当然，你可以做一些简化，不考虑单词出现的顺序，那么问题就变成了可重复组合，但是数量仍然非常巨大。

如何解决0概率和高复杂度的问题呢？马尔科夫假设和多元文法模型能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：

![](https://static001.geekbang.org/resource/image/6d/b1/6dfe1f3c267787677090a611fee964b1.png?wh=651%2A38)

这样，系统的复杂度大致在(C(m, 1) + C(m, 2) + C(m, 3))这个数量级，而且$P(w\_{n}|w\_{n-2}, w\_{n-1})$为0的概率也会大大低于$P(w\_{n}|w\_{1}, w\_{2}, …, w\_{n-1})$ （其中$n&gt;&gt;3$）为0的概率。当然，多元文法模型中的N还是不能太大。随着N的增大，系统复杂度仍然会快速升高，就无法体现出多元文法的优势了。

## 语言模型的应用

基于概率的语言模型，本身不是新兴的技术。它已经在机器翻译、语音识别和中文分词中得到了成功应用。近几年来，人们也开始在信息检索领域中尝试语言模型。下面我就来讲讲语言模型在信息检索和中文分词这两个方面是如何发挥作用的。

### 1.信息检索

信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？为了解决相关性问题，布尔模型和向量空间检索模型都是从查询的角度出发，观察查询和文档之间的相似程度，并以此来决定如何找出相关的文档。这里的“相似程度”，你可以理解为两者长得有多像。那么，语言模型如何来刻画查询和文档之间的相关度呢？

它不再使用相似度定义，而是采用了概率。一种常见的做法是计算$P(d|q)$，其中$q$表示一个查询，$d$表示一篇文档。$P(d|q)$表示用户输入查询$q$的情况下，文档$d$出现的概率是多少？如果这个概率越高，我们就认为$q$和$d$之间的相关性越高。

通过我们手头的文档集合，并不能直接获得$P(d|q)$。好在我们已经学习过了贝叶斯定理，通过这个定理，我们可以将$P(d|q)$重写如下：

![](https://static001.geekbang.org/resource/image/de/67/de8d794df4e2758063f5ffb26546c467.png?wh=197%2A66)

对于同一个查询，其出现概率$P(q)$都是相同的，同一个文档$d$的出现概率$P(d)$也是固定的。因此它们可以忽略，我们只要关注如何计算$P(q|d)$。而语言模型，为我们解决了如何计算$P(q|d)$的问题，让$k\_{1}, k\_{2}, …, k\_{n}$表示查询q里包含的$n$个关键词。那么根据之前的链式法则公式，可以重写为这样：

![](https://static001.geekbang.org/resource/image/d7/66/d78ba0db38a575269eff4f6cb89afa66.png?wh=735%2A54)

为了提升效率，我们也使用马尔科夫假设和多元文法。假设是三元文法，那么我们可以写成这样：

![](https://static001.geekbang.org/resource/image/e9/13/e977fb4b682e67a8693f1b76f1febe13.png?wh=712%2A52)

最终，当用户输入一个查询$q$之后，对于每一篇文档$d$，我们都能获得$P(d|q)$的值。根据每篇文档所获得的$P(d|q)$这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。

### 2.中文分词

和拉丁语系不同，中文存在分词的问题。如果想进行分词，你就可以使用语言模型。我举个例子给你解释一下，你就明白了。

最普遍的分词方法之一是基于常用词的词典。如果一个尚未分词的句子里发现了存在于字典里的词，我们就认为找到一个新的词，并把它切分出来。这种切分不会出现完全离谱的结果，但是无法解决某些歧义。我下面来举个例子，原句是“兵乓球拍卖完了”。我在读的时候，会有所停顿，你就能理解分词应该如何进行。可是，仅仅从书面来看，至少有以下几种分词方式：

```
第一种，兵乓|球|拍卖|完了
第二种，兵乓球|拍卖|完了
第三种，兵乓|球拍|卖完|了
第四种，兵乓|球拍|卖|完了
```

上面分词的例子，从字面来看都是合理的，所以这种歧义无法通过这句话本身来解决。那么这种情况下，语言模型能为我们做什么呢？我们知道，语言模型是基于大量的语料来统计的，所以我们可以使用这个模型来估算，哪种情况更合理。

假设整个文档集合是D，要分词的句子是$s$，分词结果为$w\_{1}$, … $w\_{n}$，那么我们可以求$P(s)$的概率为：

![](https://static001.geekbang.org/resource/image/c7/34/c74b222e54b6c2a9a6385a988896f334.png?wh=809%2A47)

请注意，在信息检索中，我们关心的是每篇文章产生一个句子（也就是查询）的概率，而这里可以是整个文档集合D产生一个句子的概率。

根据链式法则和三元文法模型，那么上面的式子可以重写为：

![](https://static001.geekbang.org/resource/image/d4/be/d4cfe338d5aed842cd1b366008a551be.png?wh=784%2A44)

也就是说，语言模型可以帮我们估计某种分词结果，在文档集合中出现的概率。但是由于不同的分词方法，会导致$w\_{1}$到$w\_{n}$的不同，因此就会产生不同的$P(s)$。接下来，我们只要取最大的$P(s)$，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。我们可以使用这个公式来求解：

![](https://static001.geekbang.org/resource/image/1c/dc/1cfeaeb018f38f86c448b57b61525adc.png?wh=152%2A42)

其中，$W\_{i}$表示第$i$种分词方法。

回到“兵乓球拍卖完了”这句话，如果文档集合都是讲述的有关体育用品的销售，而不是拍卖行，那么“兵乓|球拍|卖完|了”这种分词的可能性应该更高。

## 小结

这一节，我介绍了基于概率论的语言模型，以及它在信息检索和中文分词领域中的应用。这一节的公式比较多，你刚开始看可能觉得有点犯晕。不用急，我给你梳理了几个要点，你只要掌握这几个要点，依次再进行细节学习，就会事半功倍。

第一，使用联合概率，条件概率和边缘概率的“三角”关系，进行相互推导。链式法则就是很好的体现。

第二，使用马尔科夫假设，把受较多随机变量影响的条件概率，简化为受较少随机变量影响的条件概率，甚至是边缘概率。

第三，使用贝叶斯定理，通过先验概率推导后验概率。在信息检索中，给定查询的情况下推导文档的概率，就需要用到这个定理。

如果你记住了这几点，那么不仅能很快地理解本篇的内容，还能根据实际需求，设计出满足自己需要的语言模型。

## 思考题

在中文分词的时候，我们也可以考虑文章的分类。比如，这样一句话“使用纯净水源浇灌的大米”，正确的切分应该是：

```
使用|纯净|水源|浇灌|的|大米
```

如果我们知道这句描述来自于“大米”类商品，而不是“纯净水”类商品，那么就不会错误地切分为：

```
使用|纯净水|源|浇灌|的|大米
```

想想看，如何对我介绍的语言模型加以改进，把分类信息也包含进去？

欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。你可以点击“请朋友读”，把今天的内容分享给你的好友，和他一起精进。
<div><strong>精选留言（15）</strong></div><ul>
<li><span>枫林火山</span> 👍（22） 💬（3）<p>黄老师，一直没想明白多元文法里的前面N个词的是否有顺序。例如: 大家好， 家大好 。 这2种情况都符合三元文法中的P(xn|xn-2,xn-1)的统计条件吗？
推广下 P(x1,x2,x3,x4….xn) 等于 P(xn,xn-1,xn-2,….,x4,x3,x2,x1) 吗？
百度-联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。我的理解联合概率的条件是可以交换顺序的。
所以从联合概率定义的角度理解是等于的， 但是从语法模型的角度理解，语法是有顺序的，那用联合概率表示对不对。
本节讲的语法模型是把一句话当作有序队列去对待的还是无序集合对待的？ 我听您的讲解是感觉是有序的，但是我理解公式从联合概率定义又感觉公式是在说一个无序的一组条件。 没法把这两者联系起来。
以下两组公式，我只能知道当使用一元文法时，二者时相等的。二元以上有点懵！老师能不能讲解下
P(x1,x2,x3,x4….xn) = P(x1)*P(x2|x1)*P(x3|x1,x2)*…….*P(xn|x1,x2,x3,x4….xn-1) 
P(xn,xn-1,xn-2,….,x4,x3,x2,x1) = P(xn)*P(xn-1|xn)*P(xn-2|xn,xn-1)*…….*P(x1|xn,xn-1,……,x2) </p>2019-04-11</li><br/><li><span></span> 👍（8） 💬（1）<p>文本分类器，对给定文本进行判断。用特征词代表该文本。应该和上篇文章分类的计算有类似之处。计算每个特征词出现在该类文章的概率。然后根据权重分类？或者根据每个词的词频。
（我也很迷糊）那中文中有时词的顺序错乱也能表达一个意思。
比如，密码是123和321是密码；蹦迪坟头和坟头蹦迪。
比如（相互和互相；代替和替代）比
如，纳税可以是一个专有名词，也可以是人名，姓纳名税。还有那遇到多音字咋办？</p>2019-02-08</li><br/><li><span>qinggeouye</span> 👍（6） 💬（1）<p>思考题：

首先，利用语言模型进行中文分词，计算句子 S=使用纯净水源浇灌的大米，属于哪种分词结果 W（“使用|纯净|水源|浇灌|的|大米”、“使用|纯净水|源|浇灌|的|大米”）的概率最大？

然后，回到上节的文本分类，再计算 分词结果W 属于哪种分类（“大米”、“纯净水”）的概率比较大？</p>2019-03-05</li><br/><li><span>唯她命</span> 👍（5） 💬（1）<p>已经求得p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) ....
那么我们怎么求得 p(k2|k1,d) 和  p(k3|k2,k1,d)呢</p>2019-02-26</li><br/><li><span>OP_未央</span> 👍（4） 💬（2）<p>思考题：
可以增加类别的先验概率，P(w1,w2...wn|C)*P(C)；或者已知大米广告的条件下，通过得到的不同分词计算所属类别的概率，选择属于大米概率大的那种分词？</p>2019-04-04</li><br/><li><span>seleven</span> 👍（3） 💬（1）<p>换句话说：其实我是想问，如何能更好的利用全文或者说全部训练集的语义信息？</p>2019-02-16</li><br/><li><span>seleven</span> 👍（3） 💬（1）<p>这篇文章和上篇文章中介绍的分类基本都是在假定文章中的词语相互独立的情况下进行的，虽然马尔科夫模型用到前面词的概率，这可以说是结合了部分上下文之间的语义关系，只使用了和前面词的语义关系，对文本分类其实效果已经很好，尤其是在语料充足的情况下可以使用机器学习的方法让分类模型自迭代优化，目前很多机器翻译就是这么干的。但是我想问下老师，针对语料不是很充足的文本集，如何使用全篇文章的语义（不只是前面词的上下文关系）来设计一个文本分类器？</p>2019-02-15</li><br/><li><span>建强</span> 👍（2） 💬（1）<p>思考题：
考虑分类信息的分词模式，只要把分词概率计算公式中的文档集合D，改为某一分类的文档集合T，即公式改为：arg maxP(Wi|T)，在某个文档类别下，取分词概率最大的那种切分方式，具体计算时，应在同类别下的所有文档中去计算每个分词出现的概率(即分词出现的频度)。</p>2020-05-30</li><br/><li><span>唯她命</span> 👍（2） 💬（1）<p>老师你好，p(K2 | k1,d) 指的是K2 | k1  和   d的联合概率
还是指的是 在满足k1和d的条件下，出现k2的概率？</p>2019-02-26</li><br/><li><span>A君</span> 👍（1） 💬（1）<p>链式法则将联合概率转成了一系列条件概率相乘，而马尔科夫假设简化了多条件概率的计算复杂度，两者结合，就可以计算一个查询、一条文本在数据集&#47;文档中出现的概率，不仅可以计算查询和文档的匹配度，还可以用于进行中文分词，根据数据集特性来对文本进行分词。</p>2021-01-13</li><br/><li><span>罗耀龙@坐忘</span> 👍（1） 💬（1）<p>茶艺师学编程

思考题：如何对语言模型加以改进，把分类信息也包含进去？

1、用另一纬度的信息来帮忙减少不确定性……我想到的是用另外一组信息表达式为y2，原来的为y1，求y1＊y2=0（正交），从而敲定正确分法。

2、链式法则公式中的文档D，直接替换成“全局函数”，即本段文字的语境，接下来就是计算在本语境下合适的分法是哪个?</p>2020-04-16</li><br/><li><span>J.Smile</span> 👍（1） 💬（1）<p>思考题：朴素贝叶斯算法，计算哪种分词方法分类为大米或者纯净水的概率是最大的，比如分词方法A分为大米的概率最大，分词方法B分为纯净水的概率最大，那么根据实际文章分类情况选择不同的分词方法。</p>2020-02-26</li><br/><li><span>唯她命</span> 👍（1） 💬（1）<p>老师，$P(w_{1}, w_{2}, …, w_{n})$ 要在集合中找到一模一样的句子，基本是不可能的

不一定要找到一模一样的句子吧   例如abc   难道cab 这种打乱顺序的不行吗？</p>2019-02-26</li><br/><li><span>013923</span> 👍（0） 💬（1）<p>学习了！谢谢！</p>2022-07-28</li><br/><li><span>鼠里鼠气</span> 👍（0） 💬（1）<p>“二元文法表示，某个单词出现的概率只和它前面的 1 个单词有关。”  这里的“某个单词出现的概率”为什么是条件概率呐？怎么看的?老师</p>2020-11-23</li><br/>
</ul>