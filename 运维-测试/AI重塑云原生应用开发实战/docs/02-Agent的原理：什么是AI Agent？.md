你好，我是邢云阳。

在上节课中，我们详细讨论了 Function Calling 的技术背景和功能，并用一个小例子带你体验了大模型通过工具与外界交互的能力。这节课，我将沿着上节课的脚步，探讨一下，AI Agent 是什么，有了 Function Calling 为什么还需要 Agent？

## AI Agent 是什么？

我先来用比较官方的术语介绍一下 Agent。很多同学在第一次看到这个词时，以为是类似Nginx 等等之类的代理，实际上并不是。在英文中，Agent 指的是人、动物，甚至是具有自主性的概念或实体；当这个词被引入到AI领域后，其含义就引申为了一个计算实体，可以通过传感器感知周围环境，自主作出决策，并通过执行器去采取行动。因此在AI的中文圈子中，Agent 被翻译成“智能体”。

大家看到新的名词不要害怕，也不要感到很神秘，认为又要像学习一门编程语言一样花大力气学习一项新技术。其实 Agent 只是一种思想，是人类为了让大模型更好地解决问题，而设计的一种手段。这种手段让大模型在解决实际问题时，能够不急不躁地推理规划问题解决步骤，并一步步的通过工具调用来解决问题，还能观察问题解决的对不对，从而及时纠错。我们只要理解了这种手段的原理是什么，便可以轻松掌握 Agent。

## 为什么需要 AI Agent？

在上节课中，我们知道了大模型在很多时候具有局限性，我们再来回顾一下大模型的几个缺点：

1. 会产生“幻觉”。
2. 缺乏垂直领域数据的训练，无法回答专业性很强的问题。
3. 对实时了解有限或一无所知。
4. 对于复杂的数学计算，无法完成。

因此，在上节课我们使用大模型的 Function Calling 能力，让其借助工具完成了简单的加法减法任务。

但是在课后思考题中，我提到对于复杂一点的任务，例如对于计算 “1+2+3+4-5-6=？” 直接使用 Function Calling 能否完成？答案是有可能完成不了。原因在于虽然这道题对于人类来说，只是一道小学一二年级的数学题，但是对于大模型，这是一个复杂的多步任务，需要多次调用工具。如果我是一个优秀的大模型，我会这么做：

```plain
1.计算1+2+3+4，我需要调用加法工具
  得到结果10
2.计算10-5，我需要调用减法工具
  得到结果5
3.计算5-6，我需要调用减法工具
  得到结果-1
最终，结果是-1 
```

有基础的同学，可能会知道，以上的过程，是一个把大问题拆分成小问题，逐步解决的过程，专业名词叫思维链（COT），具体的思维链的使用方法和原理我会在下节课详述。

大模型通常不会自带思维链思维，因此一般的大模型通常在调用到第二次、第三次工具时，就“崩溃了”，要么停止计算，要么给出一个幻觉答案。因此我们需要一些手段，来帮助大模型建立良好的思维模式，从而更好地解决问题。

思维链只是其中一种手段，常用的还有ReAct等等，我都会在下节课讲述。

## AI Agent 是如何实现的？

在论文《The Rise and Potential of Large Language Model Based Agents: A Survey》中，有一幅非常经典的图片为我们展示了 Agent 在处理问题时的思考过程。

![图片](https://static001.geekbang.org/resource/image/5e/20/5e8b8393926006193c04aeaefe5a8e20.png?wh=793x471)

可以看到，图片中将 Agent 拆分成了感知（Perception）、大脑（Brain）以及行动（Action）三部分。

### 感知（Perception）

正如人类具备五感一样，大模型也可以通过各类传感器获取输入信息，这被称作多模态输入。多模态输入主要包括：

- 文本输入：例如人类用文字与 ChatGPT 聊天或者上传一个文件让 ChatGPT 分析内容等等。
- 视觉输入：例如人类可以上传一张蒙娜丽莎的图片到 ChatGPT4，让其说明图片上是什么。
- 语音输入：人类可以通过语音与大模型进行交流。
- 其他输入：在物联网时代，人类利用各类传感器实现物物相连。如今，也可以通过传感器为大模型配置上各种感官能力，例如温湿度、传感器等等。

### 大脑（Brain）

在日常生活中，有时会遇到这样的情况：明明已经向某人详细交代了某项任务，但他却当作耳边风一样，很快就忘记了。这种现象，我们形象地称之为“左耳进，右耳出”。这似乎暗示着，信息在从耳朵进入大脑的过程中，并没有得到应有的重视和处理，就像是一条没有被大脑这座桥梁连接的河流，直接从一边流入，又从另一边流出了。

同样，对于大模型，如果只有信息输入，而不去做信息的加工，那么输入的信息将毫无意义。

在图中，大脑所完成的工作大致分成三个部分。

1. **自然语言的处理**

<!--THE END-->

- 多轮对话处理能力

大模型能够结合对话上下文，理解人类对话的意图，最终完成对话目标。当上下文对话过长时，会有很多处理手法，例如图中所显示的 Summary，即摘要，可以对上文对话做总结，从而节省token。

- 语义理解与高质量文本生成

不同的人以及不同的语种提问的方式都不相同，大模型能够理解人类的提问，并给出相应的文本生成。

2. **存储**

图中给出了 Memory（记忆）和 Knowledge（知识）两种存储。

记忆有助于大模型更好地理解人类意图，从而更好地处理问题，例如如下场景：

```plain
history:
human: 北京的天气如何？
AI: 北京晴，13~20℃


question:
human: 济南呢？
```

如果没有记忆，大模型在处理“济南呢？”这个问题时，就无法参考history的历史对话，从而无法理解这个问题是想要问什么。如果有记忆，则可以理解问题问的是“济南的天气如何？”。

记忆可以分为长期记忆和短期记忆。长期记忆即可以将对话历史存储到Redis等数据库中。短期记忆就是在内存中存储的上下文对话。

当记忆过长时，为了节省token以及更好的让大模型参考，可以使用上文提到的摘要的方法，对记忆进行总结。

除了记忆还有知识。因为大模型训练数据的局限性，很多垂直领域问题无法回答。因此可以通过RAG的方式，为大模型提供知识库，来提高大模型回答的准确度。

3. **决策**

当 Agent 获取了信息输入，也理解了信息所表达的含义后，应采取什么样行为来解决问题，这个思考过程称为决策。

决策分为两种类型，分别是 Reasoning（推理）和 Planning（规划）。

推理是 Agent 是否能解决复杂问题的关键。人类为大模型设计了很多推理方法，例如思维链（COT）、Self-Consistency等等，但其方法内容基本都是演绎、归纳和溯因。

规划是 Agent 在推理的基础上可以将复杂的任务拆解成小任务，并为每个小任务设计合适的方法来完成，最终完成大任务。在这个过程中，Agent 可以观察每一个小任务的完成情况，并自我纠错。

### 行动（Action）

行动即 Agent 完成决策后，所要执行的与外部环境交互的动作。行动可以是多样化的，例如：

- 文本输出：这是大模型常规的能力。
- 工具调用：这是 Agent 最常见的用法。上节课讲过的 Function Calling 便是工具调用。
- 具身智能：当 Agent 与物理世界结合后，便产生了具身智能的概念。比如餐饮行业的炒菜机器人，再比如无人驾驶技术等等。

## 总结

今天这节课，我们沿着上节课的思考题，为你引出了 AI Agent 的概念，相信学完本节课，你对于 Agent 有了一个基本的认识。我来做一个简单的总结回顾。

Agent 只是人类为了让大模型变得更聪明，而设计的一种巧妙的思想或手段，绝对不是新技术，在学完后续的两节课程后，你会发现原来我们只需要调整一下 prompt ，就可以完成 Agent 的搭建，这也是 prompt 被称作魔法的魅力。

想象一下，我们在学生时代，有没有过，公式就在书本上，但遇到习题时，我们却依然不会解的场景。但如果老师告诉我们应该怎么思考后，是不是我们就可以一步步的推理出问题的解决步骤，并调用相应的公式完成问题的解决呢？这就是有了 Function Calling 为什么还需要 Agent 的原因，后面结合代码实战，我们再慢慢体会。

最后我引用了一篇论文的观点，从理论的角度拆解了 AI Agent 的组成。各位同学可结合思维导图查看，我就不再赘述。

![图片](https://static001.geekbang.org/resource/image/6e/59/6e87f9684c0c879f0ffe005152562259.png?wh=1920x884)  
下节课开始，我将讲解常见的 Agent 推理方案，并逐步开始做代码实战。

## 思考题

假设通过测试发现 Function Calling 无法解决某些复杂任务，我们希望使用思维链的方式让大模型能够一步步的拆分问题并选择工具，最终解决复杂问题，应该怎样把思维链赋能给大模型呢？

欢迎大家在留言区展示你的思考过程，我们一起探讨。如果你觉得这节课的内容对你有帮助的话，也欢迎你分享给其他朋友，我们下节课再见！
<div><strong>精选留言（1）</strong></div><ul>
<li><span>欢乐马</span> 👍（1） 💬（1）<p>目前的o1, deepseek r1其实已经内置了cot, 那么agent的意义会被削弱吗</p>2025-02-15</li><br/>
</ul>